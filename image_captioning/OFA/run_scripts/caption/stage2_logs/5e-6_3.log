2022-05-18 17:36:40 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2022-05-18 17:36:40 - utils.py[line:261] - INFO: Start init
2022-05-18 17:36:40 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2022-05-18 17:36:40 - utils.py[line:261] - INFO: Start init
2022-05-18 17:36:40 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2022-05-18 17:36:40 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-05-18 17:36:40 - utils.py[line:274] - INFO: initialized host vdsl as rank 0
single-machine distributed training is initialized.
2022-05-18 17:36:40 - utils.py[line:274] - INFO: initialized host vdsl as rank 1
single-machine distributed training is initialized.
2022-05-18 17:36:43 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 500, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 3, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [5e-06], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/database/jhkim/stage2_checkpoints//5e-6_3', 'restore_file': '../../checkpoints/checkpoint_stage1_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'cider', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_large', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='ofa_large', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=1, batch_size_valid=1, best_checkpoint_metric='cider', bf16=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='scst_reward_criterion', cross_self_attention=False, curriculum=0, data='../../dataset/caption_data/caption_stage1_train_ct2.tsv,../../dataset/caption_data/caption_val.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=16, decoder_drop_path_rate=0.0, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, dropout=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_drop_path_rate=0.0, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=2e-07, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', eval_bleu=False, eval_cider=True, eval_cider_cached_tokens='../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, freeze_resnet=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-06], lr_scheduler='polynomial_decay', max_epoch=3, max_source_positions=1024, max_src_length=80, max_target_positions=1024, max_tgt_length=20, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet152', restore_file='../../checkpoints/checkpoint_stage1_best.pt', save_dir='/database/jhkim/stage2_checkpoints//5e-6_3', save_interval=1, save_interval_updates=500, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', scst=True, scst_args='{"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', scst_cider_cached_tokens='../../dataset/caption_data/cider_cached_tokens/coco-train-words.p', seed=1, selected_cols='1,4,2', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, sync_bn=False, task='caption', tensorboard_logdir=None, threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[8], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='../../ofa_module', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=500, wandb_project=None, warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'caption', 'data': '../../dataset/caption_data/caption_stage1_train_ct2.tsv,../../dataset/caption_data/caption_val.tsv', 'selected_cols': '1,4,2', 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 80, 'max_tgt_length': 20, 'code_dict_size': 8192, 'patch_image_size': 480, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_bleu': False, 'eval_cider': True, 'eval_args': '{"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', 'eval_print_samples': False, 'eval_cider_cached_tokens': '../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', 'scst': True, 'scst_args': '{"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}'}, 'criterion': {'_name': 'scst_reward_criterion', 'scst_cider_cached_tokens': '../../dataset/caption_data/cider_cached_tokens/coco-train-words.p', 'ignore_prefix_size': 0, 'sentence_avg': False, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-06]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 2e-07, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-06]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-18 17:36:44 - ofa_task.py[line:102] - INFO: source dictionary: 59457 types
2022-05-18 17:36:44 - ofa_task.py[line:103] - INFO: target dictionary: 59457 types
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2022-05-18 17:36:55 - train.py[line:101] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 1024)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (23): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (24): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (25): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (26): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (27): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (28): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (29): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (30): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (31): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (32): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (33): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (34): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (35): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (patch_layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 1024)
    (embed_image_positions): Embedding(1765, 1024)
    (pos_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (pos_k_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 16)
      (1): Embedding(511, 16)
      (2): Embedding(511, 16)
      (3): Embedding(511, 16)
      (4): Embedding(511, 16)
      (5): Embedding(511, 16)
      (6): Embedding(511, 16)
      (7): Embedding(511, 16)
      (8): Embedding(511, 16)
      (9): Embedding(511, 16)
      (10): Embedding(511, 16)
      (11): Embedding(511, 16)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 16)
      (1): Embedding(6892, 16)
      (2): Embedding(6892, 16)
      (3): Embedding(6892, 16)
      (4): Embedding(6892, 16)
      (5): Embedding(6892, 16)
      (6): Embedding(6892, 16)
      (7): Embedding(6892, 16)
      (8): Embedding(6892, 16)
      (9): Embedding(6892, 16)
      (10): Embedding(6892, 16)
      (11): Embedding(6892, 16)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 1024)
    (embed_image_positions): Embedding(1765, 1024)
    (pos_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (self_pos_k_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (cross_pos_q_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (cross_pos_k_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (code_layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (6): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (7): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (8): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (9): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (10): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (11): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 16)
      (1): Embedding(511, 16)
      (2): Embedding(511, 16)
      (3): Embedding(511, 16)
      (4): Embedding(511, 16)
      (5): Embedding(511, 16)
      (6): Embedding(511, 16)
      (7): Embedding(511, 16)
      (8): Embedding(511, 16)
      (9): Embedding(511, 16)
      (10): Embedding(511, 16)
      (11): Embedding(511, 16)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 16)
      (1): Embedding(6892, 16)
      (2): Embedding(6892, 16)
      (3): Embedding(6892, 16)
      (4): Embedding(6892, 16)
      (5): Embedding(6892, 16)
      (6): Embedding(6892, 16)
      (7): Embedding(6892, 16)
      (8): Embedding(6892, 16)
      (9): Embedding(6892, 16)
      (10): Embedding(6892, 16)
      (11): Embedding(6892, 16)
    )
  )
  (classification_heads): ModuleDict()
)
2022-05-18 17:36:55 - train.py[line:102] - INFO: task: CaptionTask
2022-05-18 17:36:55 - train.py[line:103] - INFO: model: OFAModel
2022-05-18 17:36:55 - train.py[line:104] - INFO: criterion: ScstRewardCriterion
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_val.tsv slice_id 1 row count 2500 total row count 5000
/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-05-18 17:36:55 - train.py[line:108] - INFO: num. shared model params: 472,977,152 (num. trained: 411,964,288)
2022-05-18 17:36:55 - train.py[line:115] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_val.tsv slice_id 0 row count 2500 total row count 5000
/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-05-18 17:36:55 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.4.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.4.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.4.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.5.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.5.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.5.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.6.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.6.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.6.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.7.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.7.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.7.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.23.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.23.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.23.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.24.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.24.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.24.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.25.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.25.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.25.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.26.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.26.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.26.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.27.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.27.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.27.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.28.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.28.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.28.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.29.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.29.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.29.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.30.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.30.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.30.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.31.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.31.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.31.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.32.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.32.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.32.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.33.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.33.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.33.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.34.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.34.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.34.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.35.conv1.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.35.conv2.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.35.conv3.bias
2022-05-18 17:36:55 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-05-18 17:36:55 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-05-18 17:36:55 - utils.py[line:765] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-05-18 17:36:55 - utils.py[line:765] - INFO: rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-05-18 17:36:55 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-05-18 17:36:55 - train.py[line:145] - INFO: training on 2 devices (GPUs/TPUs)
2022-05-18 17:36:55 - train.py[line:151] - INFO: max tokens per device = None and max sentences per device = 1
2022-05-18 17:36:55 - trainer.py[line:458] - INFO: Preparing to load checkpoint ../../checkpoints/checkpoint_stage1_best.pt
2022-05-18 17:37:03 - trainer.py[line:619] - INFO: Loaded checkpoint ../../checkpoints/checkpoint_stage1_best.pt (epoch 2 @ 0 updates)
2022-05-18 17:37:03 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 0 row count 56644 total row count 113287
local datafile ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 1 row count 56643 total row count 113287
slice_id 1 seek offset 56644
Total steps 21243, warmup steps 1274, warmup_factor 0.0007849293563579278
slice_id 0 seek offset 0
Total steps 21243, warmup steps 1274, warmup_factor 0.0007849293563579278
2022-05-18 17:37:06 - trainer.py[line:703] - INFO: begin training epoch 1
2022-05-18 17:37:06 - train.py[line:296] - INFO: Start iterating over samples
/home/wgus5950/OFA/fairseq/fairseq/utils.py:373: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
/home/wgus5950/OFA/fairseq/fairseq/utils.py:373: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-05-18 17:38:32 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 7081 loss=-0.003, score=1.236, ntokens=876.8, nsentences=80, sample_size=876.8, wps=103.7, ups=0.12, wpb=876.8, bsz=80, num_updates=10, lr=3.92465e-08, gnorm=1.416, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=97
2022-05-18 17:39:57 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 7081 loss=-0.005, score=1.324, ntokens=868.4, nsentences=80, sample_size=868.4, wps=102.4, ups=0.12, wpb=868.4, bsz=80, num_updates=20, lr=7.84929e-08, gnorm=1.22, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=182
2022-05-18 17:41:21 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 7081 loss=-0.007, score=1.264, ntokens=858.6, nsentences=80, sample_size=858.6, wps=101.9, ups=0.12, wpb=858.6, bsz=80, num_updates=30, lr=1.17739e-07, gnorm=1.357, clip=70, loss_scale=128, train_wall=84, gb_free=6.8, wall=266
2022-05-18 17:42:46 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 7081 loss=-0.005, score=1.243, ntokens=862.9, nsentences=80, sample_size=862.9, wps=102, ups=0.12, wpb=862.9, bsz=80, num_updates=40, lr=1.56986e-07, gnorm=1.588, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=351
2022-05-18 17:44:11 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 7081 loss=-0.008, score=1.283, ntokens=867.4, nsentences=80, sample_size=867.4, wps=102.1, ups=0.12, wpb=867.4, bsz=80, num_updates=50, lr=1.96232e-07, gnorm=1.327, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=436
2022-05-18 17:45:35 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 7081 loss=-0.005, score=1.298, ntokens=861.1, nsentences=80, sample_size=861.1, wps=102.5, ups=0.12, wpb=861.1, bsz=80, num_updates=60, lr=2.35479e-07, gnorm=1.194, clip=60, loss_scale=128, train_wall=84, gb_free=6.8, wall=520
2022-05-18 17:46:59 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 7081 loss=-0.005, score=1.299, ntokens=860.1, nsentences=80, sample_size=860.1, wps=102.2, ups=0.12, wpb=860.1, bsz=80, num_updates=70, lr=2.74725e-07, gnorm=1.186, clip=80, loss_scale=128, train_wall=84, gb_free=6.8, wall=604
2022-05-18 17:48:24 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 7081 loss=-0.009, score=1.409, ntokens=868, nsentences=80, sample_size=868, wps=102.2, ups=0.12, wpb=868, bsz=80, num_updates=80, lr=3.13972e-07, gnorm=1.878, clip=100, loss_scale=128, train_wall=85, gb_free=6.8, wall=689
2022-05-18 17:49:49 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 7081 loss=-0.008, score=1.389, ntokens=860.6, nsentences=80, sample_size=860.6, wps=101.6, ups=0.12, wpb=860.6, bsz=80, num_updates=90, lr=3.53218e-07, gnorm=1.831, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=774
2022-05-18 17:51:14 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 7081 loss=-0.001, score=1.256, ntokens=873, nsentences=80, sample_size=873, wps=102.9, ups=0.12, wpb=873, bsz=80, num_updates=100, lr=3.92465e-07, gnorm=1.596, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=858
2022-05-18 17:52:38 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 7081 loss=-0.005, score=1.324, ntokens=865.3, nsentences=80, sample_size=865.3, wps=102.3, ups=0.12, wpb=865.3, bsz=80, num_updates=110, lr=4.31711e-07, gnorm=1.873, clip=100, loss_scale=128, train_wall=84, gb_free=6.8, wall=943
2022-05-18 17:54:03 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 7081 loss=-0.004, score=1.272, ntokens=874, nsentences=80, sample_size=874, wps=103.2, ups=0.12, wpb=874, bsz=80, num_updates=120, lr=4.70958e-07, gnorm=1.31, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=1028
2022-05-18 17:55:27 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 7081 loss=-0.006, score=1.288, ntokens=871.6, nsentences=80, sample_size=871.6, wps=102.9, ups=0.12, wpb=871.6, bsz=80, num_updates=130, lr=5.10204e-07, gnorm=1.163, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=1112
2022-05-18 17:56:52 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 7081 loss=-0.004, score=1.263, ntokens=867, nsentences=80, sample_size=867, wps=102.2, ups=0.12, wpb=867, bsz=80, num_updates=140, lr=5.49451e-07, gnorm=1.403, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=1197
2022-05-18 17:58:17 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 7081 loss=-0.005, score=1.395, ntokens=867.2, nsentences=80, sample_size=867.2, wps=102.1, ups=0.12, wpb=867.2, bsz=80, num_updates=150, lr=5.88697e-07, gnorm=1.269, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=1282
2022-05-18 17:59:42 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 7081 loss=-0.007, score=1.194, ntokens=861.6, nsentences=80, sample_size=861.6, wps=101.4, ups=0.12, wpb=861.6, bsz=80, num_updates=160, lr=6.27943e-07, gnorm=1.249, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=1367
2022-05-18 18:01:07 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 7081 loss=-0.004, score=1.302, ntokens=865.5, nsentences=80, sample_size=865.5, wps=102.5, ups=0.12, wpb=865.5, bsz=80, num_updates=170, lr=6.6719e-07, gnorm=1.437, clip=70, loss_scale=128, train_wall=84, gb_free=6.8, wall=1451
2022-05-18 18:02:31 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 7081 loss=-0.006, score=1.287, ntokens=860.5, nsentences=80, sample_size=860.5, wps=101.8, ups=0.12, wpb=860.5, bsz=80, num_updates=180, lr=7.06436e-07, gnorm=1.118, clip=70, loss_scale=128, train_wall=84, gb_free=6.8, wall=1536
2022-05-18 18:03:56 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 7081 loss=-0.004, score=1.209, ntokens=869.9, nsentences=80, sample_size=869.9, wps=102.9, ups=0.12, wpb=869.9, bsz=80, num_updates=190, lr=7.45683e-07, gnorm=1.628, clip=70, loss_scale=128, train_wall=84, gb_free=6.8, wall=1620
2022-05-18 18:05:21 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 7081 loss=-0.006, score=1.335, ntokens=867.9, nsentences=80, sample_size=867.9, wps=102.2, ups=0.12, wpb=867.9, bsz=80, num_updates=200, lr=7.84929e-07, gnorm=1.039, clip=50, loss_scale=128, train_wall=85, gb_free=6.8, wall=1705
2022-05-18 18:06:45 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 7081 loss=-0.004, score=1.349, ntokens=875.2, nsentences=80, sample_size=875.2, wps=103.3, ups=0.12, wpb=875.2, bsz=80, num_updates=210, lr=8.24176e-07, gnorm=1.221, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=1790
2022-05-18 18:08:10 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 7081 loss=-0.005, score=1.268, ntokens=869.7, nsentences=80, sample_size=869.7, wps=102.6, ups=0.12, wpb=869.7, bsz=80, num_updates=220, lr=8.63422e-07, gnorm=1.259, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=1875
2022-05-18 18:09:35 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 7081 loss=-0.005, score=1.339, ntokens=875.6, nsentences=80, sample_size=875.6, wps=103.2, ups=0.12, wpb=875.6, bsz=80, num_updates=230, lr=9.02669e-07, gnorm=1.282, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=1960
2022-05-18 18:11:00 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 7081 loss=-0.004, score=1.356, ntokens=857.8, nsentences=80, sample_size=857.8, wps=101.3, ups=0.12, wpb=857.8, bsz=80, num_updates=240, lr=9.41915e-07, gnorm=1.445, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=2044
2022-05-18 18:12:25 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 7081 loss=-0.008, score=1.274, ntokens=859.7, nsentences=80, sample_size=859.7, wps=101.2, ups=0.12, wpb=859.7, bsz=80, num_updates=250, lr=9.81162e-07, gnorm=1.469, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=2129
2022-05-18 18:13:50 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 7081 loss=-0.006, score=1.336, ntokens=867.9, nsentences=80, sample_size=867.9, wps=101.6, ups=0.12, wpb=867.9, bsz=80, num_updates=260, lr=1.02041e-06, gnorm=1.656, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=2215
2022-05-18 18:15:15 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 7081 loss=-0.008, score=1.448, ntokens=857.9, nsentences=80, sample_size=857.9, wps=101.3, ups=0.12, wpb=857.9, bsz=80, num_updates=270, lr=1.05965e-06, gnorm=1.504, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=2299
2022-05-18 18:16:40 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 7081 loss=-0.005, score=1.198, ntokens=881, nsentences=80, sample_size=881, wps=103.4, ups=0.12, wpb=881, bsz=80, num_updates=280, lr=1.0989e-06, gnorm=1.354, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=2385
2022-05-18 18:18:05 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 7081 loss=-0.007, score=1.296, ntokens=864, nsentences=80, sample_size=864, wps=101, ups=0.12, wpb=864, bsz=80, num_updates=290, lr=1.13815e-06, gnorm=1.351, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=2470
2022-05-18 18:19:30 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 7081 loss=-0.005, score=1.328, ntokens=871.2, nsentences=80, sample_size=871.2, wps=102.5, ups=0.12, wpb=871.2, bsz=80, num_updates=300, lr=1.17739e-06, gnorm=1.193, clip=50, loss_scale=128, train_wall=85, gb_free=6.8, wall=2555
2022-05-18 18:20:56 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 7081 loss=-0.006, score=1.376, ntokens=874.6, nsentences=80, sample_size=874.6, wps=102.6, ups=0.12, wpb=874.6, bsz=80, num_updates=310, lr=1.21664e-06, gnorm=1.295, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=2640
2022-05-18 18:22:21 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 7081 loss=-0.006, score=1.257, ntokens=866.3, nsentences=80, sample_size=866.3, wps=101.8, ups=0.12, wpb=866.3, bsz=80, num_updates=320, lr=1.25589e-06, gnorm=1.199, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=2725
2022-05-18 18:23:46 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 7081 loss=-0.005, score=1.322, ntokens=863.8, nsentences=80, sample_size=863.8, wps=101.7, ups=0.12, wpb=863.8, bsz=80, num_updates=330, lr=1.29513e-06, gnorm=1.283, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=2810
2022-05-18 18:25:11 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 7081 loss=-0.005, score=1.282, ntokens=865.5, nsentences=80, sample_size=865.5, wps=101.8, ups=0.12, wpb=865.5, bsz=80, num_updates=340, lr=1.33438e-06, gnorm=1.206, clip=50, loss_scale=128, train_wall=85, gb_free=6.8, wall=2895
2022-05-18 18:26:35 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 7081 loss=-0.005, score=1.274, ntokens=861.7, nsentences=80, sample_size=861.7, wps=101.8, ups=0.12, wpb=861.7, bsz=80, num_updates=350, lr=1.37363e-06, gnorm=1.411, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=2980
2022-05-18 18:28:01 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 7081 loss=-0.002, score=1.291, ntokens=870.5, nsentences=80, sample_size=870.5, wps=102.1, ups=0.12, wpb=870.5, bsz=80, num_updates=360, lr=1.41287e-06, gnorm=1.572, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=3065
2022-05-18 18:29:25 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 7081 loss=-0.005, score=1.322, ntokens=856.3, nsentences=80, sample_size=856.3, wps=101.2, ups=0.12, wpb=856.3, bsz=80, num_updates=370, lr=1.45212e-06, gnorm=1.234, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=3150
2022-05-18 18:30:51 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 7081 loss=-0.007, score=1.342, ntokens=871.7, nsentences=80, sample_size=871.7, wps=102.2, ups=0.12, wpb=871.7, bsz=80, num_updates=380, lr=1.49137e-06, gnorm=1.48, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=3235
2022-05-18 18:32:16 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 7081 loss=-0.005, score=1.402, ntokens=873, nsentences=80, sample_size=873, wps=102.6, ups=0.12, wpb=873, bsz=80, num_updates=390, lr=1.53061e-06, gnorm=1.275, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=3320
2022-05-18 18:33:41 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 7081 loss=-0.004, score=1.474, ntokens=885.9, nsentences=80, sample_size=885.9, wps=103.5, ups=0.12, wpb=885.9, bsz=80, num_updates=400, lr=1.56986e-06, gnorm=1.523, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=3406
2022-05-18 18:35:07 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 7081 loss=-0.007, score=1.318, ntokens=873.3, nsentences=80, sample_size=873.3, wps=102.1, ups=0.12, wpb=873.3, bsz=80, num_updates=410, lr=1.60911e-06, gnorm=1.307, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=3492
2022-05-18 18:36:31 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 7081 loss=-0.005, score=1.23, ntokens=865.7, nsentences=80, sample_size=865.7, wps=102.4, ups=0.12, wpb=865.7, bsz=80, num_updates=420, lr=1.64835e-06, gnorm=1.196, clip=70, loss_scale=128, train_wall=84, gb_free=6.8, wall=3576
2022-05-18 18:37:56 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 7081 loss=-0.005, score=1.344, ntokens=869.2, nsentences=80, sample_size=869.2, wps=102.7, ups=0.12, wpb=869.2, bsz=80, num_updates=430, lr=1.6876e-06, gnorm=1.402, clip=100, loss_scale=128, train_wall=85, gb_free=6.8, wall=3661
2022-05-18 18:39:21 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 7081 loss=-0.004, score=1.14, ntokens=869, nsentences=80, sample_size=869, wps=102.2, ups=0.12, wpb=869, bsz=80, num_updates=440, lr=1.72684e-06, gnorm=1.406, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=3746
2022-05-18 18:40:46 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 7081 loss=-0.006, score=1.41, ntokens=875.3, nsentences=80, sample_size=875.3, wps=102.7, ups=0.12, wpb=875.3, bsz=80, num_updates=450, lr=1.76609e-06, gnorm=1.322, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=3831
2022-05-18 18:42:11 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 7081 loss=-0.003, score=1.271, ntokens=869.7, nsentences=80, sample_size=869.7, wps=102.3, ups=0.12, wpb=869.7, bsz=80, num_updates=460, lr=1.80534e-06, gnorm=1.186, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=3916
2022-05-18 18:43:36 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 7081 loss=-0.007, score=1.214, ntokens=867.1, nsentences=80, sample_size=867.1, wps=102.1, ups=0.12, wpb=867.1, bsz=80, num_updates=470, lr=1.84458e-06, gnorm=1.257, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=4001
2022-05-18 18:45:01 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 7081 loss=-0.005, score=1.365, ntokens=863.5, nsentences=80, sample_size=863.5, wps=101.5, ups=0.12, wpb=863.5, bsz=80, num_updates=480, lr=1.88383e-06, gnorm=1.317, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=4086
2022-05-18 18:46:27 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 7081 loss=-0.004, score=1.272, ntokens=875, nsentences=80, sample_size=875, wps=102.1, ups=0.12, wpb=875, bsz=80, num_updates=490, lr=1.92308e-06, gnorm=1.494, clip=100, loss_scale=128, train_wall=86, gb_free=6.8, wall=4172
2022-05-18 18:47:52 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 7081 loss=-0.006, score=1.281, ntokens=871.3, nsentences=80, sample_size=871.3, wps=102, ups=0.12, wpb=871.3, bsz=80, num_updates=500, lr=1.96232e-06, gnorm=1.362, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=4257
2022-05-18 18:47:52 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-05-18 19:29:02 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.305 | ntokens 109.882 | nsentences 10 | sample_size 109.882 | cider 1.396 | wps 111.2 | wpb 109.9 | bsz 10 | num_updates 500
2022-05-18 19:29:02 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 500 updates
2022-05-18 19:29:02 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_500.pt
2022-05-18 19:29:11 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_500.pt
2022-05-18 19:29:22 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_500.pt (epoch 1 @ 500 updates, score 1.396) (writing took 19.530631626956165 seconds)
2022-05-18 19:30:45 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 7081 loss=-0.004, score=1.357, ntokens=867.2, nsentences=80, sample_size=867.2, wps=3.4, ups=0, wpb=867.2, bsz=80, num_updates=510, lr=2.00157e-06, gnorm=1.396, clip=80, loss_scale=128, train_wall=83, gb_free=6.8, wall=6830
2022-05-18 19:32:10 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 7081 loss=-0.003, score=1.199, ntokens=881.4, nsentences=80, sample_size=881.4, wps=103.7, ups=0.12, wpb=881.4, bsz=80, num_updates=520, lr=2.04082e-06, gnorm=1.224, clip=80, loss_scale=256, train_wall=85, gb_free=6.8, wall=6915
2022-05-18 19:33:35 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 7081 loss=-0.003, score=1.267, ntokens=879.5, nsentences=80, sample_size=879.5, wps=103.5, ups=0.12, wpb=879.5, bsz=80, num_updates=530, lr=2.08006e-06, gnorm=1.668, clip=80, loss_scale=256, train_wall=85, gb_free=6.8, wall=7000
2022-05-18 19:35:00 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 7081 loss=-0.003, score=1.321, ntokens=866.2, nsentences=80, sample_size=866.2, wps=101.7, ups=0.12, wpb=866.2, bsz=80, num_updates=540, lr=2.11931e-06, gnorm=1.157, clip=70, loss_scale=256, train_wall=85, gb_free=6.8, wall=7085
2022-05-18 19:36:17 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-05-18 19:36:34 - progress_bar.py[line:274] - INFO: epoch 001:    551 / 7081 loss=-0.007, score=1.294, ntokens=875.5, nsentences=80, sample_size=875.5, wps=93.2, ups=0.11, wpb=875.5, bsz=80, num_updates=550, lr=2.15856e-06, gnorm=1.159, clip=50, loss_scale=128, train_wall=94, gb_free=6.8, wall=7179
2022-05-18 19:37:59 - progress_bar.py[line:274] - INFO: epoch 001:    561 / 7081 loss=-0.005, score=1.318, ntokens=882.3, nsentences=80, sample_size=882.3, wps=103.7, ups=0.12, wpb=882.3, bsz=80, num_updates=560, lr=2.1978e-06, gnorm=1.444, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=7264
2022-05-18 19:39:24 - progress_bar.py[line:274] - INFO: epoch 001:    571 / 7081 loss=-0.003, score=1.373, ntokens=866.7, nsentences=80, sample_size=866.7, wps=102.8, ups=0.12, wpb=866.7, bsz=80, num_updates=570, lr=2.23705e-06, gnorm=1.449, clip=80, loss_scale=128, train_wall=84, gb_free=6.8, wall=7348
2022-05-18 19:40:49 - progress_bar.py[line:274] - INFO: epoch 001:    581 / 7081 loss=-0.006, score=1.36, ntokens=878.8, nsentences=80, sample_size=878.8, wps=102.4, ups=0.12, wpb=878.8, bsz=80, num_updates=580, lr=2.2763e-06, gnorm=1.351, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=7434
2022-05-18 19:42:15 - progress_bar.py[line:274] - INFO: epoch 001:    591 / 7081 loss=-0.006, score=1.363, ntokens=875.7, nsentences=80, sample_size=875.7, wps=102.4, ups=0.12, wpb=875.7, bsz=80, num_updates=590, lr=2.31554e-06, gnorm=1.169, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=7520
2022-05-18 19:43:40 - progress_bar.py[line:274] - INFO: epoch 001:    601 / 7081 loss=-0.004, score=1.368, ntokens=873, nsentences=80, sample_size=873, wps=102.6, ups=0.12, wpb=873, bsz=80, num_updates=600, lr=2.35479e-06, gnorm=1.4, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=7605
2022-05-18 19:45:05 - progress_bar.py[line:274] - INFO: epoch 001:    611 / 7081 loss=-0.004, score=1.359, ntokens=872.3, nsentences=80, sample_size=872.3, wps=102.2, ups=0.12, wpb=872.3, bsz=80, num_updates=610, lr=2.39403e-06, gnorm=1.248, clip=50, loss_scale=128, train_wall=85, gb_free=6.8, wall=7690
2022-05-18 19:46:30 - progress_bar.py[line:274] - INFO: epoch 001:    621 / 7081 loss=-0.004, score=1.459, ntokens=876.9, nsentences=80, sample_size=876.9, wps=103.1, ups=0.12, wpb=876.9, bsz=80, num_updates=620, lr=2.43328e-06, gnorm=1.614, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=7775
2022-05-18 19:47:56 - progress_bar.py[line:274] - INFO: epoch 001:    631 / 7081 loss=-0.006, score=1.299, ntokens=884.4, nsentences=80, sample_size=884.4, wps=103.7, ups=0.12, wpb=884.4, bsz=80, num_updates=630, lr=2.47253e-06, gnorm=1.477, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=7860
2022-05-18 19:49:21 - progress_bar.py[line:274] - INFO: epoch 001:    641 / 7081 loss=-0.007, score=1.338, ntokens=881.1, nsentences=80, sample_size=881.1, wps=103.3, ups=0.12, wpb=881.1, bsz=80, num_updates=640, lr=2.51177e-06, gnorm=1.441, clip=50, loss_scale=128, train_wall=85, gb_free=6.8, wall=7946
2022-05-18 19:50:47 - progress_bar.py[line:274] - INFO: epoch 001:    651 / 7081 loss=-0.003, score=1.31, ntokens=877.3, nsentences=80, sample_size=877.3, wps=102.3, ups=0.12, wpb=877.3, bsz=80, num_updates=650, lr=2.55102e-06, gnorm=1.739, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=8031
2022-05-18 19:52:11 - progress_bar.py[line:274] - INFO: epoch 001:    661 / 7081 loss=-0.004, score=1.253, ntokens=861.8, nsentences=80, sample_size=861.8, wps=101.6, ups=0.12, wpb=861.8, bsz=80, num_updates=660, lr=2.59027e-06, gnorm=1.364, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=8116
2022-05-18 19:53:36 - progress_bar.py[line:274] - INFO: epoch 001:    671 / 7081 loss=-0.004, score=1.336, ntokens=870, nsentences=80, sample_size=870, wps=102.6, ups=0.12, wpb=870, bsz=80, num_updates=670, lr=2.62951e-06, gnorm=1.195, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=8201
2022-05-18 19:55:02 - progress_bar.py[line:274] - INFO: epoch 001:    681 / 7081 loss=-0.003, score=1.322, ntokens=877, nsentences=80, sample_size=877, wps=102.6, ups=0.12, wpb=877, bsz=80, num_updates=680, lr=2.66876e-06, gnorm=1.665, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=8287
2022-05-18 19:56:27 - progress_bar.py[line:274] - INFO: epoch 001:    691 / 7081 loss=-0.004, score=1.299, ntokens=873.6, nsentences=80, sample_size=873.6, wps=102.2, ups=0.12, wpb=873.6, bsz=80, num_updates=690, lr=2.70801e-06, gnorm=1.163, clip=50, loss_scale=128, train_wall=85, gb_free=6.8, wall=8372
2022-05-18 19:57:53 - progress_bar.py[line:274] - INFO: epoch 001:    701 / 7081 loss=-0.005, score=1.286, ntokens=878.8, nsentences=80, sample_size=878.8, wps=102.9, ups=0.12, wpb=878.8, bsz=80, num_updates=700, lr=2.74725e-06, gnorm=1.318, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=8457
2022-05-18 19:59:18 - progress_bar.py[line:274] - INFO: epoch 001:    711 / 7081 loss=-0.004, score=1.265, ntokens=878.5, nsentences=80, sample_size=878.5, wps=102.6, ups=0.12, wpb=878.5, bsz=80, num_updates=710, lr=2.7865e-06, gnorm=1.372, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=8543
2022-05-18 20:00:44 - progress_bar.py[line:274] - INFO: epoch 001:    721 / 7081 loss=-0.006, score=1.266, ntokens=883.1, nsentences=80, sample_size=883.1, wps=103.3, ups=0.12, wpb=883.1, bsz=80, num_updates=720, lr=2.82575e-06, gnorm=1.208, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=8629
2022-05-18 20:02:09 - progress_bar.py[line:274] - INFO: epoch 001:    731 / 7081 loss=-0.005, score=1.216, ntokens=890.5, nsentences=80, sample_size=890.5, wps=104.1, ups=0.12, wpb=890.5, bsz=80, num_updates=730, lr=2.86499e-06, gnorm=1.158, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=8714
2022-05-18 20:03:35 - progress_bar.py[line:274] - INFO: epoch 001:    741 / 7081 loss=-0.005, score=1.286, ntokens=875.8, nsentences=80, sample_size=875.8, wps=102.4, ups=0.12, wpb=875.8, bsz=80, num_updates=740, lr=2.90424e-06, gnorm=1.185, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=8800
2022-05-18 20:05:00 - progress_bar.py[line:274] - INFO: epoch 001:    751 / 7081 loss=-0.006, score=1.393, ntokens=875.2, nsentences=80, sample_size=875.2, wps=102.8, ups=0.12, wpb=875.2, bsz=80, num_updates=750, lr=2.94349e-06, gnorm=1.378, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=8885
2022-05-18 20:06:25 - progress_bar.py[line:274] - INFO: epoch 001:    761 / 7081 loss=-0.005, score=1.413, ntokens=870.7, nsentences=80, sample_size=870.7, wps=102.3, ups=0.12, wpb=870.7, bsz=80, num_updates=760, lr=2.98273e-06, gnorm=1.669, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=8970
2022-05-18 20:07:50 - progress_bar.py[line:274] - INFO: epoch 001:    771 / 7081 loss=-0.004, score=1.259, ntokens=879.8, nsentences=80, sample_size=879.8, wps=103.2, ups=0.12, wpb=879.8, bsz=80, num_updates=770, lr=3.02198e-06, gnorm=1.092, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=9055
2022-05-18 20:09:16 - progress_bar.py[line:274] - INFO: epoch 001:    781 / 7081 loss=-0.005, score=1.304, ntokens=876.5, nsentences=80, sample_size=876.5, wps=102.7, ups=0.12, wpb=876.5, bsz=80, num_updates=780, lr=3.06122e-06, gnorm=1.3, clip=60, loss_scale=128, train_wall=85, gb_free=6.8, wall=9140
2022-05-18 20:10:41 - progress_bar.py[line:274] - INFO: epoch 001:    791 / 7081 loss=-0.006, score=1.339, ntokens=871.6, nsentences=80, sample_size=871.6, wps=101.8, ups=0.12, wpb=871.6, bsz=80, num_updates=790, lr=3.10047e-06, gnorm=1.369, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=9226
2022-05-18 20:12:07 - progress_bar.py[line:274] - INFO: epoch 001:    801 / 7081 loss=-0.007, score=1.262, ntokens=877.1, nsentences=80, sample_size=877.1, wps=102.4, ups=0.12, wpb=877.1, bsz=80, num_updates=800, lr=3.13972e-06, gnorm=1.799, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=9312
2022-05-18 20:13:32 - progress_bar.py[line:274] - INFO: epoch 001:    811 / 7081 loss=-0.003, score=1.301, ntokens=873.7, nsentences=80, sample_size=873.7, wps=102.6, ups=0.12, wpb=873.7, bsz=80, num_updates=810, lr=3.17896e-06, gnorm=1.396, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=9397
2022-05-18 20:14:57 - progress_bar.py[line:274] - INFO: epoch 001:    821 / 7081 loss=-0.007, score=1.397, ntokens=873.9, nsentences=80, sample_size=873.9, wps=102.9, ups=0.12, wpb=873.9, bsz=80, num_updates=820, lr=3.21821e-06, gnorm=1.927, clip=100, loss_scale=128, train_wall=85, gb_free=6.8, wall=9482
2022-05-18 20:16:22 - progress_bar.py[line:274] - INFO: epoch 001:    831 / 7081 loss=-0.006, score=1.326, ntokens=872.4, nsentences=80, sample_size=872.4, wps=102.7, ups=0.12, wpb=872.4, bsz=80, num_updates=830, lr=3.25746e-06, gnorm=1.449, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=9567
2022-05-18 20:17:48 - progress_bar.py[line:274] - INFO: epoch 001:    841 / 7081 loss=-0.009, score=1.468, ntokens=890, nsentences=80, sample_size=890, wps=103.7, ups=0.12, wpb=890, bsz=80, num_updates=840, lr=3.2967e-06, gnorm=1.539, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=9653
2022-05-18 20:19:13 - progress_bar.py[line:274] - INFO: epoch 001:    851 / 7081 loss=-0.002, score=1.242, ntokens=869.3, nsentences=80, sample_size=869.3, wps=101.9, ups=0.12, wpb=869.3, bsz=80, num_updates=850, lr=3.33595e-06, gnorm=1.573, clip=70, loss_scale=128, train_wall=85, gb_free=6.8, wall=9738
2022-05-18 20:20:39 - progress_bar.py[line:274] - INFO: epoch 001:    861 / 7081 loss=-0.005, score=1.27, ntokens=878.4, nsentences=80, sample_size=878.4, wps=102.2, ups=0.12, wpb=878.4, bsz=80, num_updates=860, lr=3.3752e-06, gnorm=1.298, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=9824
2022-05-18 20:22:05 - progress_bar.py[line:274] - INFO: epoch 001:    871 / 7081 loss=-0.006, score=1.352, ntokens=878.8, nsentences=80, sample_size=878.8, wps=102.5, ups=0.12, wpb=878.8, bsz=80, num_updates=870, lr=3.41444e-06, gnorm=1.088, clip=40, loss_scale=128, train_wall=86, gb_free=6.8, wall=9910
2022-05-18 20:23:30 - progress_bar.py[line:274] - INFO: epoch 001:    881 / 7081 loss=-0.006, score=1.313, ntokens=865.7, nsentences=80, sample_size=865.7, wps=102.2, ups=0.12, wpb=865.7, bsz=80, num_updates=880, lr=3.45369e-06, gnorm=1.31, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=9994
2022-05-18 20:24:55 - progress_bar.py[line:274] - INFO: epoch 001:    891 / 7081 loss=-0.004, score=1.309, ntokens=867.5, nsentences=80, sample_size=867.5, wps=101.4, ups=0.12, wpb=867.5, bsz=80, num_updates=890, lr=3.49294e-06, gnorm=1.44, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=10080
2022-05-18 20:26:20 - progress_bar.py[line:274] - INFO: epoch 001:    901 / 7081 loss=-0.005, score=1.243, ntokens=865.7, nsentences=80, sample_size=865.7, wps=101.7, ups=0.12, wpb=865.7, bsz=80, num_updates=900, lr=3.53218e-06, gnorm=1.063, clip=50, loss_scale=128, train_wall=85, gb_free=6.8, wall=10165
2022-05-18 20:27:46 - progress_bar.py[line:274] - INFO: epoch 001:    911 / 7081 loss=-0.006, score=1.344, ntokens=874, nsentences=80, sample_size=874, wps=102.2, ups=0.12, wpb=874, bsz=80, num_updates=910, lr=3.57143e-06, gnorm=1.565, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=10251
2022-05-18 20:29:11 - progress_bar.py[line:274] - INFO: epoch 001:    921 / 7081 loss=-0.006, score=1.371, ntokens=866.2, nsentences=80, sample_size=866.2, wps=102, ups=0.12, wpb=866.2, bsz=80, num_updates=920, lr=3.61068e-06, gnorm=1.221, clip=50, loss_scale=128, train_wall=85, gb_free=6.8, wall=10336
2022-05-18 20:30:36 - progress_bar.py[line:274] - INFO: epoch 001:    931 / 7081 loss=-0.006, score=1.288, ntokens=865.5, nsentences=80, sample_size=865.5, wps=101.2, ups=0.12, wpb=865.5, bsz=80, num_updates=930, lr=3.64992e-06, gnorm=1.553, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=10421
2022-05-18 20:32:02 - progress_bar.py[line:274] - INFO: epoch 001:    941 / 7081 loss=-0.005, score=1.326, ntokens=882.3, nsentences=80, sample_size=882.3, wps=102.9, ups=0.12, wpb=882.3, bsz=80, num_updates=940, lr=3.68917e-06, gnorm=1.345, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=10507
2022-05-18 20:33:29 - progress_bar.py[line:274] - INFO: epoch 001:    951 / 7081 loss=-0.004, score=1.387, ntokens=866.2, nsentences=80, sample_size=866.2, wps=99.6, ups=0.11, wpb=866.2, bsz=80, num_updates=950, lr=3.72841e-06, gnorm=1.467, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=10594
2022-05-18 20:34:55 - progress_bar.py[line:274] - INFO: epoch 001:    961 / 7081 loss=-0.005, score=1.339, ntokens=876.6, nsentences=80, sample_size=876.6, wps=101.6, ups=0.12, wpb=876.6, bsz=80, num_updates=960, lr=3.76766e-06, gnorm=1.223, clip=50, loss_scale=128, train_wall=86, gb_free=6.8, wall=10680
2022-05-18 20:36:21 - progress_bar.py[line:274] - INFO: epoch 001:    971 / 7081 loss=-0.006, score=1.388, ntokens=869.5, nsentences=80, sample_size=869.5, wps=101.3, ups=0.12, wpb=869.5, bsz=80, num_updates=970, lr=3.80691e-06, gnorm=1.346, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=10766
2022-05-18 20:37:48 - progress_bar.py[line:274] - INFO: epoch 001:    981 / 7081 loss=-0.004, score=1.289, ntokens=878.6, nsentences=80, sample_size=878.6, wps=101.6, ups=0.12, wpb=878.6, bsz=80, num_updates=980, lr=3.84615e-06, gnorm=1.145, clip=60, loss_scale=128, train_wall=86, gb_free=6.8, wall=10852
2022-05-18 20:39:14 - progress_bar.py[line:274] - INFO: epoch 001:    991 / 7081 loss=-0.005, score=1.32, ntokens=879.8, nsentences=80, sample_size=879.8, wps=101.7, ups=0.12, wpb=879.8, bsz=80, num_updates=990, lr=3.8854e-06, gnorm=1.264, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=10939
2022-05-18 20:40:41 - progress_bar.py[line:274] - INFO: epoch 001:   1001 / 7081 loss=-0.004, score=1.378, ntokens=864.8, nsentences=80, sample_size=864.8, wps=99.9, ups=0.12, wpb=864.8, bsz=80, num_updates=1000, lr=3.92465e-06, gnorm=1.537, clip=100, loss_scale=128, train_wall=86, gb_free=6.8, wall=11025
2022-05-18 20:40:41 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-18 21:22:36 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.319 | ntokens 110.187 | nsentences 10 | sample_size 110.187 | cider 1.411 | wps 109.5 | wpb 110.2 | bsz 10 | num_updates 1000 | best_cider 1.411
2022-05-18 21:22:36 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 1000 updates
2022-05-18 21:22:36 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_1000.pt
2022-05-18 21:22:45 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_1000.pt
2022-05-18 21:26:20 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 1.411) (writing took 223.80926390504465 seconds)
2022-05-18 21:27:45 - progress_bar.py[line:274] - INFO: epoch 001:   1011 / 7081 loss=-0.007, score=1.343, ntokens=859.9, nsentences=80, sample_size=859.9, wps=3, ups=0, wpb=859.9, bsz=80, num_updates=1010, lr=3.96389e-06, gnorm=1.462, clip=90, loss_scale=128, train_wall=84, gb_free=6.8, wall=13850
2022-05-18 21:29:11 - progress_bar.py[line:274] - INFO: epoch 001:   1021 / 7081 loss=-0.007, score=1.369, ntokens=877.3, nsentences=80, sample_size=877.3, wps=101.8, ups=0.12, wpb=877.3, bsz=80, num_updates=1020, lr=4.00314e-06, gnorm=1.053, clip=40, loss_scale=128, train_wall=86, gb_free=6.8, wall=13936
2022-05-18 21:30:37 - progress_bar.py[line:274] - INFO: epoch 001:   1031 / 7081 loss=-0.004, score=1.32, ntokens=879.8, nsentences=80, sample_size=879.8, wps=102.6, ups=0.12, wpb=879.8, bsz=80, num_updates=1030, lr=4.04239e-06, gnorm=1.718, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=14022
2022-05-18 21:32:04 - progress_bar.py[line:274] - INFO: epoch 001:   1041 / 7081 loss=-0.005, score=1.397, ntokens=871.2, nsentences=80, sample_size=871.2, wps=100.8, ups=0.12, wpb=871.2, bsz=80, num_updates=1040, lr=4.08163e-06, gnorm=1.235, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=14108
2022-05-18 21:33:29 - progress_bar.py[line:274] - INFO: epoch 001:   1051 / 7081 loss=-0.004, score=1.348, ntokens=863.6, nsentences=80, sample_size=863.6, wps=100.6, ups=0.12, wpb=863.6, bsz=80, num_updates=1050, lr=4.12088e-06, gnorm=1.293, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=14194
2022-05-18 21:34:55 - progress_bar.py[line:274] - INFO: epoch 001:   1061 / 7081 loss=-0.002, score=1.396, ntokens=864, nsentences=80, sample_size=864, wps=100.8, ups=0.12, wpb=864, bsz=80, num_updates=1060, lr=4.16013e-06, gnorm=1.418, clip=70, loss_scale=256, train_wall=86, gb_free=6.8, wall=14280
2022-05-18 21:36:22 - progress_bar.py[line:274] - INFO: epoch 001:   1071 / 7081 loss=-0.004, score=1.325, ntokens=887.7, nsentences=80, sample_size=887.7, wps=102.1, ups=0.11, wpb=887.7, bsz=80, num_updates=1070, lr=4.19937e-06, gnorm=1.289, clip=60, loss_scale=256, train_wall=87, gb_free=6.8, wall=14367
2022-05-18 21:37:48 - progress_bar.py[line:274] - INFO: epoch 001:   1081 / 7081 loss=-0.006, score=1.315, ntokens=872.1, nsentences=80, sample_size=872.1, wps=101.1, ups=0.12, wpb=872.1, bsz=80, num_updates=1080, lr=4.23862e-06, gnorm=1.308, clip=80, loss_scale=256, train_wall=86, gb_free=6.8, wall=14453
2022-05-18 21:39:14 - progress_bar.py[line:274] - INFO: epoch 001:   1091 / 7081 loss=-0.006, score=1.336, ntokens=864.9, nsentences=80, sample_size=864.9, wps=100.5, ups=0.12, wpb=864.9, bsz=80, num_updates=1090, lr=4.27786e-06, gnorm=1.279, clip=70, loss_scale=256, train_wall=86, gb_free=6.8, wall=14539
2022-05-18 21:40:41 - progress_bar.py[line:274] - INFO: epoch 001:   1101 / 7081 loss=-0.007, score=1.386, ntokens=870.7, nsentences=80, sample_size=870.7, wps=100.7, ups=0.12, wpb=870.7, bsz=80, num_updates=1100, lr=4.31711e-06, gnorm=1.498, clip=70, loss_scale=256, train_wall=86, gb_free=6.8, wall=14626
2022-05-18 21:42:07 - progress_bar.py[line:274] - INFO: epoch 001:   1111 / 7081 loss=-0.005, score=1.378, ntokens=872.5, nsentences=80, sample_size=872.5, wps=101.5, ups=0.12, wpb=872.5, bsz=80, num_updates=1110, lr=4.35636e-06, gnorm=1.348, clip=70, loss_scale=256, train_wall=86, gb_free=6.8, wall=14712
2022-05-18 21:43:33 - progress_bar.py[line:274] - INFO: epoch 001:   1121 / 7081 loss=-0.004, score=1.287, ntokens=880.3, nsentences=80, sample_size=880.3, wps=102, ups=0.12, wpb=880.3, bsz=80, num_updates=1120, lr=4.3956e-06, gnorm=1.225, clip=70, loss_scale=256, train_wall=86, gb_free=6.8, wall=14798
2022-05-18 21:44:33 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-05-18 21:45:07 - progress_bar.py[line:274] - INFO: epoch 001:   1132 / 7081 loss=-0.006, score=1.377, ntokens=869.9, nsentences=80, sample_size=869.9, wps=92.8, ups=0.11, wpb=869.9, bsz=80, num_updates=1130, lr=4.43485e-06, gnorm=1.448, clip=70, loss_scale=128, train_wall=94, gb_free=6.8, wall=14892
2022-05-18 21:46:33 - progress_bar.py[line:274] - INFO: epoch 001:   1142 / 7081 loss=-0.006, score=1.342, ntokens=870.6, nsentences=80, sample_size=870.6, wps=100.9, ups=0.12, wpb=870.6, bsz=80, num_updates=1140, lr=4.4741e-06, gnorm=1.351, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=14978
2022-05-18 21:48:00 - progress_bar.py[line:274] - INFO: epoch 001:   1152 / 7081 loss=-0.005, score=1.377, ntokens=882.7, nsentences=80, sample_size=882.7, wps=102.1, ups=0.12, wpb=882.7, bsz=80, num_updates=1150, lr=4.51334e-06, gnorm=1.513, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=15064
2022-05-18 21:49:26 - progress_bar.py[line:274] - INFO: epoch 001:   1162 / 7081 loss=-0.004, score=1.358, ntokens=879.9, nsentences=80, sample_size=879.9, wps=101.3, ups=0.12, wpb=879.9, bsz=80, num_updates=1160, lr=4.55259e-06, gnorm=1.178, clip=60, loss_scale=128, train_wall=87, gb_free=6.8, wall=15151
2022-05-18 21:50:53 - progress_bar.py[line:274] - INFO: epoch 001:   1172 / 7081 loss=-0.004, score=1.314, ntokens=883.8, nsentences=80, sample_size=883.8, wps=101.5, ups=0.11, wpb=883.8, bsz=80, num_updates=1170, lr=4.59184e-06, gnorm=1.472, clip=70, loss_scale=128, train_wall=87, gb_free=6.8, wall=15238
2022-05-18 21:52:20 - progress_bar.py[line:274] - INFO: epoch 001:   1182 / 7081 loss=-0.006, score=1.398, ntokens=872.5, nsentences=80, sample_size=872.5, wps=100.5, ups=0.12, wpb=872.5, bsz=80, num_updates=1180, lr=4.63108e-06, gnorm=1.479, clip=50, loss_scale=128, train_wall=87, gb_free=6.8, wall=15325
2022-05-18 21:53:47 - progress_bar.py[line:274] - INFO: epoch 001:   1192 / 7081 loss=-0.003, score=1.265, ntokens=887.3, nsentences=80, sample_size=887.3, wps=102.7, ups=0.12, wpb=887.3, bsz=80, num_updates=1190, lr=4.67033e-06, gnorm=1.513, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=15412
2022-05-18 21:55:13 - progress_bar.py[line:274] - INFO: epoch 001:   1202 / 7081 loss=-0.003, score=1.269, ntokens=885.2, nsentences=80, sample_size=885.2, wps=102.9, ups=0.12, wpb=885.2, bsz=80, num_updates=1200, lr=4.70958e-06, gnorm=1.464, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=15498
2022-05-18 21:56:39 - progress_bar.py[line:274] - INFO: epoch 001:   1212 / 7081 loss=-0.004, score=1.356, ntokens=873.5, nsentences=80, sample_size=873.5, wps=101.3, ups=0.12, wpb=873.5, bsz=80, num_updates=1210, lr=4.74882e-06, gnorm=1.561, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=15584
2022-05-18 21:58:05 - progress_bar.py[line:274] - INFO: epoch 001:   1222 / 7081 loss=-0.003, score=1.483, ntokens=870.7, nsentences=80, sample_size=870.7, wps=101.4, ups=0.12, wpb=870.7, bsz=80, num_updates=1220, lr=4.78807e-06, gnorm=1.476, clip=50, loss_scale=128, train_wall=86, gb_free=6.8, wall=15670
2022-05-18 21:59:31 - progress_bar.py[line:274] - INFO: epoch 001:   1232 / 7081 loss=-0.006, score=1.407, ntokens=872.7, nsentences=80, sample_size=872.7, wps=100.9, ups=0.12, wpb=872.7, bsz=80, num_updates=1230, lr=4.82732e-06, gnorm=1.98, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=15756
2022-05-18 22:00:58 - progress_bar.py[line:274] - INFO: epoch 001:   1242 / 7081 loss=-0.003, score=1.235, ntokens=882, nsentences=80, sample_size=882, wps=101.5, ups=0.12, wpb=882, bsz=80, num_updates=1240, lr=4.86656e-06, gnorm=1.184, clip=60, loss_scale=128, train_wall=87, gb_free=6.8, wall=15843
2022-05-18 22:02:25 - progress_bar.py[line:274] - INFO: epoch 001:   1252 / 7081 loss=-0.006, score=1.354, ntokens=877.7, nsentences=80, sample_size=877.7, wps=101.4, ups=0.12, wpb=877.7, bsz=80, num_updates=1250, lr=4.90581e-06, gnorm=1.149, clip=60, loss_scale=128, train_wall=86, gb_free=6.8, wall=15930
2022-05-18 22:03:52 - progress_bar.py[line:274] - INFO: epoch 001:   1262 / 7081 loss=-0.006, score=1.426, ntokens=887, nsentences=80, sample_size=887, wps=102, ups=0.12, wpb=887, bsz=80, num_updates=1260, lr=4.94505e-06, gnorm=1.351, clip=70, loss_scale=128, train_wall=87, gb_free=6.8, wall=16017
2022-05-18 22:05:18 - progress_bar.py[line:274] - INFO: epoch 001:   1272 / 7081 loss=-0.005, score=1.309, ntokens=880.6, nsentences=80, sample_size=880.6, wps=101.8, ups=0.12, wpb=880.6, bsz=80, num_updates=1270, lr=4.9843e-06, gnorm=1.668, clip=60, loss_scale=128, train_wall=86, gb_free=6.8, wall=16103
2022-05-18 22:06:45 - progress_bar.py[line:274] - INFO: epoch 001:   1282 / 7081 loss=-0.005, score=1.461, ntokens=888.8, nsentences=80, sample_size=888.8, wps=102.4, ups=0.12, wpb=888.8, bsz=80, num_updates=1280, lr=4.99856e-06, gnorm=1.141, clip=60, loss_scale=128, train_wall=87, gb_free=6.8, wall=16190
2022-05-18 22:08:12 - progress_bar.py[line:274] - INFO: epoch 001:   1292 / 7081 loss=-0.008, score=1.343, ntokens=891, nsentences=80, sample_size=891, wps=102.3, ups=0.11, wpb=891, bsz=80, num_updates=1290, lr=4.99615e-06, gnorm=1.387, clip=100, loss_scale=128, train_wall=87, gb_free=6.8, wall=16277
2022-05-18 22:09:38 - progress_bar.py[line:274] - INFO: epoch 001:   1302 / 7081 loss=-0.005, score=1.381, ntokens=869.1, nsentences=80, sample_size=869.1, wps=100.8, ups=0.12, wpb=869.1, bsz=80, num_updates=1300, lr=4.99375e-06, gnorm=1.497, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=16363
2022-05-18 22:11:05 - progress_bar.py[line:274] - INFO: epoch 001:   1312 / 7081 loss=-0.005, score=1.291, ntokens=883.5, nsentences=80, sample_size=883.5, wps=101.5, ups=0.11, wpb=883.5, bsz=80, num_updates=1310, lr=4.99135e-06, gnorm=0.937, clip=50, loss_scale=128, train_wall=87, gb_free=6.8, wall=16450
2022-05-18 22:12:32 - progress_bar.py[line:274] - INFO: epoch 001:   1322 / 7081 loss=-0.006, score=1.175, ntokens=882, nsentences=80, sample_size=882, wps=102.1, ups=0.12, wpb=882, bsz=80, num_updates=1320, lr=4.98894e-06, gnorm=1.473, clip=60, loss_scale=128, train_wall=86, gb_free=6.8, wall=16537
2022-05-18 22:13:58 - progress_bar.py[line:274] - INFO: epoch 001:   1332 / 7081 loss=-0.002, score=1.283, ntokens=877.5, nsentences=80, sample_size=877.5, wps=101.5, ups=0.12, wpb=877.5, bsz=80, num_updates=1330, lr=4.98654e-06, gnorm=1.441, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=16623
2022-05-18 22:15:25 - progress_bar.py[line:274] - INFO: epoch 001:   1342 / 7081 loss=-0.004, score=1.303, ntokens=880, nsentences=80, sample_size=880, wps=101.3, ups=0.12, wpb=880, bsz=80, num_updates=1340, lr=4.98414e-06, gnorm=1.523, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=16710
2022-05-18 22:16:51 - progress_bar.py[line:274] - INFO: epoch 001:   1352 / 7081 loss=-0.005, score=1.361, ntokens=878.5, nsentences=80, sample_size=878.5, wps=102.5, ups=0.12, wpb=878.5, bsz=80, num_updates=1350, lr=4.98173e-06, gnorm=1.385, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=16796
2022-05-18 22:18:17 - progress_bar.py[line:274] - INFO: epoch 001:   1362 / 7081 loss=-0.008, score=1.381, ntokens=878.1, nsentences=80, sample_size=878.1, wps=101.9, ups=0.12, wpb=878.1, bsz=80, num_updates=1360, lr=4.97933e-06, gnorm=1.328, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=16882
2022-05-18 22:19:44 - progress_bar.py[line:274] - INFO: epoch 001:   1372 / 7081 loss=-0.006, score=1.311, ntokens=884.9, nsentences=80, sample_size=884.9, wps=102, ups=0.12, wpb=884.9, bsz=80, num_updates=1370, lr=4.97692e-06, gnorm=1.441, clip=60, loss_scale=128, train_wall=87, gb_free=6.8, wall=16969
2022-05-18 22:20:10 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-18 22:21:19 - progress_bar.py[line:274] - INFO: epoch 001:   1383 / 7081 loss=-0.009, score=1.423, ntokens=877.2, nsentences=80, sample_size=877.2, wps=92.1, ups=0.1, wpb=877.2, bsz=80, num_updates=1380, lr=4.97452e-06, gnorm=1.421, clip=80, loss_scale=64, train_wall=95, gb_free=6.8, wall=17064
2022-05-18 22:22:45 - progress_bar.py[line:274] - INFO: epoch 001:   1393 / 7081 loss=-0.008, score=1.333, ntokens=879.7, nsentences=80, sample_size=879.7, wps=102.1, ups=0.12, wpb=879.7, bsz=80, num_updates=1390, lr=4.97212e-06, gnorm=1.547, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=17150
2022-05-18 22:24:12 - progress_bar.py[line:274] - INFO: epoch 001:   1403 / 7081 loss=-0.007, score=1.374, ntokens=883.7, nsentences=80, sample_size=883.7, wps=101.4, ups=0.11, wpb=883.7, bsz=80, num_updates=1400, lr=4.96971e-06, gnorm=1.588, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=17237
2022-05-18 22:25:39 - progress_bar.py[line:274] - INFO: epoch 001:   1413 / 7081 loss=-0.009, score=1.356, ntokens=886.5, nsentences=80, sample_size=886.5, wps=102.8, ups=0.12, wpb=886.5, bsz=80, num_updates=1410, lr=4.96731e-06, gnorm=1.505, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=17323
2022-05-18 22:27:05 - progress_bar.py[line:274] - INFO: epoch 001:   1423 / 7081 loss=-0.006, score=1.434, ntokens=888.2, nsentences=80, sample_size=888.2, wps=102.3, ups=0.12, wpb=888.2, bsz=80, num_updates=1420, lr=4.96491e-06, gnorm=1.251, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=17410
2022-05-18 22:28:32 - progress_bar.py[line:274] - INFO: epoch 001:   1433 / 7081 loss=-0.006, score=1.36, ntokens=873.7, nsentences=80, sample_size=873.7, wps=101, ups=0.12, wpb=873.7, bsz=80, num_updates=1430, lr=4.9625e-06, gnorm=1.116, clip=50, loss_scale=64, train_wall=86, gb_free=6.8, wall=17497
2022-05-18 22:29:58 - progress_bar.py[line:274] - INFO: epoch 001:   1443 / 7081 loss=-0.004, score=1.418, ntokens=869.5, nsentences=80, sample_size=869.5, wps=101.2, ups=0.12, wpb=869.5, bsz=80, num_updates=1440, lr=4.9601e-06, gnorm=1.535, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=17583
2022-05-18 22:31:25 - progress_bar.py[line:274] - INFO: epoch 001:   1453 / 7081 loss=-0.005, score=1.412, ntokens=881.7, nsentences=80, sample_size=881.7, wps=101.4, ups=0.12, wpb=881.7, bsz=80, num_updates=1450, lr=4.95769e-06, gnorm=1.334, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=17670
2022-05-18 22:32:51 - progress_bar.py[line:274] - INFO: epoch 001:   1463 / 7081 loss=-0.004, score=1.417, ntokens=872.2, nsentences=80, sample_size=872.2, wps=101.5, ups=0.12, wpb=872.2, bsz=80, num_updates=1460, lr=4.95529e-06, gnorm=1.591, clip=100, loss_scale=64, train_wall=86, gb_free=6.8, wall=17756
2022-05-18 22:34:17 - progress_bar.py[line:274] - INFO: epoch 001:   1473 / 7081 loss=-0.005, score=1.321, ntokens=868.8, nsentences=80, sample_size=868.8, wps=100.8, ups=0.12, wpb=868.8, bsz=80, num_updates=1470, lr=4.95289e-06, gnorm=1.321, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=17842
2022-05-18 22:35:44 - progress_bar.py[line:274] - INFO: epoch 001:   1483 / 7081 loss=-0.005, score=1.475, ntokens=888.1, nsentences=80, sample_size=888.1, wps=102.1, ups=0.12, wpb=888.1, bsz=80, num_updates=1480, lr=4.95048e-06, gnorm=1.133, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=17929
2022-05-18 22:37:10 - progress_bar.py[line:274] - INFO: epoch 001:   1493 / 7081 loss=-0.004, score=1.307, ntokens=875.5, nsentences=80, sample_size=875.5, wps=101.4, ups=0.12, wpb=875.5, bsz=80, num_updates=1490, lr=4.94808e-06, gnorm=1.264, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=18015
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-18 22:38:37 - progress_bar.py[line:274] - INFO: epoch 001:   1503 / 7081 loss=-0.006, score=1.441, ntokens=878.7, nsentences=80, sample_size=878.7, wps=101.2, ups=0.12, wpb=878.7, bsz=80, num_updates=1500, lr=4.94568e-06, gnorm=2.049, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=18102
2022-05-18 22:38:37 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-05-18 23:20:52 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.335 | ntokens 111.081 | nsentences 10 | sample_size 111.081 | cider 1.429 | wps 109.5 | wpb 111.1 | bsz 10 | num_updates 1500 | best_cider 1.429
2022-05-18 23:20:52 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 1500 updates
2022-05-18 23:20:52 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_1500.pt
2022-05-18 23:21:01 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_1500.pt
2022-05-18 23:22:51 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_1500.pt (epoch 1 @ 1500 updates, score 1.429) (writing took 118.69181824405678 seconds)
2022-05-18 23:24:16 - progress_bar.py[line:274] - INFO: epoch 001:   1513 / 7081 loss=-0.005, score=1.326, ntokens=885.7, nsentences=80, sample_size=885.7, wps=3.2, ups=0, wpb=885.7, bsz=80, num_updates=1510, lr=4.94327e-06, gnorm=1.418, clip=80, loss_scale=64, train_wall=85, gb_free=6.8, wall=20841
2022-05-18 23:25:42 - progress_bar.py[line:274] - INFO: epoch 001:   1523 / 7081 loss=-0.003, score=1.372, ntokens=874.7, nsentences=80, sample_size=874.7, wps=102, ups=0.12, wpb=874.7, bsz=80, num_updates=1520, lr=4.94087e-06, gnorm=1.267, clip=50, loss_scale=64, train_wall=86, gb_free=6.8, wall=20927
2022-05-18 23:27:09 - progress_bar.py[line:274] - INFO: epoch 001:   1533 / 7081 loss=-0.007, score=1.334, ntokens=874, nsentences=80, sample_size=874, wps=101.1, ups=0.12, wpb=874, bsz=80, num_updates=1530, lr=4.93846e-06, gnorm=1.312, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=21013
2022-05-18 23:28:35 - progress_bar.py[line:274] - INFO: epoch 001:   1543 / 7081 loss=-0.005, score=1.439, ntokens=879.4, nsentences=80, sample_size=879.4, wps=101.5, ups=0.12, wpb=879.4, bsz=80, num_updates=1540, lr=4.93606e-06, gnorm=1.393, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=21100
2022-05-18 23:30:02 - progress_bar.py[line:274] - INFO: epoch 001:   1553 / 7081 loss=-0.004, score=1.29, ntokens=887.5, nsentences=80, sample_size=887.5, wps=101.8, ups=0.11, wpb=887.5, bsz=80, num_updates=1550, lr=4.93366e-06, gnorm=1.271, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=21187
2022-05-18 23:31:29 - progress_bar.py[line:274] - INFO: epoch 001:   1563 / 7081 loss=-0.005, score=1.401, ntokens=893.4, nsentences=80, sample_size=893.4, wps=102.9, ups=0.12, wpb=893.4, bsz=80, num_updates=1560, lr=4.93125e-06, gnorm=1.038, clip=30, loss_scale=64, train_wall=87, gb_free=6.8, wall=21274
2022-05-18 23:32:56 - progress_bar.py[line:274] - INFO: epoch 001:   1573 / 7081 loss=-0.004, score=1.381, ntokens=881.7, nsentences=80, sample_size=881.7, wps=101.8, ups=0.12, wpb=881.7, bsz=80, num_updates=1570, lr=4.92885e-06, gnorm=1.261, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=21361
2022-05-18 23:34:22 - progress_bar.py[line:274] - INFO: epoch 001:   1583 / 7081 loss=-0.006, score=1.414, ntokens=874.2, nsentences=80, sample_size=874.2, wps=101.6, ups=0.12, wpb=874.2, bsz=80, num_updates=1580, lr=4.92645e-06, gnorm=1.526, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=21447
2022-05-18 23:35:49 - progress_bar.py[line:274] - INFO: epoch 001:   1593 / 7081 loss=-0.004, score=1.308, ntokens=880.1, nsentences=80, sample_size=880.1, wps=101.3, ups=0.12, wpb=880.1, bsz=80, num_updates=1590, lr=4.92404e-06, gnorm=1.143, clip=50, loss_scale=64, train_wall=87, gb_free=6.8, wall=21534
2022-05-18 23:37:16 - progress_bar.py[line:274] - INFO: epoch 001:   1603 / 7081 loss=-0.005, score=1.332, ntokens=876.6, nsentences=80, sample_size=876.6, wps=101, ups=0.12, wpb=876.6, bsz=80, num_updates=1600, lr=4.92164e-06, gnorm=1.404, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=21620
2022-05-18 23:38:41 - progress_bar.py[line:274] - INFO: epoch 001:   1613 / 7081 loss=-0.005, score=1.373, ntokens=861.3, nsentences=80, sample_size=861.3, wps=100.3, ups=0.12, wpb=861.3, bsz=80, num_updates=1610, lr=4.91923e-06, gnorm=1.42, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=21706
2022-05-18 23:40:08 - progress_bar.py[line:274] - INFO: epoch 001:   1623 / 7081 loss=-0.007, score=1.335, ntokens=867.8, nsentences=80, sample_size=867.8, wps=100.7, ups=0.12, wpb=867.8, bsz=80, num_updates=1620, lr=4.91683e-06, gnorm=1.183, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=21792
2022-05-18 23:41:34 - progress_bar.py[line:274] - INFO: epoch 001:   1633 / 7081 loss=-0.005, score=1.371, ntokens=876.6, nsentences=80, sample_size=876.6, wps=101.3, ups=0.12, wpb=876.6, bsz=80, num_updates=1630, lr=4.91443e-06, gnorm=1.407, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=21879
2022-05-18 23:43:01 - progress_bar.py[line:274] - INFO: epoch 001:   1643 / 7081 loss=-0.006, score=1.345, ntokens=888.2, nsentences=80, sample_size=888.2, wps=102.5, ups=0.12, wpb=888.2, bsz=80, num_updates=1640, lr=4.91202e-06, gnorm=1.353, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=21966
2022-05-18 23:44:27 - progress_bar.py[line:274] - INFO: epoch 001:   1653 / 7081 loss=-0.006, score=1.352, ntokens=877.6, nsentences=80, sample_size=877.6, wps=101.2, ups=0.12, wpb=877.6, bsz=80, num_updates=1650, lr=4.90962e-06, gnorm=1.294, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=22052
2022-05-18 23:45:54 - progress_bar.py[line:274] - INFO: epoch 001:   1663 / 7081 loss=-0.003, score=1.393, ntokens=880, nsentences=80, sample_size=880, wps=101.2, ups=0.11, wpb=880, bsz=80, num_updates=1660, lr=4.90722e-06, gnorm=2.13, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=22139
2022-05-18 23:47:21 - progress_bar.py[line:274] - INFO: epoch 001:   1673 / 7081 loss=-0.006, score=1.302, ntokens=887.1, nsentences=80, sample_size=887.1, wps=102.5, ups=0.12, wpb=887.1, bsz=80, num_updates=1670, lr=4.90481e-06, gnorm=1.267, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=22226
2022-05-18 23:48:47 - progress_bar.py[line:274] - INFO: epoch 001:   1683 / 7081 loss=-0.006, score=1.453, ntokens=878.1, nsentences=80, sample_size=878.1, wps=101.7, ups=0.12, wpb=878.1, bsz=80, num_updates=1680, lr=4.90241e-06, gnorm=1.473, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=22312
2022-05-18 23:50:14 - progress_bar.py[line:274] - INFO: epoch 001:   1693 / 7081 loss=-0.004, score=1.321, ntokens=877.8, nsentences=80, sample_size=877.8, wps=101.8, ups=0.12, wpb=877.8, bsz=80, num_updates=1690, lr=4.90001e-06, gnorm=1.826, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=22398
2022-05-18 23:51:40 - progress_bar.py[line:274] - INFO: epoch 001:   1703 / 7081 loss=-0.003, score=1.295, ntokens=874.3, nsentences=80, sample_size=874.3, wps=100.9, ups=0.12, wpb=874.3, bsz=80, num_updates=1700, lr=4.8976e-06, gnorm=1.498, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=22485
2022-05-18 23:53:08 - progress_bar.py[line:274] - INFO: epoch 001:   1713 / 7081 loss=-0.004, score=1.408, ntokens=887.7, nsentences=80, sample_size=887.7, wps=101.6, ups=0.11, wpb=887.7, bsz=80, num_updates=1710, lr=4.8952e-06, gnorm=1.505, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=22572
2022-05-18 23:54:34 - progress_bar.py[line:274] - INFO: epoch 001:   1723 / 7081 loss=-0.004, score=1.32, ntokens=886.8, nsentences=80, sample_size=886.8, wps=102.2, ups=0.12, wpb=886.8, bsz=80, num_updates=1720, lr=4.89279e-06, gnorm=1.952, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=22659
2022-05-18 23:56:02 - progress_bar.py[line:274] - INFO: epoch 001:   1733 / 7081 loss=-0.004, score=1.349, ntokens=896, nsentences=80, sample_size=896, wps=102.8, ups=0.11, wpb=896, bsz=80, num_updates=1730, lr=4.89039e-06, gnorm=1.485, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=22746
2022-05-18 23:57:28 - progress_bar.py[line:274] - INFO: epoch 001:   1743 / 7081 loss=-0.004, score=1.379, ntokens=888.7, nsentences=80, sample_size=888.7, wps=102.5, ups=0.12, wpb=888.7, bsz=80, num_updates=1740, lr=4.88799e-06, gnorm=1.582, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=22833
2022-05-18 23:58:55 - progress_bar.py[line:274] - INFO: epoch 001:   1753 / 7081 loss=-0.006, score=1.432, ntokens=877.5, nsentences=80, sample_size=877.5, wps=101.3, ups=0.12, wpb=877.5, bsz=80, num_updates=1750, lr=4.88558e-06, gnorm=1.237, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=22920
2022-05-19 00:00:22 - progress_bar.py[line:274] - INFO: epoch 001:   1763 / 7081 loss=-0.004, score=1.444, ntokens=884.6, nsentences=80, sample_size=884.6, wps=101.6, ups=0.11, wpb=884.6, bsz=80, num_updates=1760, lr=4.88318e-06, gnorm=1.615, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=23007
2022-05-19 00:01:49 - progress_bar.py[line:274] - INFO: epoch 001:   1773 / 7081 loss=-0.007, score=1.375, ntokens=890.5, nsentences=80, sample_size=890.5, wps=102.6, ups=0.12, wpb=890.5, bsz=80, num_updates=1770, lr=4.88078e-06, gnorm=1.398, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=23094
2022-05-19 00:03:15 - progress_bar.py[line:274] - INFO: epoch 001:   1783 / 7081 loss=-0.006, score=1.288, ntokens=885.6, nsentences=80, sample_size=885.6, wps=102.2, ups=0.12, wpb=885.6, bsz=80, num_updates=1780, lr=4.87837e-06, gnorm=1.556, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=23180
2022-05-19 00:04:43 - progress_bar.py[line:274] - INFO: epoch 001:   1793 / 7081 loss=-0.007, score=1.36, ntokens=890.8, nsentences=80, sample_size=890.8, wps=102.2, ups=0.11, wpb=890.8, bsz=80, num_updates=1790, lr=4.87597e-06, gnorm=1.317, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=23267
2022-05-19 00:06:10 - progress_bar.py[line:274] - INFO: epoch 001:   1803 / 7081 loss=-0.003, score=1.378, ntokens=897.9, nsentences=80, sample_size=897.9, wps=102.3, ups=0.11, wpb=897.9, bsz=80, num_updates=1800, lr=4.87356e-06, gnorm=1.532, clip=70, loss_scale=64, train_wall=88, gb_free=6.8, wall=23355
2022-05-19 00:07:37 - progress_bar.py[line:274] - INFO: epoch 001:   1813 / 7081 loss=-0.005, score=1.345, ntokens=894.4, nsentences=80, sample_size=894.4, wps=102.8, ups=0.11, wpb=894.4, bsz=80, num_updates=1810, lr=4.87116e-06, gnorm=1.777, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=23442
2022-05-19 00:09:04 - progress_bar.py[line:274] - INFO: epoch 001:   1823 / 7081 loss=-0.006, score=1.31, ntokens=881.7, nsentences=80, sample_size=881.7, wps=101.5, ups=0.12, wpb=881.7, bsz=80, num_updates=1820, lr=4.86876e-06, gnorm=1.212, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=23529
2022-05-19 00:10:32 - progress_bar.py[line:274] - INFO: epoch 001:   1833 / 7081 loss=-0.003, score=1.271, ntokens=885.1, nsentences=80, sample_size=885.1, wps=101.4, ups=0.11, wpb=885.1, bsz=80, num_updates=1830, lr=4.86635e-06, gnorm=1.609, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=23616
2022-05-19 00:11:58 - progress_bar.py[line:274] - INFO: epoch 001:   1843 / 7081 loss=-0.005, score=1.462, ntokens=878.7, nsentences=80, sample_size=878.7, wps=101.9, ups=0.12, wpb=878.7, bsz=80, num_updates=1840, lr=4.86395e-06, gnorm=1.754, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=23703
2022-05-19 00:13:25 - progress_bar.py[line:274] - INFO: epoch 001:   1853 / 7081 loss=-0.005, score=1.378, ntokens=896.5, nsentences=80, sample_size=896.5, wps=102.9, ups=0.11, wpb=896.5, bsz=80, num_updates=1850, lr=4.86155e-06, gnorm=1.486, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=23790
2022-05-19 00:14:52 - progress_bar.py[line:274] - INFO: epoch 001:   1863 / 7081 loss=-0.007, score=1.467, ntokens=885.3, nsentences=80, sample_size=885.3, wps=101.7, ups=0.11, wpb=885.3, bsz=80, num_updates=1860, lr=4.85914e-06, gnorm=1.464, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=23877
2022-05-19 00:16:19 - progress_bar.py[line:274] - INFO: epoch 001:   1873 / 7081 loss=-0.006, score=1.391, ntokens=882.7, nsentences=80, sample_size=882.7, wps=101.4, ups=0.11, wpb=882.7, bsz=80, num_updates=1870, lr=4.85674e-06, gnorm=1.27, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=23964
2022-05-19 00:17:46 - progress_bar.py[line:274] - INFO: epoch 001:   1883 / 7081 loss=-0.006, score=1.444, ntokens=897.4, nsentences=80, sample_size=897.4, wps=103.2, ups=0.12, wpb=897.4, bsz=80, num_updates=1880, lr=4.85433e-06, gnorm=1.376, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=24051
2022-05-19 00:19:13 - progress_bar.py[line:274] - INFO: epoch 001:   1893 / 7081 loss=-0.005, score=1.308, ntokens=879.9, nsentences=80, sample_size=879.9, wps=101.4, ups=0.12, wpb=879.9, bsz=80, num_updates=1890, lr=4.85193e-06, gnorm=1.473, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=24137
2022-05-19 00:20:40 - progress_bar.py[line:274] - INFO: epoch 001:   1903 / 7081 loss=-0.002, score=1.341, ntokens=892.8, nsentences=80, sample_size=892.8, wps=102.3, ups=0.11, wpb=892.8, bsz=80, num_updates=1900, lr=4.84953e-06, gnorm=1.453, clip=60, loss_scale=128, train_wall=87, gb_free=6.8, wall=24225
2022-05-19 00:22:07 - progress_bar.py[line:274] - INFO: epoch 001:   1913 / 7081 loss=-0.004, score=1.397, ntokens=882.3, nsentences=80, sample_size=882.3, wps=101.3, ups=0.11, wpb=882.3, bsz=80, num_updates=1910, lr=4.84712e-06, gnorm=1.72, clip=70, loss_scale=128, train_wall=87, gb_free=6.8, wall=24312
2022-05-19 00:23:33 - progress_bar.py[line:274] - INFO: epoch 001:   1923 / 7081 loss=-0.008, score=1.329, ntokens=874.2, nsentences=80, sample_size=874.2, wps=101.5, ups=0.12, wpb=874.2, bsz=80, num_updates=1920, lr=4.84472e-06, gnorm=1.514, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=24398
2022-05-19 00:25:00 - progress_bar.py[line:274] - INFO: epoch 001:   1933 / 7081 loss=-0.003, score=1.433, ntokens=882.8, nsentences=80, sample_size=882.8, wps=101.2, ups=0.11, wpb=882.8, bsz=80, num_updates=1930, lr=4.84232e-06, gnorm=1.576, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=24485
2022-05-19 00:26:27 - progress_bar.py[line:274] - INFO: epoch 001:   1943 / 7081 loss=-0.005, score=1.331, ntokens=883.7, nsentences=80, sample_size=883.7, wps=102.3, ups=0.12, wpb=883.7, bsz=80, num_updates=1940, lr=4.83991e-06, gnorm=1.803, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=24572
2022-05-19 00:27:54 - progress_bar.py[line:274] - INFO: epoch 001:   1953 / 7081 loss=-0.006, score=1.385, ntokens=887.7, nsentences=80, sample_size=887.7, wps=102.1, ups=0.12, wpb=887.7, bsz=80, num_updates=1950, lr=4.83751e-06, gnorm=1.31, clip=70, loss_scale=128, train_wall=87, gb_free=6.8, wall=24659
2022-05-19 00:29:21 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-19 00:29:30 - progress_bar.py[line:274] - INFO: epoch 001:   1964 / 7081 loss=-0.006, score=1.295, ntokens=898.6, nsentences=80, sample_size=898.6, wps=93.3, ups=0.1, wpb=898.6, bsz=80, num_updates=1960, lr=4.8351e-06, gnorm=1.842, clip=100, loss_scale=64, train_wall=96, gb_free=6.8, wall=24755
2022-05-19 00:30:58 - progress_bar.py[line:274] - INFO: epoch 001:   1974 / 7081 loss=-0.006, score=1.325, ntokens=897.7, nsentences=80, sample_size=897.7, wps=102.7, ups=0.11, wpb=897.7, bsz=80, num_updates=1970, lr=4.8327e-06, gnorm=1.626, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=24842
2022-05-19 00:32:25 - progress_bar.py[line:274] - INFO: epoch 001:   1984 / 7081 loss=-0.006, score=1.46, ntokens=888.8, nsentences=80, sample_size=888.8, wps=102, ups=0.11, wpb=888.8, bsz=80, num_updates=1980, lr=4.8303e-06, gnorm=1.948, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=24929
2022-05-19 00:33:52 - progress_bar.py[line:274] - INFO: epoch 001:   1994 / 7081 loss=-0.007, score=1.492, ntokens=891.6, nsentences=80, sample_size=891.6, wps=102.3, ups=0.11, wpb=891.6, bsz=80, num_updates=1990, lr=4.82789e-06, gnorm=1.533, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=25017
2022-05-19 00:35:18 - progress_bar.py[line:274] - INFO: epoch 001:   2004 / 7081 loss=-0.004, score=1.395, ntokens=881.1, nsentences=80, sample_size=881.1, wps=101.8, ups=0.12, wpb=881.1, bsz=80, num_updates=2000, lr=4.82549e-06, gnorm=1.915, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=25103
2022-05-19 00:35:18 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-05-19 01:18:15 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.338 | ntokens 112.993 | nsentences 10 | sample_size 112.993 | cider 1.433 | wps 109.6 | wpb 113 | bsz 10 | num_updates 2000 | best_cider 1.433
2022-05-19 01:18:15 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2022-05-19 01:18:15 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_2000.pt
2022-05-19 01:18:24 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_2000.pt
2022-05-19 01:19:55 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 1.433) (writing took 100.06810809206218 seconds)
2022-05-19 01:21:21 - progress_bar.py[line:274] - INFO: epoch 001:   2014 / 7081 loss=-0.005, score=1.352, ntokens=893.8, nsentences=80, sample_size=893.8, wps=3.2, ups=0, wpb=893.8, bsz=80, num_updates=2010, lr=4.82309e-06, gnorm=1.253, clip=70, loss_scale=64, train_wall=85, gb_free=6.8, wall=27866
2022-05-19 01:22:48 - progress_bar.py[line:274] - INFO: epoch 001:   2024 / 7081 loss=-0.005, score=1.352, ntokens=896.7, nsentences=80, sample_size=896.7, wps=102.8, ups=0.11, wpb=896.7, bsz=80, num_updates=2020, lr=4.82068e-06, gnorm=1.238, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=27953
2022-05-19 01:24:15 - progress_bar.py[line:274] - INFO: epoch 001:   2034 / 7081 loss=-0.004, score=1.278, ntokens=888.2, nsentences=80, sample_size=888.2, wps=102.4, ups=0.12, wpb=888.2, bsz=80, num_updates=2030, lr=4.81828e-06, gnorm=1.238, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=28040
2022-05-19 01:25:42 - progress_bar.py[line:274] - INFO: epoch 001:   2044 / 7081 loss=-0.006, score=1.373, ntokens=883.5, nsentences=80, sample_size=883.5, wps=101.3, ups=0.11, wpb=883.5, bsz=80, num_updates=2040, lr=4.81587e-06, gnorm=1.373, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=28127
2022-05-19 01:27:09 - progress_bar.py[line:274] - INFO: epoch 001:   2054 / 7081 loss=-0.005, score=1.414, ntokens=890.9, nsentences=80, sample_size=890.9, wps=102.7, ups=0.12, wpb=890.9, bsz=80, num_updates=2050, lr=4.81347e-06, gnorm=1.226, clip=50, loss_scale=64, train_wall=87, gb_free=6.8, wall=28214
2022-05-19 01:28:36 - progress_bar.py[line:274] - INFO: epoch 001:   2064 / 7081 loss=-0.004, score=1.373, ntokens=902.2, nsentences=80, sample_size=902.2, wps=103.1, ups=0.11, wpb=902.2, bsz=80, num_updates=2060, lr=4.81107e-06, gnorm=1.471, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=28301
2022-05-19 01:30:03 - progress_bar.py[line:274] - INFO: epoch 001:   2074 / 7081 loss=-0.007, score=1.278, ntokens=876.9, nsentences=80, sample_size=876.9, wps=101.7, ups=0.12, wpb=876.9, bsz=80, num_updates=2070, lr=4.80866e-06, gnorm=1.42, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=28388
2022-05-19 01:31:30 - progress_bar.py[line:274] - INFO: epoch 001:   2084 / 7081 loss=-0.008, score=1.392, ntokens=890.4, nsentences=80, sample_size=890.4, wps=102.2, ups=0.11, wpb=890.4, bsz=80, num_updates=2080, lr=4.80626e-06, gnorm=1.769, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=28475
2022-05-19 01:32:57 - progress_bar.py[line:274] - INFO: epoch 001:   2094 / 7081 loss=-0.004, score=1.336, ntokens=874.8, nsentences=80, sample_size=874.8, wps=100.1, ups=0.11, wpb=874.8, bsz=80, num_updates=2090, lr=4.80386e-06, gnorm=1.547, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=28562
2022-05-19 01:34:25 - progress_bar.py[line:274] - INFO: epoch 001:   2104 / 7081 loss=-0.009, score=1.425, ntokens=905.4, nsentences=80, sample_size=905.4, wps=103, ups=0.11, wpb=905.4, bsz=80, num_updates=2100, lr=4.80145e-06, gnorm=1.573, clip=80, loss_scale=64, train_wall=88, gb_free=6.8, wall=28650
2022-05-19 01:35:52 - progress_bar.py[line:274] - INFO: epoch 001:   2114 / 7081 loss=-0.004, score=1.341, ntokens=890.2, nsentences=80, sample_size=890.2, wps=102.7, ups=0.12, wpb=890.2, bsz=80, num_updates=2110, lr=4.79905e-06, gnorm=1.431, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=28737
2022-05-19 01:37:18 - progress_bar.py[line:274] - INFO: epoch 001:   2124 / 7081 loss=-0.004, score=1.41, ntokens=885.3, nsentences=80, sample_size=885.3, wps=102.1, ups=0.12, wpb=885.3, bsz=80, num_updates=2120, lr=4.79664e-06, gnorm=1.393, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=28823
2022-05-19 01:38:45 - progress_bar.py[line:274] - INFO: epoch 001:   2134 / 7081 loss=-0.003, score=1.36, ntokens=898.9, nsentences=80, sample_size=898.9, wps=103.7, ups=0.12, wpb=898.9, bsz=80, num_updates=2130, lr=4.79424e-06, gnorm=1.353, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=28910
2022-05-19 01:40:12 - progress_bar.py[line:274] - INFO: epoch 001:   2144 / 7081 loss=-0.005, score=1.368, ntokens=886.7, nsentences=80, sample_size=886.7, wps=102, ups=0.12, wpb=886.7, bsz=80, num_updates=2140, lr=4.79184e-06, gnorm=1.504, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=28997
2022-05-19 01:41:39 - progress_bar.py[line:274] - INFO: epoch 001:   2154 / 7081 loss=-0.004, score=1.325, ntokens=895.6, nsentences=80, sample_size=895.6, wps=103.2, ups=0.12, wpb=895.6, bsz=80, num_updates=2150, lr=4.78943e-06, gnorm=1.191, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=29084
2022-05-19 01:43:06 - progress_bar.py[line:274] - INFO: epoch 001:   2164 / 7081 loss=-0.007, score=1.286, ntokens=882.1, nsentences=80, sample_size=882.1, wps=101.2, ups=0.11, wpb=882.1, bsz=80, num_updates=2160, lr=4.78703e-06, gnorm=1.304, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=29171
2022-05-19 01:44:32 - progress_bar.py[line:274] - INFO: epoch 001:   2174 / 7081 loss=-0.005, score=1.536, ntokens=898.5, nsentences=80, sample_size=898.5, wps=103.9, ups=0.12, wpb=898.5, bsz=80, num_updates=2170, lr=4.78463e-06, gnorm=1.322, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=29257
2022-05-19 01:45:59 - progress_bar.py[line:274] - INFO: epoch 001:   2184 / 7081 loss=-0.004, score=1.308, ntokens=886.2, nsentences=80, sample_size=886.2, wps=102.1, ups=0.12, wpb=886.2, bsz=80, num_updates=2180, lr=4.78222e-06, gnorm=1.134, clip=50, loss_scale=64, train_wall=87, gb_free=6.8, wall=29344
2022-05-19 01:47:26 - progress_bar.py[line:274] - INFO: epoch 001:   2194 / 7081 loss=-0.004, score=1.455, ntokens=895.7, nsentences=80, sample_size=895.7, wps=103.4, ups=0.12, wpb=895.7, bsz=80, num_updates=2190, lr=4.77982e-06, gnorm=1.643, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=29431
2022-05-19 01:48:53 - progress_bar.py[line:274] - INFO: epoch 001:   2204 / 7081 loss=-0.005, score=1.391, ntokens=888.7, nsentences=80, sample_size=888.7, wps=102.5, ups=0.12, wpb=888.7, bsz=80, num_updates=2200, lr=4.77741e-06, gnorm=1.825, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=29517
2022-05-19 01:50:19 - progress_bar.py[line:274] - INFO: epoch 001:   2214 / 7081 loss=-0.003, score=1.319, ntokens=894.8, nsentences=80, sample_size=894.8, wps=103.1, ups=0.12, wpb=894.8, bsz=80, num_updates=2210, lr=4.77501e-06, gnorm=1.634, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=29604
2022-05-19 01:51:47 - progress_bar.py[line:274] - INFO: epoch 001:   2224 / 7081 loss=-0.004, score=1.416, ntokens=886.7, nsentences=80, sample_size=886.7, wps=101.3, ups=0.11, wpb=886.7, bsz=80, num_updates=2220, lr=4.77261e-06, gnorm=1.329, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=29692
2022-05-19 01:53:14 - progress_bar.py[line:274] - INFO: epoch 001:   2234 / 7081 loss=-0.003, score=1.405, ntokens=885.8, nsentences=80, sample_size=885.8, wps=101.9, ups=0.12, wpb=885.8, bsz=80, num_updates=2230, lr=4.7702e-06, gnorm=1.527, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=29779
2022-05-19 01:54:41 - progress_bar.py[line:274] - INFO: epoch 001:   2244 / 7081 loss=-0.003, score=1.313, ntokens=895.6, nsentences=80, sample_size=895.6, wps=102.7, ups=0.11, wpb=895.6, bsz=80, num_updates=2240, lr=4.7678e-06, gnorm=1.055, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=29866
2022-05-19 01:56:08 - progress_bar.py[line:274] - INFO: epoch 001:   2254 / 7081 loss=-0.007, score=1.467, ntokens=890.7, nsentences=80, sample_size=890.7, wps=102.7, ups=0.12, wpb=890.7, bsz=80, num_updates=2250, lr=4.7654e-06, gnorm=1.701, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=29953
2022-05-19 01:57:35 - progress_bar.py[line:274] - INFO: epoch 001:   2264 / 7081 loss=-0.007, score=1.439, ntokens=891.9, nsentences=80, sample_size=891.9, wps=102.5, ups=0.11, wpb=891.9, bsz=80, num_updates=2260, lr=4.76299e-06, gnorm=1.697, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=30040
2022-05-19 01:59:02 - progress_bar.py[line:274] - INFO: epoch 001:   2274 / 7081 loss=-0.004, score=1.312, ntokens=903.3, nsentences=80, sample_size=903.3, wps=103.3, ups=0.11, wpb=903.3, bsz=80, num_updates=2270, lr=4.76059e-06, gnorm=1.123, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=30127
2022-05-19 02:00:29 - progress_bar.py[line:274] - INFO: epoch 001:   2284 / 7081 loss=-0.006, score=1.427, ntokens=880.9, nsentences=80, sample_size=880.9, wps=101.1, ups=0.11, wpb=880.9, bsz=80, num_updates=2280, lr=4.75819e-06, gnorm=1.297, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=30214
2022-05-19 02:01:57 - progress_bar.py[line:274] - INFO: epoch 001:   2294 / 7081 loss=-0.004, score=1.355, ntokens=907.9, nsentences=80, sample_size=907.9, wps=103.5, ups=0.11, wpb=907.9, bsz=80, num_updates=2290, lr=4.75578e-06, gnorm=1.272, clip=60, loss_scale=64, train_wall=88, gb_free=6.8, wall=30302
2022-05-19 02:03:24 - progress_bar.py[line:274] - INFO: epoch 001:   2304 / 7081 loss=-0.007, score=1.376, ntokens=896.2, nsentences=80, sample_size=896.2, wps=103.7, ups=0.12, wpb=896.2, bsz=80, num_updates=2300, lr=4.75338e-06, gnorm=1.28, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=30388
2022-05-19 02:04:50 - progress_bar.py[line:274] - INFO: epoch 001:   2314 / 7081 loss=-0.006, score=1.323, ntokens=893.6, nsentences=80, sample_size=893.6, wps=103.2, ups=0.12, wpb=893.6, bsz=80, num_updates=2310, lr=4.75097e-06, gnorm=1.527, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=30475
2022-05-19 02:06:16 - progress_bar.py[line:274] - INFO: epoch 001:   2324 / 7081 loss=-0.004, score=1.391, ntokens=887.9, nsentences=80, sample_size=887.9, wps=102.9, ups=0.12, wpb=887.9, bsz=80, num_updates=2320, lr=4.74857e-06, gnorm=1.605, clip=100, loss_scale=64, train_wall=86, gb_free=6.8, wall=30561
2022-05-19 02:07:43 - progress_bar.py[line:274] - INFO: epoch 001:   2334 / 7081 loss=-0.005, score=1.42, ntokens=888.4, nsentences=80, sample_size=888.4, wps=102.5, ups=0.12, wpb=888.4, bsz=80, num_updates=2330, lr=4.74617e-06, gnorm=1.962, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=30648
2022-05-19 02:09:10 - progress_bar.py[line:274] - INFO: epoch 001:   2344 / 7081 loss=-0.004, score=1.451, ntokens=893.1, nsentences=80, sample_size=893.1, wps=102.7, ups=0.11, wpb=893.1, bsz=80, num_updates=2340, lr=4.74376e-06, gnorm=1.322, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=30735
2022-05-19 02:10:37 - progress_bar.py[line:274] - INFO: epoch 001:   2354 / 7081 loss=-0.006, score=1.38, ntokens=883.1, nsentences=80, sample_size=883.1, wps=101.7, ups=0.12, wpb=883.1, bsz=80, num_updates=2350, lr=4.74136e-06, gnorm=1.31, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=30822
2022-05-19 02:12:04 - progress_bar.py[line:274] - INFO: epoch 001:   2364 / 7081 loss=-0.005, score=1.361, ntokens=887.2, nsentences=80, sample_size=887.2, wps=102, ups=0.12, wpb=887.2, bsz=80, num_updates=2360, lr=4.73896e-06, gnorm=1.277, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=30909
2022-05-19 02:13:31 - progress_bar.py[line:274] - INFO: epoch 001:   2374 / 7081 loss=-0.003, score=1.344, ntokens=888.3, nsentences=80, sample_size=888.3, wps=101.9, ups=0.11, wpb=888.3, bsz=80, num_updates=2370, lr=4.73655e-06, gnorm=1.247, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=30996
2022-05-19 02:14:57 - progress_bar.py[line:274] - INFO: epoch 001:   2384 / 7081 loss=-0.006, score=1.345, ntokens=889.3, nsentences=80, sample_size=889.3, wps=102.8, ups=0.12, wpb=889.3, bsz=80, num_updates=2380, lr=4.73415e-06, gnorm=1.813, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=31082
2022-05-19 02:16:23 - progress_bar.py[line:274] - INFO: epoch 001:   2394 / 7081 loss=-0.007, score=1.417, ntokens=878.2, nsentences=80, sample_size=878.2, wps=102.1, ups=0.12, wpb=878.2, bsz=80, num_updates=2390, lr=4.73174e-06, gnorm=1.471, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=31168
2022-05-19 02:17:50 - progress_bar.py[line:274] - INFO: epoch 001:   2404 / 7081 loss=-0.003, score=1.331, ntokens=880.2, nsentences=80, sample_size=880.2, wps=101.7, ups=0.12, wpb=880.2, bsz=80, num_updates=2400, lr=4.72934e-06, gnorm=1.396, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=31255
2022-05-19 02:19:18 - progress_bar.py[line:274] - INFO: epoch 001:   2414 / 7081 loss=-0.005, score=1.272, ntokens=898.2, nsentences=80, sample_size=898.2, wps=102.5, ups=0.11, wpb=898.2, bsz=80, num_updates=2410, lr=4.72694e-06, gnorm=1.196, clip=70, loss_scale=64, train_wall=88, gb_free=6.8, wall=31342
2022-05-19 02:20:45 - progress_bar.py[line:274] - INFO: epoch 001:   2424 / 7081 loss=-0.005, score=1.407, ntokens=897.1, nsentences=80, sample_size=897.1, wps=103.3, ups=0.12, wpb=897.1, bsz=80, num_updates=2420, lr=4.72453e-06, gnorm=1.705, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=31429
2022-05-19 02:22:13 - progress_bar.py[line:274] - INFO: epoch 001:   2434 / 7081 loss=-0.008, score=1.365, ntokens=910.5, nsentences=80, sample_size=910.5, wps=103.5, ups=0.11, wpb=910.5, bsz=80, num_updates=2430, lr=4.72213e-06, gnorm=1.627, clip=90, loss_scale=64, train_wall=88, gb_free=6.8, wall=31517
2022-05-19 02:23:39 - progress_bar.py[line:274] - INFO: epoch 001:   2444 / 7081 loss=-0.002, score=1.28, ntokens=889.6, nsentences=80, sample_size=889.6, wps=102.4, ups=0.12, wpb=889.6, bsz=80, num_updates=2440, lr=4.71973e-06, gnorm=1.555, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=31604
2022-05-19 02:25:06 - progress_bar.py[line:274] - INFO: epoch 001:   2454 / 7081 loss=-0.006, score=1.351, ntokens=894.3, nsentences=80, sample_size=894.3, wps=102.8, ups=0.11, wpb=894.3, bsz=80, num_updates=2450, lr=4.71732e-06, gnorm=1.545, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=31691
2022-05-19 02:26:33 - progress_bar.py[line:274] - INFO: epoch 001:   2464 / 7081 loss=-0.003, score=1.365, ntokens=877.4, nsentences=80, sample_size=877.4, wps=101.7, ups=0.12, wpb=877.4, bsz=80, num_updates=2460, lr=4.71492e-06, gnorm=1.986, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=31777
2022-05-19 02:28:00 - progress_bar.py[line:274] - INFO: epoch 001:   2474 / 7081 loss=-0.007, score=1.458, ntokens=891.3, nsentences=80, sample_size=891.3, wps=101.7, ups=0.11, wpb=891.3, bsz=80, num_updates=2470, lr=4.71251e-06, gnorm=1.429, clip=80, loss_scale=64, train_wall=88, gb_free=6.8, wall=31865
2022-05-19 02:29:28 - progress_bar.py[line:274] - INFO: epoch 001:   2484 / 7081 loss=-0.005, score=1.309, ntokens=903.8, nsentences=80, sample_size=903.8, wps=103.5, ups=0.11, wpb=903.8, bsz=80, num_updates=2480, lr=4.71011e-06, gnorm=1.159, clip=60, loss_scale=128, train_wall=87, gb_free=6.8, wall=31952
2022-05-19 02:30:55 - progress_bar.py[line:274] - INFO: epoch 001:   2494 / 7081 loss=-0.004, score=1.366, ntokens=889.6, nsentences=80, sample_size=889.6, wps=102.3, ups=0.11, wpb=889.6, bsz=80, num_updates=2490, lr=4.70771e-06, gnorm=1.382, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=32039
2022-05-19 02:32:22 - progress_bar.py[line:274] - INFO: epoch 001:   2504 / 7081 loss=-0.004, score=1.47, ntokens=880.2, nsentences=80, sample_size=880.2, wps=101.1, ups=0.11, wpb=880.2, bsz=80, num_updates=2500, lr=4.7053e-06, gnorm=1.572, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=32126
2022-05-19 02:32:22 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-19 03:15:11 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.345 | ntokens 112.486 | nsentences 10 | sample_size 112.486 | cider 1.442 | wps 109.5 | wpb 112.5 | bsz 10 | num_updates 2500 | best_cider 1.442
2022-05-19 03:15:11 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2500 updates
2022-05-19 03:15:11 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_2500.pt
2022-05-19 03:15:19 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_2500.pt
2022-05-19 03:16:42 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_2500.pt (epoch 1 @ 2500 updates, score 1.442) (writing took 91.36036706902087 seconds)
2022-05-19 03:18:08 - progress_bar.py[line:274] - INFO: epoch 001:   2514 / 7081 loss=-0.006, score=1.286, ntokens=895.6, nsentences=80, sample_size=895.6, wps=3.3, ups=0, wpb=895.6, bsz=80, num_updates=2510, lr=4.7029e-06, gnorm=1.44, clip=80, loss_scale=128, train_wall=85, gb_free=6.8, wall=34873
2022-05-19 03:19:34 - progress_bar.py[line:274] - INFO: epoch 001:   2524 / 7081 loss=-0.007, score=1.389, ntokens=891.7, nsentences=80, sample_size=891.7, wps=103.6, ups=0.12, wpb=891.7, bsz=80, num_updates=2520, lr=4.7005e-06, gnorm=1.493, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=34959
2022-05-19 03:21:01 - progress_bar.py[line:274] - INFO: epoch 001:   2534 / 7081 loss=-0.003, score=1.42, ntokens=882.9, nsentences=80, sample_size=882.9, wps=101.6, ups=0.12, wpb=882.9, bsz=80, num_updates=2530, lr=4.69809e-06, gnorm=1.172, clip=70, loss_scale=128, train_wall=87, gb_free=6.8, wall=35046
2022-05-19 03:22:28 - progress_bar.py[line:274] - INFO: epoch 001:   2544 / 7081 loss=-0.004, score=1.365, ntokens=893.7, nsentences=80, sample_size=893.7, wps=103.2, ups=0.12, wpb=893.7, bsz=80, num_updates=2540, lr=4.69569e-06, gnorm=1.645, clip=100, loss_scale=128, train_wall=86, gb_free=6.8, wall=35133
2022-05-19 03:23:54 - progress_bar.py[line:274] - INFO: epoch 001:   2554 / 7081 loss=-0.005, score=1.543, ntokens=894.8, nsentences=80, sample_size=894.8, wps=103.5, ups=0.12, wpb=894.8, bsz=80, num_updates=2550, lr=4.69328e-06, gnorm=1.621, clip=100, loss_scale=128, train_wall=86, gb_free=6.8, wall=35219
2022-05-19 03:25:21 - progress_bar.py[line:274] - INFO: epoch 001:   2564 / 7081 loss=-0.003, score=1.361, ntokens=887.2, nsentences=80, sample_size=887.2, wps=101.7, ups=0.11, wpb=887.2, bsz=80, num_updates=2560, lr=4.69088e-06, gnorm=1.511, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=35306
2022-05-19 03:26:49 - progress_bar.py[line:274] - INFO: epoch 001:   2574 / 7081 loss=-0.004, score=1.313, ntokens=891.8, nsentences=80, sample_size=891.8, wps=102, ups=0.11, wpb=891.8, bsz=80, num_updates=2570, lr=4.68848e-06, gnorm=1.312, clip=60, loss_scale=128, train_wall=87, gb_free=6.8, wall=35394
2022-05-19 03:28:15 - progress_bar.py[line:274] - INFO: epoch 001:   2584 / 7081 loss=-0.003, score=1.321, ntokens=885.7, nsentences=80, sample_size=885.7, wps=102.3, ups=0.12, wpb=885.7, bsz=80, num_updates=2580, lr=4.68607e-06, gnorm=1.469, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=35480
2022-05-19 03:29:42 - progress_bar.py[line:274] - INFO: epoch 001:   2594 / 7081 loss=-0.007, score=1.436, ntokens=897, nsentences=80, sample_size=897, wps=103.1, ups=0.11, wpb=897, bsz=80, num_updates=2590, lr=4.68367e-06, gnorm=1.31, clip=60, loss_scale=128, train_wall=87, gb_free=6.8, wall=35567
2022-05-19 03:31:09 - progress_bar.py[line:274] - INFO: epoch 001:   2604 / 7081 loss=-0.005, score=1.473, ntokens=887.6, nsentences=80, sample_size=887.6, wps=102.1, ups=0.12, wpb=887.6, bsz=80, num_updates=2600, lr=4.68127e-06, gnorm=1.601, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=35654
2022-05-19 03:31:44 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-19 03:32:44 - progress_bar.py[line:274] - INFO: epoch 001:   2615 / 7081 loss=-0.006, score=1.428, ntokens=886.7, nsentences=80, sample_size=886.7, wps=93.4, ups=0.11, wpb=886.7, bsz=80, num_updates=2610, lr=4.67886e-06, gnorm=1.989, clip=90, loss_scale=64, train_wall=95, gb_free=6.8, wall=35749
2022-05-19 03:34:11 - progress_bar.py[line:274] - INFO: epoch 001:   2625 / 7081 loss=-0.006, score=1.4, ntokens=876.5, nsentences=80, sample_size=876.5, wps=101.2, ups=0.12, wpb=876.5, bsz=80, num_updates=2620, lr=4.67646e-06, gnorm=1.413, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=35836
2022-05-19 03:35:37 - progress_bar.py[line:274] - INFO: epoch 001:   2635 / 7081 loss=-0.003, score=1.504, ntokens=886.4, nsentences=80, sample_size=886.4, wps=102.6, ups=0.12, wpb=886.4, bsz=80, num_updates=2630, lr=4.67405e-06, gnorm=1.563, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=35922
2022-05-19 03:37:04 - progress_bar.py[line:274] - INFO: epoch 001:   2645 / 7081 loss=-0.003, score=1.28, ntokens=895.8, nsentences=80, sample_size=895.8, wps=103.6, ups=0.12, wpb=895.8, bsz=80, num_updates=2640, lr=4.67165e-06, gnorm=2.25, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=36008
2022-05-19 03:38:31 - progress_bar.py[line:274] - INFO: epoch 001:   2655 / 7081 loss=-0.007, score=1.394, ntokens=892, nsentences=80, sample_size=892, wps=102.6, ups=0.12, wpb=892, bsz=80, num_updates=2650, lr=4.66925e-06, gnorm=1.522, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=36095
2022-05-19 03:39:57 - progress_bar.py[line:274] - INFO: epoch 001:   2665 / 7081 loss=-0.005, score=1.413, ntokens=884.6, nsentences=80, sample_size=884.6, wps=102.3, ups=0.12, wpb=884.6, bsz=80, num_updates=2660, lr=4.66684e-06, gnorm=1.083, clip=60, loss_scale=64, train_wall=86, gb_free=6.8, wall=36182
2022-05-19 03:41:23 - progress_bar.py[line:274] - INFO: epoch 001:   2675 / 7081 loss=-0.002, score=1.379, ntokens=885.6, nsentences=80, sample_size=885.6, wps=102.8, ups=0.12, wpb=885.6, bsz=80, num_updates=2670, lr=4.66444e-06, gnorm=1.299, clip=60, loss_scale=64, train_wall=86, gb_free=6.8, wall=36268
2022-05-19 03:42:50 - progress_bar.py[line:274] - INFO: epoch 001:   2685 / 7081 loss=-0.004, score=1.396, ntokens=884.5, nsentences=80, sample_size=884.5, wps=102.1, ups=0.12, wpb=884.5, bsz=80, num_updates=2680, lr=4.66204e-06, gnorm=1.3, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=36355
2022-05-19 03:44:16 - progress_bar.py[line:274] - INFO: epoch 001:   2695 / 7081 loss=-0.001, score=1.371, ntokens=891.1, nsentences=80, sample_size=891.1, wps=103, ups=0.12, wpb=891.1, bsz=80, num_updates=2690, lr=4.65963e-06, gnorm=1.218, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=36441
2022-05-19 03:45:43 - progress_bar.py[line:274] - INFO: epoch 001:   2705 / 7081 loss=-0.007, score=1.404, ntokens=891.8, nsentences=80, sample_size=891.8, wps=103, ups=0.12, wpb=891.8, bsz=80, num_updates=2700, lr=4.65723e-06, gnorm=1.102, clip=40, loss_scale=64, train_wall=87, gb_free=6.8, wall=36528
2022-05-19 03:47:10 - progress_bar.py[line:274] - INFO: epoch 001:   2715 / 7081 loss=-0.006, score=1.444, ntokens=900.5, nsentences=80, sample_size=900.5, wps=103.1, ups=0.11, wpb=900.5, bsz=80, num_updates=2710, lr=4.65482e-06, gnorm=1.386, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=36615
2022-05-19 03:48:37 - progress_bar.py[line:274] - INFO: epoch 001:   2725 / 7081 loss=-0.008, score=1.376, ntokens=891, nsentences=80, sample_size=891, wps=102.8, ups=0.12, wpb=891, bsz=80, num_updates=2720, lr=4.65242e-06, gnorm=1.566, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=36702
2022-05-19 03:50:03 - progress_bar.py[line:274] - INFO: epoch 001:   2735 / 7081 loss=-0.006, score=1.432, ntokens=878.3, nsentences=80, sample_size=878.3, wps=101.7, ups=0.12, wpb=878.3, bsz=80, num_updates=2730, lr=4.65002e-06, gnorm=1.464, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=36788
2022-05-19 03:51:30 - progress_bar.py[line:274] - INFO: epoch 001:   2745 / 7081 loss=-0.002, score=1.51, ntokens=892.7, nsentences=80, sample_size=892.7, wps=102.6, ups=0.11, wpb=892.7, bsz=80, num_updates=2740, lr=4.64761e-06, gnorm=1.509, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=36875
2022-05-19 03:52:56 - progress_bar.py[line:274] - INFO: epoch 001:   2755 / 7081 loss=-0.005, score=1.361, ntokens=882.1, nsentences=80, sample_size=882.1, wps=102.5, ups=0.12, wpb=882.1, bsz=80, num_updates=2750, lr=4.64521e-06, gnorm=1.748, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=36961
2022-05-19 03:54:23 - progress_bar.py[line:274] - INFO: epoch 001:   2765 / 7081 loss=-0.006, score=1.364, ntokens=885.9, nsentences=80, sample_size=885.9, wps=101.9, ups=0.11, wpb=885.9, bsz=80, num_updates=2760, lr=4.64281e-06, gnorm=1.235, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=37048
2022-05-19 03:55:50 - progress_bar.py[line:274] - INFO: epoch 001:   2775 / 7081 loss=-0.004, score=1.372, ntokens=887.9, nsentences=80, sample_size=887.9, wps=102.1, ups=0.11, wpb=887.9, bsz=80, num_updates=2770, lr=4.6404e-06, gnorm=1.429, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=37135
2022-05-19 03:57:17 - progress_bar.py[line:274] - INFO: epoch 001:   2785 / 7081 loss=-0.003, score=1.36, ntokens=895.6, nsentences=80, sample_size=895.6, wps=103.1, ups=0.12, wpb=895.6, bsz=80, num_updates=2780, lr=4.638e-06, gnorm=1.727, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=37222
2022-05-19 03:58:43 - progress_bar.py[line:274] - INFO: epoch 001:   2795 / 7081 loss=-0.007, score=1.415, ntokens=888.8, nsentences=80, sample_size=888.8, wps=103.1, ups=0.12, wpb=888.8, bsz=80, num_updates=2790, lr=4.6356e-06, gnorm=1.214, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=37308
2022-05-19 04:00:10 - progress_bar.py[line:274] - INFO: epoch 001:   2805 / 7081 loss=-0.008, score=1.38, ntokens=886.1, nsentences=80, sample_size=886.1, wps=102.2, ups=0.12, wpb=886.1, bsz=80, num_updates=2800, lr=4.63319e-06, gnorm=1.517, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=37395
2022-05-19 04:01:37 - progress_bar.py[line:274] - INFO: epoch 001:   2815 / 7081 loss=-0.004, score=1.414, ntokens=893.2, nsentences=80, sample_size=893.2, wps=103.3, ups=0.12, wpb=893.2, bsz=80, num_updates=2810, lr=4.63079e-06, gnorm=1.507, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=37481
2022-05-19 04:03:03 - progress_bar.py[line:274] - INFO: epoch 001:   2825 / 7081 loss=-0.005, score=1.45, ntokens=878.3, nsentences=80, sample_size=878.3, wps=101.9, ups=0.12, wpb=878.3, bsz=80, num_updates=2820, lr=4.62838e-06, gnorm=1.697, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=37568
2022-05-19 04:04:29 - progress_bar.py[line:274] - INFO: epoch 001:   2835 / 7081 loss=-0.003, score=1.244, ntokens=885.8, nsentences=80, sample_size=885.8, wps=102.3, ups=0.12, wpb=885.8, bsz=80, num_updates=2830, lr=4.62598e-06, gnorm=1.64, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=37654
2022-05-19 04:05:56 - progress_bar.py[line:274] - INFO: epoch 001:   2845 / 7081 loss=-0.006, score=1.369, ntokens=881.9, nsentences=80, sample_size=881.9, wps=101.6, ups=0.12, wpb=881.9, bsz=80, num_updates=2840, lr=4.62358e-06, gnorm=1.57, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=37741
2022-05-19 04:07:22 - progress_bar.py[line:274] - INFO: epoch 001:   2855 / 7081 loss=-0.005, score=1.413, ntokens=883.1, nsentences=80, sample_size=883.1, wps=102.3, ups=0.12, wpb=883.1, bsz=80, num_updates=2850, lr=4.62117e-06, gnorm=1.875, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=37827
2022-05-19 04:08:50 - progress_bar.py[line:274] - INFO: epoch 001:   2865 / 7081 loss=-0.005, score=1.323, ntokens=895.4, nsentences=80, sample_size=895.4, wps=102.6, ups=0.11, wpb=895.4, bsz=80, num_updates=2860, lr=4.61877e-06, gnorm=1.358, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=37915
2022-05-19 04:10:17 - progress_bar.py[line:274] - INFO: epoch 001:   2875 / 7081 loss=-0.005, score=1.385, ntokens=898.3, nsentences=80, sample_size=898.3, wps=102.7, ups=0.11, wpb=898.3, bsz=80, num_updates=2870, lr=4.61637e-06, gnorm=1.628, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=38002
2022-05-19 04:11:44 - progress_bar.py[line:274] - INFO: epoch 001:   2885 / 7081 loss=-0.006, score=1.447, ntokens=883, nsentences=80, sample_size=883, wps=102.3, ups=0.12, wpb=883, bsz=80, num_updates=2880, lr=4.61396e-06, gnorm=1.895, clip=100, loss_scale=64, train_wall=86, gb_free=6.8, wall=38088
2022-05-19 04:13:10 - progress_bar.py[line:274] - INFO: epoch 001:   2895 / 7081 loss=-0.006, score=1.526, ntokens=877.3, nsentences=80, sample_size=877.3, wps=101.9, ups=0.12, wpb=877.3, bsz=80, num_updates=2890, lr=4.61156e-06, gnorm=1.637, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=38174
2022-05-19 04:14:36 - progress_bar.py[line:274] - INFO: epoch 001:   2905 / 7081 loss=-0.005, score=1.378, ntokens=883.5, nsentences=80, sample_size=883.5, wps=101.7, ups=0.12, wpb=883.5, bsz=80, num_updates=2900, lr=4.60915e-06, gnorm=1.881, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=38261
2022-05-19 04:16:03 - progress_bar.py[line:274] - INFO: epoch 001:   2915 / 7081 loss=-0.004, score=1.308, ntokens=883.9, nsentences=80, sample_size=883.9, wps=101.8, ups=0.12, wpb=883.9, bsz=80, num_updates=2910, lr=4.60675e-06, gnorm=2.201, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=38348
2022-05-19 04:17:30 - progress_bar.py[line:274] - INFO: epoch 001:   2925 / 7081 loss=-0.005, score=1.362, ntokens=900.1, nsentences=80, sample_size=900.1, wps=103.6, ups=0.12, wpb=900.1, bsz=80, num_updates=2920, lr=4.60435e-06, gnorm=1.946, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=38435
2022-05-19 04:18:57 - progress_bar.py[line:274] - INFO: epoch 001:   2935 / 7081 loss=-0.003, score=1.407, ntokens=883.3, nsentences=80, sample_size=883.3, wps=101.9, ups=0.12, wpb=883.3, bsz=80, num_updates=2930, lr=4.60194e-06, gnorm=2.122, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=38522
2022-05-19 04:20:24 - progress_bar.py[line:274] - INFO: epoch 001:   2945 / 7081 loss=-0.006, score=1.421, ntokens=889, nsentences=80, sample_size=889, wps=102.1, ups=0.11, wpb=889, bsz=80, num_updates=2940, lr=4.59954e-06, gnorm=1.216, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=38609
2022-05-19 04:21:51 - progress_bar.py[line:274] - INFO: epoch 001:   2955 / 7081 loss=-0.005, score=1.408, ntokens=893.9, nsentences=80, sample_size=893.9, wps=102.9, ups=0.12, wpb=893.9, bsz=80, num_updates=2950, lr=4.59714e-06, gnorm=1.478, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=38696
2022-05-19 04:23:18 - progress_bar.py[line:274] - INFO: epoch 001:   2965 / 7081 loss=-0.007, score=1.432, ntokens=885.6, nsentences=80, sample_size=885.6, wps=102, ups=0.12, wpb=885.6, bsz=80, num_updates=2960, lr=4.59473e-06, gnorm=1.509, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=38782
2022-05-19 04:24:45 - progress_bar.py[line:274] - INFO: epoch 001:   2975 / 7081 loss=-0.006, score=1.408, ntokens=900, nsentences=80, sample_size=900, wps=103.1, ups=0.11, wpb=900, bsz=80, num_updates=2970, lr=4.59233e-06, gnorm=1.329, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=38870
2022-05-19 04:26:12 - progress_bar.py[line:274] - INFO: epoch 001:   2985 / 7081 loss=-0.004, score=1.362, ntokens=897.4, nsentences=80, sample_size=897.4, wps=102.8, ups=0.11, wpb=897.4, bsz=80, num_updates=2980, lr=4.58992e-06, gnorm=1.605, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=38957
2022-05-19 04:27:39 - progress_bar.py[line:274] - INFO: epoch 001:   2995 / 7081 loss=-0.006, score=1.359, ntokens=890.2, nsentences=80, sample_size=890.2, wps=102.5, ups=0.12, wpb=890.2, bsz=80, num_updates=2990, lr=4.58752e-06, gnorm=1.375, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=39044
2022-05-19 04:29:06 - progress_bar.py[line:274] - INFO: epoch 001:   3005 / 7081 loss=-0.006, score=1.351, ntokens=889.9, nsentences=80, sample_size=889.9, wps=102.2, ups=0.11, wpb=889.9, bsz=80, num_updates=3000, lr=4.58512e-06, gnorm=1.371, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=39131
2022-05-19 04:29:06 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-19 05:11:49 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.349 | ntokens 112.237 | nsentences 10 | sample_size 112.237 | cider 1.446 | wps 109.5 | wpb 112.2 | bsz 10 | num_updates 3000 | best_cider 1.446
2022-05-19 05:11:49 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 3000 updates
2022-05-19 05:11:49 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_3000.pt
2022-05-19 05:11:57 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_3000.pt
2022-05-19 05:13:21 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_3000.pt (epoch 1 @ 3000 updates, score 1.446) (writing took 92.39120992086828 seconds)
2022-05-19 05:14:47 - progress_bar.py[line:274] - INFO: epoch 001:   3015 / 7081 loss=-0.005, score=1.339, ntokens=884.9, nsentences=80, sample_size=884.9, wps=3.2, ups=0, wpb=884.9, bsz=80, num_updates=3010, lr=4.58271e-06, gnorm=1.249, clip=70, loss_scale=64, train_wall=85, gb_free=6.8, wall=41872
2022-05-19 05:16:14 - progress_bar.py[line:274] - INFO: epoch 001:   3025 / 7081 loss=-0.01, score=1.418, ntokens=880.4, nsentences=80, sample_size=880.4, wps=101.4, ups=0.12, wpb=880.4, bsz=80, num_updates=3020, lr=4.58031e-06, gnorm=2.344, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=41958
2022-05-19 05:17:40 - progress_bar.py[line:274] - INFO: epoch 001:   3035 / 7081 loss=-0.006, score=1.364, ntokens=880.4, nsentences=80, sample_size=880.4, wps=101.7, ups=0.12, wpb=880.4, bsz=80, num_updates=3030, lr=4.57791e-06, gnorm=1.543, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=42045
2022-05-19 05:19:07 - progress_bar.py[line:274] - INFO: epoch 001:   3045 / 7081 loss=-0.002, score=1.392, ntokens=895.4, nsentences=80, sample_size=895.4, wps=103.2, ups=0.12, wpb=895.4, bsz=80, num_updates=3040, lr=4.5755e-06, gnorm=1.214, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=42132
2022-05-19 05:20:34 - progress_bar.py[line:274] - INFO: epoch 001:   3055 / 7081 loss=-0.006, score=1.503, ntokens=880.7, nsentences=80, sample_size=880.7, wps=101.5, ups=0.12, wpb=880.7, bsz=80, num_updates=3050, lr=4.5731e-06, gnorm=1.412, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=42218
2022-05-19 05:22:01 - progress_bar.py[line:274] - INFO: epoch 001:   3065 / 7081 loss=-0.006, score=1.356, ntokens=874.1, nsentences=80, sample_size=874.1, wps=100.5, ups=0.12, wpb=874.1, bsz=80, num_updates=3060, lr=4.57069e-06, gnorm=1.412, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=42305
2022-05-19 05:23:28 - progress_bar.py[line:274] - INFO: epoch 001:   3075 / 7081 loss=-0.004, score=1.346, ntokens=886.3, nsentences=80, sample_size=886.3, wps=101.8, ups=0.11, wpb=886.3, bsz=80, num_updates=3070, lr=4.56829e-06, gnorm=1.449, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=42393
2022-05-19 05:24:54 - progress_bar.py[line:274] - INFO: epoch 001:   3085 / 7081 loss=-0.006, score=1.425, ntokens=886.5, nsentences=80, sample_size=886.5, wps=102.6, ups=0.12, wpb=886.5, bsz=80, num_updates=3080, lr=4.56589e-06, gnorm=1.757, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=42479
2022-05-19 05:26:20 - progress_bar.py[line:274] - INFO: epoch 001:   3095 / 7081 loss=-0.003, score=1.42, ntokens=873.8, nsentences=80, sample_size=873.8, wps=101.2, ups=0.12, wpb=873.8, bsz=80, num_updates=3090, lr=4.56348e-06, gnorm=1.635, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=42565
2022-05-19 05:27:47 - progress_bar.py[line:274] - INFO: epoch 001:   3105 / 7081 loss=-0.004, score=1.518, ntokens=880.4, nsentences=80, sample_size=880.4, wps=101.4, ups=0.12, wpb=880.4, bsz=80, num_updates=3100, lr=4.56108e-06, gnorm=1.2, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=42652
2022-05-19 05:29:14 - progress_bar.py[line:274] - INFO: epoch 001:   3115 / 7081 loss=-0.003, score=1.343, ntokens=889.6, nsentences=80, sample_size=889.6, wps=102.8, ups=0.12, wpb=889.6, bsz=80, num_updates=3110, lr=4.55868e-06, gnorm=1.431, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=42739
2022-05-19 05:30:41 - progress_bar.py[line:274] - INFO: epoch 001:   3125 / 7081 loss=-0.005, score=1.385, ntokens=880.9, nsentences=80, sample_size=880.9, wps=101.4, ups=0.12, wpb=880.9, bsz=80, num_updates=3120, lr=4.55627e-06, gnorm=1.251, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=42826
2022-05-19 05:32:07 - progress_bar.py[line:274] - INFO: epoch 001:   3135 / 7081 loss=-0.003, score=1.248, ntokens=890.7, nsentences=80, sample_size=890.7, wps=103.1, ups=0.12, wpb=890.7, bsz=80, num_updates=3130, lr=4.55387e-06, gnorm=1.347, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=42912
2022-05-19 05:33:34 - progress_bar.py[line:274] - INFO: epoch 001:   3145 / 7081 loss=-0.005, score=1.353, ntokens=880.1, nsentences=80, sample_size=880.1, wps=101.5, ups=0.12, wpb=880.1, bsz=80, num_updates=3140, lr=4.55146e-06, gnorm=1.373, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=42999
2022-05-19 05:35:01 - progress_bar.py[line:274] - INFO: epoch 001:   3155 / 7081 loss=-0.007, score=1.457, ntokens=883.6, nsentences=80, sample_size=883.6, wps=101.6, ups=0.11, wpb=883.6, bsz=80, num_updates=3150, lr=4.54906e-06, gnorm=1.192, clip=50, loss_scale=128, train_wall=87, gb_free=6.8, wall=43086
2022-05-19 05:36:27 - progress_bar.py[line:274] - INFO: epoch 001:   3165 / 7081 loss=-0.008, score=1.469, ntokens=881, nsentences=80, sample_size=881, wps=102, ups=0.12, wpb=881, bsz=80, num_updates=3160, lr=4.54666e-06, gnorm=1.479, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=43172
2022-05-19 05:37:54 - progress_bar.py[line:274] - INFO: epoch 001:   3175 / 7081 loss=-0.006, score=1.398, ntokens=876.7, nsentences=80, sample_size=876.7, wps=101.3, ups=0.12, wpb=876.7, bsz=80, num_updates=3170, lr=4.54425e-06, gnorm=1.341, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=43258
2022-05-19 05:39:20 - progress_bar.py[line:274] - INFO: epoch 001:   3185 / 7081 loss=-0.004, score=1.516, ntokens=884.1, nsentences=80, sample_size=884.1, wps=101.9, ups=0.12, wpb=884.1, bsz=80, num_updates=3180, lr=4.54185e-06, gnorm=1.381, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=43345
2022-05-19 05:40:47 - progress_bar.py[line:274] - INFO: epoch 001:   3195 / 7081 loss=-0.007, score=1.458, ntokens=866.7, nsentences=80, sample_size=866.7, wps=100.1, ups=0.12, wpb=866.7, bsz=80, num_updates=3190, lr=4.53945e-06, gnorm=1.488, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=43432
2022-05-19 05:42:13 - progress_bar.py[line:274] - INFO: epoch 001:   3205 / 7081 loss=-0.007, score=1.444, ntokens=889.9, nsentences=80, sample_size=889.9, wps=103.1, ups=0.12, wpb=889.9, bsz=80, num_updates=3200, lr=4.53704e-06, gnorm=1.184, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=43518
2022-05-19 05:43:40 - progress_bar.py[line:274] - INFO: epoch 001:   3215 / 7081 loss=-0.006, score=1.378, ntokens=874.5, nsentences=80, sample_size=874.5, wps=101.3, ups=0.12, wpb=874.5, bsz=80, num_updates=3210, lr=4.53464e-06, gnorm=1.505, clip=100, loss_scale=128, train_wall=86, gb_free=6.8, wall=43605
2022-05-19 05:45:06 - progress_bar.py[line:274] - INFO: epoch 001:   3225 / 7081 loss=-0.008, score=1.523, ntokens=880.4, nsentences=80, sample_size=880.4, wps=101.5, ups=0.12, wpb=880.4, bsz=80, num_updates=3220, lr=4.53223e-06, gnorm=1.758, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=43691
2022-05-19 05:46:34 - progress_bar.py[line:274] - INFO: epoch 001:   3235 / 7081 loss=-0.007, score=1.481, ntokens=881.3, nsentences=80, sample_size=881.3, wps=100.8, ups=0.11, wpb=881.3, bsz=80, num_updates=3230, lr=4.52983e-06, gnorm=1.596, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=43779
2022-05-19 05:48:00 - progress_bar.py[line:274] - INFO: epoch 001:   3245 / 7081 loss=-0.005, score=1.339, ntokens=880.2, nsentences=80, sample_size=880.2, wps=101.7, ups=0.12, wpb=880.2, bsz=80, num_updates=3240, lr=4.52743e-06, gnorm=1.424, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=43865
2022-05-19 05:49:28 - progress_bar.py[line:274] - INFO: epoch 001:   3255 / 7081 loss=-0.004, score=1.443, ntokens=892.7, nsentences=80, sample_size=892.7, wps=102.4, ups=0.11, wpb=892.7, bsz=80, num_updates=3250, lr=4.52502e-06, gnorm=1.286, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=43952
2022-05-19 05:50:54 - progress_bar.py[line:274] - INFO: epoch 001:   3265 / 7081 loss=-0.004, score=1.364, ntokens=880.8, nsentences=80, sample_size=880.8, wps=101.4, ups=0.12, wpb=880.8, bsz=80, num_updates=3260, lr=4.52262e-06, gnorm=1.658, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=44039
2022-05-19 05:52:21 - progress_bar.py[line:274] - INFO: epoch 001:   3275 / 7081 loss=-0.005, score=1.382, ntokens=880.7, nsentences=80, sample_size=880.7, wps=101.8, ups=0.12, wpb=880.7, bsz=80, num_updates=3270, lr=4.52022e-06, gnorm=1.447, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=44126
2022-05-19 05:53:48 - progress_bar.py[line:274] - INFO: epoch 001:   3285 / 7081 loss=-0.004, score=1.439, ntokens=878.8, nsentences=80, sample_size=878.8, wps=100.8, ups=0.11, wpb=878.8, bsz=80, num_updates=3280, lr=4.51781e-06, gnorm=1.193, clip=70, loss_scale=128, train_wall=87, gb_free=6.8, wall=44213
2022-05-19 05:55:15 - progress_bar.py[line:274] - INFO: epoch 001:   3295 / 7081 loss=-0.006, score=1.372, ntokens=877.7, nsentences=80, sample_size=877.7, wps=100.9, ups=0.11, wpb=877.7, bsz=80, num_updates=3290, lr=4.51541e-06, gnorm=2.319, clip=100, loss_scale=128, train_wall=87, gb_free=6.8, wall=44300
2022-05-19 05:56:42 - progress_bar.py[line:274] - INFO: epoch 001:   3305 / 7081 loss=-0.005, score=1.452, ntokens=890.6, nsentences=80, sample_size=890.6, wps=103, ups=0.12, wpb=890.6, bsz=80, num_updates=3300, lr=4.51301e-06, gnorm=1.499, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=44387
2022-05-19 05:58:09 - progress_bar.py[line:274] - INFO: epoch 001:   3315 / 7081 loss=-0.005, score=1.497, ntokens=888.3, nsentences=80, sample_size=888.3, wps=101.7, ups=0.11, wpb=888.3, bsz=80, num_updates=3310, lr=4.5106e-06, gnorm=1.333, clip=40, loss_scale=128, train_wall=87, gb_free=6.8, wall=44474
2022-05-19 05:59:36 - progress_bar.py[line:274] - INFO: epoch 001:   3325 / 7081 loss=-0.005, score=1.316, ntokens=879.7, nsentences=80, sample_size=879.7, wps=101.5, ups=0.12, wpb=879.7, bsz=80, num_updates=3320, lr=4.5082e-06, gnorm=1.631, clip=100, loss_scale=128, train_wall=87, gb_free=6.8, wall=44561
2022-05-19 06:01:02 - progress_bar.py[line:274] - INFO: epoch 001:   3335 / 7081 loss=-0.002, score=1.332, ntokens=886, nsentences=80, sample_size=886, wps=102.8, ups=0.12, wpb=886, bsz=80, num_updates=3330, lr=4.50579e-06, gnorm=1.376, clip=60, loss_scale=128, train_wall=86, gb_free=6.8, wall=44647
2022-05-19 06:02:29 - progress_bar.py[line:274] - INFO: epoch 001:   3345 / 7081 loss=-0.003, score=1.382, ntokens=893.5, nsentences=80, sample_size=893.5, wps=102.6, ups=0.11, wpb=893.5, bsz=80, num_updates=3340, lr=4.50339e-06, gnorm=2.213, clip=100, loss_scale=128, train_wall=87, gb_free=6.8, wall=44734
2022-05-19 06:03:56 - progress_bar.py[line:274] - INFO: epoch 001:   3355 / 7081 loss=-0.004, score=1.475, ntokens=894.2, nsentences=80, sample_size=894.2, wps=103, ups=0.12, wpb=894.2, bsz=80, num_updates=3350, lr=4.50099e-06, gnorm=1.3, clip=70, loss_scale=128, train_wall=87, gb_free=6.8, wall=44821
2022-05-19 06:05:22 - progress_bar.py[line:274] - INFO: epoch 001:   3365 / 7081 loss=-0.004, score=1.405, ntokens=889.7, nsentences=80, sample_size=889.7, wps=102.8, ups=0.12, wpb=889.7, bsz=80, num_updates=3360, lr=4.49858e-06, gnorm=2.034, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=44907
2022-05-19 06:06:49 - progress_bar.py[line:274] - INFO: epoch 001:   3375 / 7081 loss=-0.004, score=1.413, ntokens=888.7, nsentences=80, sample_size=888.7, wps=102.7, ups=0.12, wpb=888.7, bsz=80, num_updates=3370, lr=4.49618e-06, gnorm=1.193, clip=60, loss_scale=128, train_wall=86, gb_free=6.8, wall=44994
2022-05-19 06:08:15 - progress_bar.py[line:274] - INFO: epoch 001:   3385 / 7081 loss=-0.007, score=1.385, ntokens=883, nsentences=80, sample_size=883, wps=102.6, ups=0.12, wpb=883, bsz=80, num_updates=3380, lr=4.49378e-06, gnorm=1.615, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=45080
2022-05-19 06:09:42 - progress_bar.py[line:274] - INFO: epoch 001:   3395 / 7081 loss=-0.005, score=1.481, ntokens=883.9, nsentences=80, sample_size=883.9, wps=101.7, ups=0.12, wpb=883.9, bsz=80, num_updates=3390, lr=4.49137e-06, gnorm=1.72, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=45167
2022-05-19 06:11:08 - progress_bar.py[line:274] - INFO: epoch 001:   3405 / 7081 loss=-0.006, score=1.389, ntokens=879.2, nsentences=80, sample_size=879.2, wps=102.4, ups=0.12, wpb=879.2, bsz=80, num_updates=3400, lr=4.48897e-06, gnorm=1.128, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=45253
2022-05-19 06:12:35 - progress_bar.py[line:274] - INFO: epoch 001:   3415 / 7081 loss=-0.006, score=1.342, ntokens=894, nsentences=80, sample_size=894, wps=102.2, ups=0.11, wpb=894, bsz=80, num_updates=3410, lr=4.48656e-06, gnorm=1.322, clip=100, loss_scale=128, train_wall=87, gb_free=6.8, wall=45340
2022-05-19 06:14:02 - progress_bar.py[line:274] - INFO: epoch 001:   3425 / 7081 loss=-0.007, score=1.405, ntokens=893.4, nsentences=80, sample_size=893.4, wps=102.5, ups=0.11, wpb=893.4, bsz=80, num_updates=3420, lr=4.48416e-06, gnorm=1.777, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=45427
2022-05-19 06:15:29 - progress_bar.py[line:274] - INFO: epoch 001:   3435 / 7081 loss=-0.006, score=1.404, ntokens=895.4, nsentences=80, sample_size=895.4, wps=103, ups=0.11, wpb=895.4, bsz=80, num_updates=3430, lr=4.48176e-06, gnorm=1.733, clip=70, loss_scale=128, train_wall=87, gb_free=6.8, wall=45514
2022-05-19 06:16:57 - progress_bar.py[line:274] - INFO: epoch 001:   3445 / 7081 loss=-0.005, score=1.434, ntokens=901.4, nsentences=80, sample_size=901.4, wps=102.6, ups=0.11, wpb=901.4, bsz=80, num_updates=3440, lr=4.47935e-06, gnorm=1.734, clip=90, loss_scale=128, train_wall=88, gb_free=6.8, wall=45602
2022-05-19 06:18:24 - progress_bar.py[line:274] - INFO: epoch 001:   3455 / 7081 loss=-0.004, score=1.396, ntokens=884.2, nsentences=80, sample_size=884.2, wps=101.9, ups=0.12, wpb=884.2, bsz=80, num_updates=3450, lr=4.47695e-06, gnorm=1.516, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=45689
2022-05-19 06:19:51 - progress_bar.py[line:274] - INFO: epoch 001:   3465 / 7081 loss=-0.005, score=1.378, ntokens=884, nsentences=80, sample_size=884, wps=101.8, ups=0.12, wpb=884, bsz=80, num_updates=3460, lr=4.47455e-06, gnorm=1.746, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=45776
2022-05-19 06:21:17 - progress_bar.py[line:274] - INFO: epoch 001:   3475 / 7081 loss=-0.004, score=1.506, ntokens=891.4, nsentences=80, sample_size=891.4, wps=103.1, ups=0.12, wpb=891.4, bsz=80, num_updates=3470, lr=4.47214e-06, gnorm=1.84, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=45862
2022-05-19 06:22:44 - progress_bar.py[line:274] - INFO: epoch 001:   3485 / 7081 loss=-0.006, score=1.416, ntokens=877.7, nsentences=80, sample_size=877.7, wps=101, ups=0.12, wpb=877.7, bsz=80, num_updates=3480, lr=4.46974e-06, gnorm=1.908, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=45949
2022-05-19 06:24:10 - progress_bar.py[line:274] - INFO: epoch 001:   3495 / 7081 loss=-0.006, score=1.48, ntokens=881.1, nsentences=80, sample_size=881.1, wps=102.2, ups=0.12, wpb=881.1, bsz=80, num_updates=3490, lr=4.46733e-06, gnorm=1.557, clip=100, loss_scale=128, train_wall=86, gb_free=6.8, wall=46035
2022-05-19 06:25:38 - progress_bar.py[line:274] - INFO: epoch 001:   3505 / 7081 loss=-0.004, score=1.415, ntokens=889.2, nsentences=80, sample_size=889.2, wps=101.9, ups=0.11, wpb=889.2, bsz=80, num_updates=3500, lr=4.46493e-06, gnorm=1.805, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=46123
2022-05-19 06:25:38 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-19 07:08:23 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.351 | ntokens 112.297 | nsentences 10 | sample_size 112.297 | cider 1.451 | wps 109.4 | wpb 112.3 | bsz 10 | num_updates 3500 | best_cider 1.451
2022-05-19 07:08:23 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 3500 updates
2022-05-19 07:08:23 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_3500.pt
2022-05-19 07:08:32 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_3500.pt
2022-05-19 07:09:52 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_3500.pt (epoch 1 @ 3500 updates, score 1.451) (writing took 88.83264531102031 seconds)
2022-05-19 07:11:18 - progress_bar.py[line:274] - INFO: epoch 001:   3515 / 7081 loss=-0.006, score=1.367, ntokens=879.4, nsentences=80, sample_size=879.4, wps=3.2, ups=0, wpb=879.4, bsz=80, num_updates=3510, lr=4.46253e-06, gnorm=1.524, clip=90, loss_scale=128, train_wall=85, gb_free=6.8, wall=48862
2022-05-19 07:12:44 - progress_bar.py[line:274] - INFO: epoch 001:   3525 / 7081 loss=-0.009, score=1.422, ntokens=879, nsentences=80, sample_size=879, wps=102.1, ups=0.12, wpb=879, bsz=80, num_updates=3520, lr=4.46012e-06, gnorm=1.826, clip=100, loss_scale=128, train_wall=86, gb_free=6.8, wall=48948
2022-05-19 07:14:10 - progress_bar.py[line:274] - INFO: epoch 001:   3535 / 7081 loss=-0.005, score=1.488, ntokens=889.1, nsentences=80, sample_size=889.1, wps=102.4, ups=0.12, wpb=889.1, bsz=80, num_updates=3530, lr=4.45772e-06, gnorm=1.661, clip=100, loss_scale=128, train_wall=87, gb_free=6.8, wall=49035
2022-05-19 07:15:37 - progress_bar.py[line:274] - INFO: epoch 001:   3545 / 7081 loss=-0.006, score=1.509, ntokens=877.9, nsentences=80, sample_size=877.9, wps=101.6, ups=0.12, wpb=877.9, bsz=80, num_updates=3540, lr=4.45532e-06, gnorm=1.836, clip=100, loss_scale=128, train_wall=86, gb_free=6.8, wall=49122
2022-05-19 07:17:04 - progress_bar.py[line:274] - INFO: epoch 001:   3555 / 7081 loss=-0.003, score=1.331, ntokens=891.6, nsentences=80, sample_size=891.6, wps=102.7, ups=0.12, wpb=891.6, bsz=80, num_updates=3550, lr=4.45291e-06, gnorm=1.796, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=49208
2022-05-19 07:18:31 - progress_bar.py[line:274] - INFO: epoch 001:   3565 / 7081 loss=-0.004, score=1.274, ntokens=887.3, nsentences=80, sample_size=887.3, wps=101.9, ups=0.11, wpb=887.3, bsz=80, num_updates=3560, lr=4.45051e-06, gnorm=1.727, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=49295
2022-05-19 07:19:57 - progress_bar.py[line:274] - INFO: epoch 001:   3575 / 7081 loss=-0.004, score=1.271, ntokens=884.1, nsentences=80, sample_size=884.1, wps=102, ups=0.12, wpb=884.1, bsz=80, num_updates=3570, lr=4.4481e-06, gnorm=1.402, clip=90, loss_scale=128, train_wall=87, gb_free=6.8, wall=49382
2022-05-19 07:21:24 - progress_bar.py[line:274] - INFO: epoch 001:   3585 / 7081 loss=-0.01, score=1.549, ntokens=872.5, nsentences=80, sample_size=872.5, wps=100.7, ups=0.12, wpb=872.5, bsz=80, num_updates=3580, lr=4.4457e-06, gnorm=1.619, clip=100, loss_scale=128, train_wall=87, gb_free=6.8, wall=49469
2022-05-19 07:22:50 - progress_bar.py[line:274] - INFO: epoch 001:   3595 / 7081 loss=-0.007, score=1.432, ntokens=884.5, nsentences=80, sample_size=884.5, wps=102.4, ups=0.12, wpb=884.5, bsz=80, num_updates=3590, lr=4.4433e-06, gnorm=1.746, clip=80, loss_scale=128, train_wall=86, gb_free=6.8, wall=49555
2022-05-19 07:24:18 - progress_bar.py[line:274] - INFO: epoch 001:   3605 / 7081 loss=-0.007, score=1.509, ntokens=896, nsentences=80, sample_size=896, wps=101.7, ups=0.11, wpb=896, bsz=80, num_updates=3600, lr=4.44089e-06, gnorm=1.667, clip=100, loss_scale=128, train_wall=88, gb_free=6.8, wall=49643
2022-05-19 07:25:45 - progress_bar.py[line:274] - INFO: epoch 001:   3615 / 7081 loss=-0.005, score=1.398, ntokens=884.1, nsentences=80, sample_size=884.1, wps=102.2, ups=0.12, wpb=884.1, bsz=80, num_updates=3610, lr=4.43849e-06, gnorm=1.623, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=49730
2022-05-19 07:27:13 - progress_bar.py[line:274] - INFO: epoch 001:   3625 / 7081 loss=-0.006, score=1.377, ntokens=886.2, nsentences=80, sample_size=886.2, wps=100.9, ups=0.11, wpb=886.2, bsz=80, num_updates=3620, lr=4.43609e-06, gnorm=1.364, clip=80, loss_scale=128, train_wall=88, gb_free=6.8, wall=49818
2022-05-19 07:28:39 - progress_bar.py[line:274] - INFO: epoch 001:   3635 / 7081 loss=-0.004, score=1.467, ntokens=882.9, nsentences=80, sample_size=882.9, wps=102.2, ups=0.12, wpb=882.9, bsz=80, num_updates=3630, lr=4.43368e-06, gnorm=1.416, clip=100, loss_scale=256, train_wall=86, gb_free=6.8, wall=49904
2022-05-19 07:30:06 - progress_bar.py[line:274] - INFO: epoch 001:   3645 / 7081 loss=-0.002, score=1.48, ntokens=882.4, nsentences=80, sample_size=882.4, wps=102.1, ups=0.12, wpb=882.4, bsz=80, num_updates=3640, lr=4.43128e-06, gnorm=1.575, clip=70, loss_scale=256, train_wall=86, gb_free=6.8, wall=49990
2022-05-19 07:31:33 - progress_bar.py[line:274] - INFO: epoch 001:   3655 / 7081 loss=-0.008, score=1.528, ntokens=879.3, nsentences=80, sample_size=879.3, wps=100.9, ups=0.11, wpb=879.3, bsz=80, num_updates=3650, lr=4.42887e-06, gnorm=1.68, clip=90, loss_scale=256, train_wall=87, gb_free=6.8, wall=50078
2022-05-19 07:32:59 - progress_bar.py[line:274] - INFO: epoch 001:   3665 / 7081 loss=-0.001, score=1.371, ntokens=888.7, nsentences=80, sample_size=888.7, wps=102.7, ups=0.12, wpb=888.7, bsz=80, num_updates=3660, lr=4.42647e-06, gnorm=1.382, clip=60, loss_scale=256, train_wall=86, gb_free=6.8, wall=50164
2022-05-19 07:34:27 - progress_bar.py[line:274] - INFO: epoch 001:   3675 / 7081 loss=-0.003, score=1.343, ntokens=888.3, nsentences=80, sample_size=888.3, wps=101.3, ups=0.11, wpb=888.3, bsz=80, num_updates=3670, lr=4.42407e-06, gnorm=1.344, clip=70, loss_scale=256, train_wall=88, gb_free=6.8, wall=50252
2022-05-19 07:35:54 - progress_bar.py[line:274] - INFO: epoch 001:   3685 / 7081 loss=-0.002, score=1.283, ntokens=885.7, nsentences=80, sample_size=885.7, wps=101.9, ups=0.12, wpb=885.7, bsz=80, num_updates=3680, lr=4.42166e-06, gnorm=1.542, clip=80, loss_scale=256, train_wall=87, gb_free=6.8, wall=50339
2022-05-19 07:37:21 - progress_bar.py[line:274] - INFO: epoch 001:   3695 / 7081 loss=-0.004, score=1.347, ntokens=884.2, nsentences=80, sample_size=884.2, wps=101.8, ups=0.12, wpb=884.2, bsz=80, num_updates=3690, lr=4.41926e-06, gnorm=1.518, clip=80, loss_scale=256, train_wall=87, gb_free=6.8, wall=50426
2022-05-19 07:37:38 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-05-19 07:38:56 - progress_bar.py[line:274] - INFO: epoch 001:   3706 / 7081 loss=-0.005, score=1.378, ntokens=884.8, nsentences=80, sample_size=884.8, wps=93.1, ups=0.11, wpb=884.8, bsz=80, num_updates=3700, lr=4.41686e-06, gnorm=1.456, clip=60, loss_scale=128, train_wall=95, gb_free=6.8, wall=50521
2022-05-19 07:40:23 - progress_bar.py[line:274] - INFO: epoch 001:   3716 / 7081 loss=-0.005, score=1.357, ntokens=879, nsentences=80, sample_size=879, wps=101.1, ups=0.12, wpb=879, bsz=80, num_updates=3710, lr=4.41445e-06, gnorm=1.889, clip=70, loss_scale=128, train_wall=87, gb_free=6.8, wall=50608
2022-05-19 07:41:49 - progress_bar.py[line:274] - INFO: epoch 001:   3726 / 7081 loss=-0.005, score=1.439, ntokens=883, nsentences=80, sample_size=883, wps=102, ups=0.12, wpb=883, bsz=80, num_updates=3720, lr=4.41205e-06, gnorm=1.325, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=50694
2022-05-19 07:43:16 - progress_bar.py[line:274] - INFO: epoch 001:   3736 / 7081 loss=-0.005, score=1.364, ntokens=881.9, nsentences=80, sample_size=881.9, wps=102.1, ups=0.12, wpb=881.9, bsz=80, num_updates=3730, lr=4.40964e-06, gnorm=1.895, clip=90, loss_scale=128, train_wall=86, gb_free=6.8, wall=50780
2022-05-19 07:44:42 - progress_bar.py[line:274] - INFO: epoch 001:   3746 / 7081 loss=-0.004, score=1.366, ntokens=880.7, nsentences=80, sample_size=880.7, wps=102, ups=0.12, wpb=880.7, bsz=80, num_updates=3740, lr=4.40724e-06, gnorm=1.169, clip=70, loss_scale=128, train_wall=86, gb_free=6.8, wall=50867
2022-05-19 07:46:09 - progress_bar.py[line:274] - INFO: epoch 001:   3756 / 7081 loss=-0.005, score=1.324, ntokens=871.5, nsentences=80, sample_size=871.5, wps=99.7, ups=0.11, wpb=871.5, bsz=80, num_updates=3750, lr=4.40484e-06, gnorm=1.225, clip=80, loss_scale=128, train_wall=87, gb_free=6.8, wall=50954
2022-05-19 07:47:27 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-19 07:47:45 - progress_bar.py[line:274] - INFO: epoch 001:   3767 / 7081 loss=-0.002, score=1.423, ntokens=881.9, nsentences=80, sample_size=881.9, wps=92.7, ups=0.11, wpb=881.9, bsz=80, num_updates=3760, lr=4.40243e-06, gnorm=1.69, clip=80, loss_scale=64, train_wall=95, gb_free=6.8, wall=51049
2022-05-19 07:49:12 - progress_bar.py[line:274] - INFO: epoch 001:   3777 / 7081 loss=-0.004, score=1.432, ntokens=899.2, nsentences=80, sample_size=899.2, wps=103.1, ups=0.11, wpb=899.2, bsz=80, num_updates=3770, lr=4.40003e-06, gnorm=1.498, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=51137
2022-05-19 07:50:38 - progress_bar.py[line:274] - INFO: epoch 001:   3787 / 7081 loss=-0.003, score=1.41, ntokens=876.5, nsentences=80, sample_size=876.5, wps=101.5, ups=0.12, wpb=876.5, bsz=80, num_updates=3780, lr=4.39763e-06, gnorm=1.87, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=51223
2022-05-19 07:52:05 - progress_bar.py[line:274] - INFO: epoch 001:   3797 / 7081 loss=-0.004, score=1.381, ntokens=883.6, nsentences=80, sample_size=883.6, wps=101.2, ups=0.11, wpb=883.6, bsz=80, num_updates=3790, lr=4.39522e-06, gnorm=1.442, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=51310
2022-05-19 07:53:33 - progress_bar.py[line:274] - INFO: epoch 001:   3807 / 7081 loss=-0.005, score=1.397, ntokens=891.3, nsentences=80, sample_size=891.3, wps=102.3, ups=0.11, wpb=891.3, bsz=80, num_updates=3800, lr=4.39282e-06, gnorm=1.825, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=51397
2022-05-19 07:55:00 - progress_bar.py[line:274] - INFO: epoch 001:   3817 / 7081 loss=-0.003, score=1.468, ntokens=876, nsentences=80, sample_size=876, wps=100.7, ups=0.11, wpb=876, bsz=80, num_updates=3810, lr=4.39042e-06, gnorm=1.573, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=51485
2022-05-19 07:56:26 - progress_bar.py[line:274] - INFO: epoch 001:   3827 / 7081 loss=-0.004, score=1.45, ntokens=873.4, nsentences=80, sample_size=873.4, wps=101, ups=0.12, wpb=873.4, bsz=80, num_updates=3820, lr=4.38801e-06, gnorm=1.825, clip=100, loss_scale=64, train_wall=86, gb_free=6.8, wall=51571
2022-05-19 07:57:53 - progress_bar.py[line:274] - INFO: epoch 001:   3837 / 7081 loss=-0.005, score=1.441, ntokens=882.7, nsentences=80, sample_size=882.7, wps=101.5, ups=0.12, wpb=882.7, bsz=80, num_updates=3830, lr=4.38561e-06, gnorm=1.297, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=51658
2022-05-19 07:59:19 - progress_bar.py[line:274] - INFO: epoch 001:   3847 / 7081 loss=-0.007, score=1.367, ntokens=868, nsentences=80, sample_size=868, wps=100.8, ups=0.12, wpb=868, bsz=80, num_updates=3840, lr=4.3832e-06, gnorm=1.873, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=51744
2022-05-19 08:00:46 - progress_bar.py[line:274] - INFO: epoch 001:   3857 / 7081 loss=-0.007, score=1.494, ntokens=881.3, nsentences=80, sample_size=881.3, wps=101.7, ups=0.12, wpb=881.3, bsz=80, num_updates=3850, lr=4.3808e-06, gnorm=1.528, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=51831
2022-05-19 08:02:12 - progress_bar.py[line:274] - INFO: epoch 001:   3867 / 7081 loss=-0.007, score=1.408, ntokens=870.6, nsentences=80, sample_size=870.6, wps=101.1, ups=0.12, wpb=870.6, bsz=80, num_updates=3860, lr=4.3784e-06, gnorm=2.083, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=51917
2022-05-19 08:03:39 - progress_bar.py[line:274] - INFO: epoch 001:   3877 / 7081 loss=-0.007, score=1.374, ntokens=889.6, nsentences=80, sample_size=889.6, wps=102.6, ups=0.12, wpb=889.6, bsz=80, num_updates=3870, lr=4.37599e-06, gnorm=1.265, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=52004
2022-05-19 08:05:06 - progress_bar.py[line:274] - INFO: epoch 001:   3887 / 7081 loss=-0.007, score=1.456, ntokens=891.6, nsentences=80, sample_size=891.6, wps=102.1, ups=0.11, wpb=891.6, bsz=80, num_updates=3880, lr=4.37359e-06, gnorm=1.818, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=52091
2022-05-19 08:06:33 - progress_bar.py[line:274] - INFO: epoch 001:   3897 / 7081 loss=-0.003, score=1.353, ntokens=889, nsentences=80, sample_size=889, wps=101.8, ups=0.11, wpb=889, bsz=80, num_updates=3890, lr=4.37119e-06, gnorm=1.359, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=52178
2022-05-19 08:08:00 - progress_bar.py[line:274] - INFO: epoch 001:   3907 / 7081 loss=-0.006, score=1.392, ntokens=877.2, nsentences=80, sample_size=877.2, wps=101.4, ups=0.12, wpb=877.2, bsz=80, num_updates=3900, lr=4.36878e-06, gnorm=1.32, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=52265
2022-05-19 08:09:27 - progress_bar.py[line:274] - INFO: epoch 001:   3917 / 7081 loss=-0.006, score=1.434, ntokens=885.5, nsentences=80, sample_size=885.5, wps=101.7, ups=0.11, wpb=885.5, bsz=80, num_updates=3910, lr=4.36638e-06, gnorm=1.432, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=52352
2022-05-19 08:10:54 - progress_bar.py[line:274] - INFO: epoch 001:   3927 / 7081 loss=-0.005, score=1.357, ntokens=887.1, nsentences=80, sample_size=887.1, wps=101.5, ups=0.11, wpb=887.1, bsz=80, num_updates=3920, lr=4.36397e-06, gnorm=1.767, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=52439
2022-05-19 08:12:21 - progress_bar.py[line:274] - INFO: epoch 001:   3937 / 7081 loss=-0.004, score=1.411, ntokens=886.3, nsentences=80, sample_size=886.3, wps=102.4, ups=0.12, wpb=886.3, bsz=80, num_updates=3930, lr=4.36157e-06, gnorm=1.391, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=52526
2022-05-19 08:13:48 - progress_bar.py[line:274] - INFO: epoch 001:   3947 / 7081 loss=-0.004, score=1.459, ntokens=891.4, nsentences=80, sample_size=891.4, wps=101.6, ups=0.11, wpb=891.4, bsz=80, num_updates=3940, lr=4.35917e-06, gnorm=1.536, clip=70, loss_scale=64, train_wall=88, gb_free=6.8, wall=52613
2022-05-19 08:15:15 - progress_bar.py[line:274] - INFO: epoch 001:   3957 / 7081 loss=-0.007, score=1.478, ntokens=883.6, nsentences=80, sample_size=883.6, wps=101.6, ups=0.12, wpb=883.6, bsz=80, num_updates=3950, lr=4.35676e-06, gnorm=1.535, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=52700
2022-05-19 08:16:42 - progress_bar.py[line:274] - INFO: epoch 001:   3967 / 7081 loss=-0.001, score=1.367, ntokens=896.7, nsentences=80, sample_size=896.7, wps=103, ups=0.11, wpb=896.7, bsz=80, num_updates=3960, lr=4.35436e-06, gnorm=1.34, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=52787
2022-05-19 08:18:09 - progress_bar.py[line:274] - INFO: epoch 001:   3977 / 7081 loss=-0.005, score=1.454, ntokens=870.8, nsentences=80, sample_size=870.8, wps=100.9, ups=0.12, wpb=870.8, bsz=80, num_updates=3970, lr=4.35196e-06, gnorm=1.578, clip=60, loss_scale=64, train_wall=86, gb_free=6.8, wall=52874
2022-05-19 08:19:36 - progress_bar.py[line:274] - INFO: epoch 001:   3987 / 7081 loss=-0.005, score=1.502, ntokens=894.5, nsentences=80, sample_size=894.5, wps=103.1, ups=0.12, wpb=894.5, bsz=80, num_updates=3980, lr=4.34955e-06, gnorm=2.044, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=52960
2022-05-19 08:21:03 - progress_bar.py[line:274] - INFO: epoch 001:   3997 / 7081 loss=-0.004, score=1.457, ntokens=885.7, nsentences=80, sample_size=885.7, wps=101.8, ups=0.11, wpb=885.7, bsz=80, num_updates=3990, lr=4.34715e-06, gnorm=1.465, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=53047
2022-05-19 08:22:30 - progress_bar.py[line:274] - INFO: epoch 001:   4007 / 7081 loss=-0.004, score=1.423, ntokens=892.7, nsentences=80, sample_size=892.7, wps=102.6, ups=0.11, wpb=892.7, bsz=80, num_updates=4000, lr=4.34474e-06, gnorm=1.416, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=53134
2022-05-19 08:22:30 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-19 09:05:16 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.356 | ntokens 112.472 | nsentences 10 | sample_size 112.472 | cider 1.455 | wps 109.6 | wpb 112.5 | bsz 10 | num_updates 4000 | best_cider 1.455
2022-05-19 09:05:16 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2022-05-19 09:05:16 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_4000.pt
2022-05-19 09:05:25 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_4000.pt
2022-05-19 09:06:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 1.455) (writing took 90.83360984106548 seconds)
2022-05-19 09:08:12 - progress_bar.py[line:274] - INFO: epoch 001:   4017 / 7081 loss=-0.003, score=1.388, ntokens=889.3, nsentences=80, sample_size=889.3, wps=3.2, ups=0, wpb=889.3, bsz=80, num_updates=4010, lr=4.34234e-06, gnorm=1.225, clip=60, loss_scale=64, train_wall=85, gb_free=6.8, wall=55877
2022-05-19 09:09:39 - progress_bar.py[line:274] - INFO: epoch 001:   4027 / 7081 loss=-0.005, score=1.292, ntokens=880.3, nsentences=80, sample_size=880.3, wps=101.9, ups=0.12, wpb=880.3, bsz=80, num_updates=4020, lr=4.33994e-06, gnorm=1.604, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=55963
2022-05-19 09:11:05 - progress_bar.py[line:274] - INFO: epoch 001:   4037 / 7081 loss=-0.005, score=1.355, ntokens=876.4, nsentences=80, sample_size=876.4, wps=101.5, ups=0.12, wpb=876.4, bsz=80, num_updates=4030, lr=4.33753e-06, gnorm=1.716, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=56050
2022-05-19 09:12:31 - progress_bar.py[line:274] - INFO: epoch 001:   4047 / 7081 loss=-0.003, score=1.36, ntokens=880.3, nsentences=80, sample_size=880.3, wps=101.7, ups=0.12, wpb=880.3, bsz=80, num_updates=4040, lr=4.33513e-06, gnorm=1.302, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=56136
2022-05-19 09:13:59 - progress_bar.py[line:274] - INFO: epoch 001:   4057 / 7081 loss=-0.004, score=1.484, ntokens=887.2, nsentences=80, sample_size=887.2, wps=101.9, ups=0.11, wpb=887.2, bsz=80, num_updates=4050, lr=4.33273e-06, gnorm=1.318, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=56223
2022-05-19 09:15:25 - progress_bar.py[line:274] - INFO: epoch 001:   4067 / 7081 loss=-0.005, score=1.44, ntokens=900.2, nsentences=80, sample_size=900.2, wps=103.6, ups=0.12, wpb=900.2, bsz=80, num_updates=4060, lr=4.33032e-06, gnorm=1.768, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=56310
2022-05-19 09:16:52 - progress_bar.py[line:274] - INFO: epoch 001:   4077 / 7081 loss=-0.004, score=1.543, ntokens=887.3, nsentences=80, sample_size=887.3, wps=102.6, ups=0.12, wpb=887.3, bsz=80, num_updates=4070, lr=4.32792e-06, gnorm=1.441, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=56397
2022-05-19 09:18:19 - progress_bar.py[line:274] - INFO: epoch 001:   4087 / 7081 loss=-0.006, score=1.341, ntokens=882, nsentences=80, sample_size=882, wps=101.4, ups=0.12, wpb=882, bsz=80, num_updates=4080, lr=4.32551e-06, gnorm=1.58, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=56484
2022-05-19 09:19:45 - progress_bar.py[line:274] - INFO: epoch 001:   4097 / 7081 loss=-0.003, score=1.385, ntokens=880, nsentences=80, sample_size=880, wps=101.9, ups=0.12, wpb=880, bsz=80, num_updates=4090, lr=4.32311e-06, gnorm=2.16, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=56570
2022-05-19 09:21:12 - progress_bar.py[line:274] - INFO: epoch 001:   4107 / 7081 loss=-0.004, score=1.382, ntokens=882.8, nsentences=80, sample_size=882.8, wps=101.8, ups=0.12, wpb=882.8, bsz=80, num_updates=4100, lr=4.32071e-06, gnorm=1.642, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=56657
2022-05-19 09:22:38 - progress_bar.py[line:274] - INFO: epoch 001:   4117 / 7081 loss=-0.003, score=1.435, ntokens=883.4, nsentences=80, sample_size=883.4, wps=102.7, ups=0.12, wpb=883.4, bsz=80, num_updates=4110, lr=4.3183e-06, gnorm=1.855, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=56743
2022-05-19 09:24:04 - progress_bar.py[line:274] - INFO: epoch 001:   4127 / 7081 loss=-0.005, score=1.333, ntokens=876.4, nsentences=80, sample_size=876.4, wps=101.6, ups=0.12, wpb=876.4, bsz=80, num_updates=4120, lr=4.3159e-06, gnorm=1.664, clip=60, loss_scale=64, train_wall=86, gb_free=6.8, wall=56829
2022-05-19 09:25:31 - progress_bar.py[line:274] - INFO: epoch 001:   4137 / 7081 loss=-0.005, score=1.451, ntokens=881.2, nsentences=80, sample_size=881.2, wps=101.5, ups=0.12, wpb=881.2, bsz=80, num_updates=4130, lr=4.3135e-06, gnorm=1.829, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=56916
2022-05-19 09:26:57 - progress_bar.py[line:274] - INFO: epoch 001:   4147 / 7081 loss=-0.007, score=1.429, ntokens=878.5, nsentences=80, sample_size=878.5, wps=101.8, ups=0.12, wpb=878.5, bsz=80, num_updates=4140, lr=4.31109e-06, gnorm=1.525, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=57002
2022-05-19 09:28:24 - progress_bar.py[line:274] - INFO: epoch 001:   4157 / 7081 loss=-0.004, score=1.352, ntokens=889.9, nsentences=80, sample_size=889.9, wps=103.2, ups=0.12, wpb=889.9, bsz=80, num_updates=4150, lr=4.30869e-06, gnorm=1.536, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=57088
2022-05-19 09:29:50 - progress_bar.py[line:274] - INFO: epoch 001:   4167 / 7081 loss=-0.007, score=1.456, ntokens=888.7, nsentences=80, sample_size=888.7, wps=102.5, ups=0.12, wpb=888.7, bsz=80, num_updates=4160, lr=4.30628e-06, gnorm=1.582, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=57175
2022-05-19 09:31:17 - progress_bar.py[line:274] - INFO: epoch 001:   4177 / 7081 loss=-0.008, score=1.458, ntokens=896.1, nsentences=80, sample_size=896.1, wps=103.1, ups=0.12, wpb=896.1, bsz=80, num_updates=4170, lr=4.30388e-06, gnorm=1.503, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=57262
2022-05-19 09:32:44 - progress_bar.py[line:274] - INFO: epoch 001:   4187 / 7081 loss=-0.005, score=1.392, ntokens=884.6, nsentences=80, sample_size=884.6, wps=102.4, ups=0.12, wpb=884.6, bsz=80, num_updates=4180, lr=4.30148e-06, gnorm=1.618, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=57348
2022-05-19 09:34:10 - progress_bar.py[line:274] - INFO: epoch 001:   4197 / 7081 loss=-0.005, score=1.477, ntokens=892.2, nsentences=80, sample_size=892.2, wps=102.9, ups=0.12, wpb=892.2, bsz=80, num_updates=4190, lr=4.29907e-06, gnorm=1.458, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=57435
2022-05-19 09:35:37 - progress_bar.py[line:274] - INFO: epoch 001:   4207 / 7081 loss=-0.002, score=1.34, ntokens=892.9, nsentences=80, sample_size=892.9, wps=102.9, ups=0.12, wpb=892.9, bsz=80, num_updates=4200, lr=4.29667e-06, gnorm=1.668, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=57522
2022-05-19 09:37:03 - progress_bar.py[line:274] - INFO: epoch 001:   4217 / 7081 loss=-0.006, score=1.435, ntokens=874.4, nsentences=80, sample_size=874.4, wps=101.5, ups=0.12, wpb=874.4, bsz=80, num_updates=4210, lr=4.29427e-06, gnorm=2.059, clip=90, loss_scale=64, train_wall=86, gb_free=6.8, wall=57608
2022-05-19 09:38:30 - progress_bar.py[line:274] - INFO: epoch 001:   4227 / 7081 loss=-0.007, score=1.464, ntokens=893.4, nsentences=80, sample_size=893.4, wps=102.9, ups=0.12, wpb=893.4, bsz=80, num_updates=4220, lr=4.29186e-06, gnorm=1.45, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=57695
2022-05-19 09:39:57 - progress_bar.py[line:274] - INFO: epoch 001:   4237 / 7081 loss=-0.004, score=1.415, ntokens=888.6, nsentences=80, sample_size=888.6, wps=101.9, ups=0.11, wpb=888.6, bsz=80, num_updates=4230, lr=4.28946e-06, gnorm=1.765, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=57782
2022-05-19 09:41:24 - progress_bar.py[line:274] - INFO: epoch 001:   4247 / 7081 loss=-0.005, score=1.487, ntokens=886.9, nsentences=80, sample_size=886.9, wps=102.7, ups=0.12, wpb=886.9, bsz=80, num_updates=4240, lr=4.28705e-06, gnorm=1.911, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=57868
2022-05-19 09:42:07 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-05-19 09:42:59 - progress_bar.py[line:274] - INFO: epoch 001:   4258 / 7081 loss=-0.005, score=1.425, ntokens=878.7, nsentences=80, sample_size=878.7, wps=92.5, ups=0.11, wpb=878.7, bsz=80, num_updates=4250, lr=4.28465e-06, gnorm=1.339, clip=70, loss_scale=32, train_wall=95, gb_free=6.8, wall=57963
2022-05-19 09:44:25 - progress_bar.py[line:274] - INFO: epoch 001:   4268 / 7081 loss=-0.004, score=1.365, ntokens=888.1, nsentences=80, sample_size=888.1, wps=102.5, ups=0.12, wpb=888.1, bsz=80, num_updates=4260, lr=4.28225e-06, gnorm=1.614, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=58050
2022-05-19 09:45:51 - progress_bar.py[line:274] - INFO: epoch 001:   4278 / 7081 loss=-0.005, score=1.496, ntokens=891.9, nsentences=80, sample_size=891.9, wps=103.4, ups=0.12, wpb=891.9, bsz=80, num_updates=4270, lr=4.27984e-06, gnorm=1.51, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=58136
2022-05-19 09:47:18 - progress_bar.py[line:274] - INFO: epoch 001:   4288 / 7081 loss=-0.005, score=1.412, ntokens=884.8, nsentences=80, sample_size=884.8, wps=102, ups=0.12, wpb=884.8, bsz=80, num_updates=4280, lr=4.27744e-06, gnorm=1.489, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=58223
2022-05-19 09:48:45 - progress_bar.py[line:274] - INFO: epoch 001:   4298 / 7081 loss=-0.006, score=1.383, ntokens=890.6, nsentences=80, sample_size=890.6, wps=102.4, ups=0.11, wpb=890.6, bsz=80, num_updates=4290, lr=4.27504e-06, gnorm=1.338, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=58310
2022-05-19 09:50:12 - progress_bar.py[line:274] - INFO: epoch 001:   4308 / 7081 loss=-0.004, score=1.458, ntokens=885.7, nsentences=80, sample_size=885.7, wps=101.7, ups=0.11, wpb=885.7, bsz=80, num_updates=4300, lr=4.27263e-06, gnorm=1.359, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=58397
2022-05-19 09:51:39 - progress_bar.py[line:274] - INFO: epoch 001:   4318 / 7081 loss=-0.006, score=1.342, ntokens=890.9, nsentences=80, sample_size=890.9, wps=102.4, ups=0.11, wpb=890.9, bsz=80, num_updates=4310, lr=4.27023e-06, gnorm=1.536, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=58484
2022-05-19 09:53:06 - progress_bar.py[line:274] - INFO: epoch 001:   4328 / 7081 loss=-0.004, score=1.417, ntokens=890, nsentences=80, sample_size=890, wps=102.8, ups=0.12, wpb=890, bsz=80, num_updates=4320, lr=4.26783e-06, gnorm=1.219, clip=50, loss_scale=32, train_wall=86, gb_free=6.8, wall=58571
2022-05-19 09:54:33 - progress_bar.py[line:274] - INFO: epoch 001:   4338 / 7081 loss=-0.008, score=1.421, ntokens=891.2, nsentences=80, sample_size=891.2, wps=102.5, ups=0.11, wpb=891.2, bsz=80, num_updates=4330, lr=4.26542e-06, gnorm=1.383, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=58658
2022-05-19 09:56:00 - progress_bar.py[line:274] - INFO: epoch 001:   4348 / 7081 loss=-0.005, score=1.473, ntokens=882.4, nsentences=80, sample_size=882.4, wps=101.7, ups=0.12, wpb=882.4, bsz=80, num_updates=4340, lr=4.26302e-06, gnorm=1.129, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=58744
2022-05-19 09:57:26 - progress_bar.py[line:274] - INFO: epoch 001:   4358 / 7081 loss=-0.006, score=1.379, ntokens=885.7, nsentences=80, sample_size=885.7, wps=102, ups=0.12, wpb=885.7, bsz=80, num_updates=4350, lr=4.26061e-06, gnorm=1.156, clip=60, loss_scale=32, train_wall=87, gb_free=6.8, wall=58831
2022-05-19 09:58:53 - progress_bar.py[line:274] - INFO: epoch 001:   4368 / 7081 loss=-0.007, score=1.426, ntokens=879.3, nsentences=80, sample_size=879.3, wps=101.4, ups=0.12, wpb=879.3, bsz=80, num_updates=4360, lr=4.25821e-06, gnorm=1.909, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=58918
2022-05-19 10:00:19 - progress_bar.py[line:274] - INFO: epoch 001:   4378 / 7081 loss=-0.005, score=1.353, ntokens=890.8, nsentences=80, sample_size=890.8, wps=103.4, ups=0.12, wpb=890.8, bsz=80, num_updates=4370, lr=4.25581e-06, gnorm=1.429, clip=70, loss_scale=32, train_wall=86, gb_free=6.8, wall=59004
2022-05-19 10:01:46 - progress_bar.py[line:274] - INFO: epoch 001:   4388 / 7081 loss=-0.005, score=1.474, ntokens=893.6, nsentences=80, sample_size=893.6, wps=103.1, ups=0.12, wpb=893.6, bsz=80, num_updates=4380, lr=4.2534e-06, gnorm=1.291, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=59091
2022-05-19 10:03:13 - progress_bar.py[line:274] - INFO: epoch 001:   4398 / 7081 loss=-0.004, score=1.339, ntokens=889.5, nsentences=80, sample_size=889.5, wps=101.8, ups=0.11, wpb=889.5, bsz=80, num_updates=4390, lr=4.251e-06, gnorm=1.339, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=59178
2022-05-19 10:04:39 - progress_bar.py[line:274] - INFO: epoch 001:   4408 / 7081 loss=-0.006, score=1.391, ntokens=887, nsentences=80, sample_size=887, wps=103.1, ups=0.12, wpb=887, bsz=80, num_updates=4400, lr=4.2486e-06, gnorm=1.232, clip=70, loss_scale=32, train_wall=86, gb_free=6.8, wall=59264
2022-05-19 10:06:06 - progress_bar.py[line:274] - INFO: epoch 001:   4418 / 7081 loss=-0.005, score=1.47, ntokens=878.7, nsentences=80, sample_size=878.7, wps=101.7, ups=0.12, wpb=878.7, bsz=80, num_updates=4410, lr=4.24619e-06, gnorm=1.186, clip=70, loss_scale=32, train_wall=86, gb_free=6.8, wall=59351
2022-05-19 10:07:33 - progress_bar.py[line:274] - INFO: epoch 001:   4428 / 7081 loss=-0.005, score=1.351, ntokens=884.7, nsentences=80, sample_size=884.7, wps=101.8, ups=0.12, wpb=884.7, bsz=80, num_updates=4420, lr=4.24379e-06, gnorm=1.377, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=59437
2022-05-19 10:08:59 - progress_bar.py[line:274] - INFO: epoch 001:   4438 / 7081 loss=-0.004, score=1.227, ntokens=879.8, nsentences=80, sample_size=879.8, wps=101.6, ups=0.12, wpb=879.8, bsz=80, num_updates=4430, lr=4.24138e-06, gnorm=1.481, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=59524
2022-05-19 10:10:26 - progress_bar.py[line:274] - INFO: epoch 001:   4448 / 7081 loss=-0.008, score=1.401, ntokens=879.1, nsentences=80, sample_size=879.1, wps=101, ups=0.11, wpb=879.1, bsz=80, num_updates=4440, lr=4.23898e-06, gnorm=2.125, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=59611
2022-05-19 10:11:52 - progress_bar.py[line:274] - INFO: epoch 001:   4458 / 7081 loss=-0.002, score=1.408, ntokens=871.9, nsentences=80, sample_size=871.9, wps=101.2, ups=0.12, wpb=871.9, bsz=80, num_updates=4450, lr=4.23658e-06, gnorm=1.215, clip=80, loss_scale=32, train_wall=86, gb_free=6.8, wall=59697
2022-05-19 10:13:20 - progress_bar.py[line:274] - INFO: epoch 001:   4468 / 7081 loss=-0.003, score=1.365, ntokens=887.4, nsentences=80, sample_size=887.4, wps=101.7, ups=0.11, wpb=887.4, bsz=80, num_updates=4460, lr=4.23417e-06, gnorm=1.617, clip=60, loss_scale=32, train_wall=87, gb_free=6.8, wall=59784
2022-05-19 10:14:46 - progress_bar.py[line:274] - INFO: epoch 001:   4478 / 7081 loss=-0.006, score=1.416, ntokens=875.8, nsentences=80, sample_size=875.8, wps=101.1, ups=0.12, wpb=875.8, bsz=80, num_updates=4470, lr=4.23177e-06, gnorm=1.5, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=59871
2022-05-19 10:16:14 - progress_bar.py[line:274] - INFO: epoch 001:   4488 / 7081 loss=-0.007, score=1.381, ntokens=893, nsentences=80, sample_size=893, wps=101.8, ups=0.11, wpb=893, bsz=80, num_updates=4480, lr=4.22937e-06, gnorm=1.989, clip=90, loss_scale=32, train_wall=88, gb_free=6.8, wall=59959
2022-05-19 10:17:41 - progress_bar.py[line:274] - INFO: epoch 001:   4498 / 7081 loss=-0.005, score=1.293, ntokens=888.3, nsentences=80, sample_size=888.3, wps=101.8, ups=0.11, wpb=888.3, bsz=80, num_updates=4490, lr=4.22696e-06, gnorm=1.644, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=60046
2022-05-19 10:19:08 - progress_bar.py[line:274] - INFO: epoch 001:   4508 / 7081 loss=-0.005, score=1.422, ntokens=875.5, nsentences=80, sample_size=875.5, wps=101.2, ups=0.12, wpb=875.5, bsz=80, num_updates=4500, lr=4.22456e-06, gnorm=1.895, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=60133
2022-05-19 10:19:08 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-19 11:01:52 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.356 | ntokens 112.177 | nsentences 10 | sample_size 112.177 | cider 1.453 | wps 109.4 | wpb 112.2 | bsz 10 | num_updates 4500 | best_cider 1.455
2022-05-19 11:01:52 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4500 updates
2022-05-19 11:01:52 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_4500.pt
2022-05-19 11:02:00 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_4500.pt
2022-05-19 11:02:57 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_4500.pt (epoch 1 @ 4500 updates, score 1.453) (writing took 65.3790859777946 seconds)
2022-05-19 11:04:22 - progress_bar.py[line:274] - INFO: epoch 001:   4518 / 7081 loss=-0.004, score=1.398, ntokens=888.1, nsentences=80, sample_size=888.1, wps=3.3, ups=0, wpb=888.1, bsz=80, num_updates=4510, lr=4.22215e-06, gnorm=1.683, clip=80, loss_scale=32, train_wall=85, gb_free=6.8, wall=62847
2022-05-19 11:05:48 - progress_bar.py[line:274] - INFO: epoch 001:   4528 / 7081 loss=-0.008, score=1.427, ntokens=873.8, nsentences=80, sample_size=873.8, wps=101.9, ups=0.12, wpb=873.8, bsz=80, num_updates=4520, lr=4.21975e-06, gnorm=1.639, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=62932
2022-05-19 11:07:14 - progress_bar.py[line:274] - INFO: epoch 001:   4538 / 7081 loss=-0.004, score=1.369, ntokens=877, nsentences=80, sample_size=877, wps=101.3, ups=0.12, wpb=877, bsz=80, num_updates=4530, lr=4.21735e-06, gnorm=1.605, clip=100, loss_scale=32, train_wall=86, gb_free=6.8, wall=63019
2022-05-19 11:08:41 - progress_bar.py[line:274] - INFO: epoch 001:   4548 / 7081 loss=-0.003, score=1.275, ntokens=882.3, nsentences=80, sample_size=882.3, wps=101.2, ups=0.11, wpb=882.3, bsz=80, num_updates=4540, lr=4.21494e-06, gnorm=1.391, clip=60, loss_scale=32, train_wall=87, gb_free=6.8, wall=63106
2022-05-19 11:10:08 - progress_bar.py[line:274] - INFO: epoch 001:   4558 / 7081 loss=-0.004, score=1.392, ntokens=883.3, nsentences=80, sample_size=883.3, wps=102, ups=0.12, wpb=883.3, bsz=80, num_updates=4550, lr=4.21254e-06, gnorm=1.273, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=63193
2022-05-19 11:11:34 - progress_bar.py[line:274] - INFO: epoch 001:   4568 / 7081 loss=-0.005, score=1.4, ntokens=892.8, nsentences=80, sample_size=892.8, wps=103.4, ups=0.12, wpb=892.8, bsz=80, num_updates=4560, lr=4.21014e-06, gnorm=1.207, clip=60, loss_scale=32, train_wall=86, gb_free=6.8, wall=63279
2022-05-19 11:13:01 - progress_bar.py[line:274] - INFO: epoch 001:   4578 / 7081 loss=-0.004, score=1.503, ntokens=866.8, nsentences=80, sample_size=866.8, wps=100.5, ups=0.12, wpb=866.8, bsz=80, num_updates=4570, lr=4.20773e-06, gnorm=1.37, clip=80, loss_scale=32, train_wall=86, gb_free=6.8, wall=63365
2022-05-19 11:14:28 - progress_bar.py[line:274] - INFO: epoch 001:   4588 / 7081 loss=-0.005, score=1.463, ntokens=885.7, nsentences=80, sample_size=885.7, wps=102, ups=0.12, wpb=885.7, bsz=80, num_updates=4580, lr=4.20533e-06, gnorm=1.504, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=63452
2022-05-19 11:15:55 - progress_bar.py[line:274] - INFO: epoch 001:   4598 / 7081 loss=-0.004, score=1.309, ntokens=892.4, nsentences=80, sample_size=892.4, wps=102.5, ups=0.11, wpb=892.4, bsz=80, num_updates=4590, lr=4.20292e-06, gnorm=1.702, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=63539
2022-05-19 11:17:22 - progress_bar.py[line:274] - INFO: epoch 001:   4608 / 7081 loss=-0.003, score=1.383, ntokens=892.2, nsentences=80, sample_size=892.2, wps=102.1, ups=0.11, wpb=892.2, bsz=80, num_updates=4600, lr=4.20052e-06, gnorm=1.891, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=63627
2022-05-19 11:18:48 - progress_bar.py[line:274] - INFO: epoch 001:   4618 / 7081 loss=-0.007, score=1.362, ntokens=881.8, nsentences=80, sample_size=881.8, wps=102.4, ups=0.12, wpb=881.8, bsz=80, num_updates=4610, lr=4.19812e-06, gnorm=1.777, clip=80, loss_scale=32, train_wall=86, gb_free=6.8, wall=63713
2022-05-19 11:20:16 - progress_bar.py[line:274] - INFO: epoch 001:   4628 / 7081 loss=-0.004, score=1.443, ntokens=887.8, nsentences=80, sample_size=887.8, wps=101.2, ups=0.11, wpb=887.8, bsz=80, num_updates=4620, lr=4.19571e-06, gnorm=1.42, clip=80, loss_scale=32, train_wall=88, gb_free=6.8, wall=63801
2022-05-19 11:21:43 - progress_bar.py[line:274] - INFO: epoch 001:   4638 / 7081 loss=-0.006, score=1.389, ntokens=890, nsentences=80, sample_size=890, wps=102, ups=0.11, wpb=890, bsz=80, num_updates=4630, lr=4.19331e-06, gnorm=1.483, clip=60, loss_scale=32, train_wall=87, gb_free=6.8, wall=63888
2022-05-19 11:23:10 - progress_bar.py[line:274] - INFO: epoch 001:   4648 / 7081 loss=-0.002, score=1.31, ntokens=887.9, nsentences=80, sample_size=887.9, wps=102.4, ups=0.12, wpb=887.9, bsz=80, num_updates=4640, lr=4.19091e-06, gnorm=1.575, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=63975
2022-05-19 11:24:36 - progress_bar.py[line:274] - INFO: epoch 001:   4658 / 7081 loss=-0.006, score=1.478, ntokens=884.8, nsentences=80, sample_size=884.8, wps=102.5, ups=0.12, wpb=884.8, bsz=80, num_updates=4650, lr=4.1885e-06, gnorm=1.789, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=64061
2022-05-19 11:26:03 - progress_bar.py[line:274] - INFO: epoch 001:   4668 / 7081 loss=-0.004, score=1.469, ntokens=887.7, nsentences=80, sample_size=887.7, wps=102.1, ups=0.12, wpb=887.7, bsz=80, num_updates=4660, lr=4.1861e-06, gnorm=1.61, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=64148
2022-05-19 11:27:30 - progress_bar.py[line:274] - INFO: epoch 001:   4678 / 7081 loss=-0.007, score=1.415, ntokens=885.1, nsentences=80, sample_size=885.1, wps=102, ups=0.12, wpb=885.1, bsz=80, num_updates=4670, lr=4.18369e-06, gnorm=1.538, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=64235
2022-05-19 11:28:57 - progress_bar.py[line:274] - INFO: epoch 001:   4688 / 7081 loss=-0.006, score=1.318, ntokens=885, nsentences=80, sample_size=885, wps=101.6, ups=0.11, wpb=885, bsz=80, num_updates=4680, lr=4.18129e-06, gnorm=1.472, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=64322
2022-05-19 11:30:23 - progress_bar.py[line:274] - INFO: epoch 001:   4698 / 7081 loss=-0.007, score=1.398, ntokens=877.8, nsentences=80, sample_size=877.8, wps=101.4, ups=0.12, wpb=877.8, bsz=80, num_updates=4690, lr=4.17889e-06, gnorm=1.251, clip=80, loss_scale=32, train_wall=86, gb_free=6.8, wall=64408
2022-05-19 11:31:51 - progress_bar.py[line:274] - INFO: epoch 001:   4708 / 7081 loss=-0.007, score=1.544, ntokens=902.9, nsentences=80, sample_size=902.9, wps=103.5, ups=0.11, wpb=902.9, bsz=80, num_updates=4700, lr=4.17648e-06, gnorm=1.295, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=64496
2022-05-19 11:33:17 - progress_bar.py[line:274] - INFO: epoch 001:   4718 / 7081 loss=-0.005, score=1.46, ntokens=880.7, nsentences=80, sample_size=880.7, wps=101.6, ups=0.12, wpb=880.7, bsz=80, num_updates=4710, lr=4.17408e-06, gnorm=2.081, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=64582
2022-05-19 11:34:44 - progress_bar.py[line:274] - INFO: epoch 001:   4728 / 7081 loss=-0.006, score=1.424, ntokens=882, nsentences=80, sample_size=882, wps=101.5, ups=0.12, wpb=882, bsz=80, num_updates=4720, lr=4.17168e-06, gnorm=1.831, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=64669
2022-05-19 11:36:11 - progress_bar.py[line:274] - INFO: epoch 001:   4738 / 7081 loss=-0.005, score=1.411, ntokens=879.7, nsentences=80, sample_size=879.7, wps=101.4, ups=0.12, wpb=879.7, bsz=80, num_updates=4730, lr=4.16927e-06, gnorm=1.458, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=64756
2022-05-19 11:37:38 - progress_bar.py[line:274] - INFO: epoch 001:   4748 / 7081 loss=-0.005, score=1.407, ntokens=887.9, nsentences=80, sample_size=887.9, wps=102, ups=0.11, wpb=887.9, bsz=80, num_updates=4740, lr=4.16687e-06, gnorm=1.473, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=64843
2022-05-19 11:39:05 - progress_bar.py[line:274] - INFO: epoch 001:   4758 / 7081 loss=-0.003, score=1.423, ntokens=878.2, nsentences=80, sample_size=878.2, wps=101.5, ups=0.12, wpb=878.2, bsz=80, num_updates=4750, lr=4.16446e-06, gnorm=1.197, clip=70, loss_scale=32, train_wall=86, gb_free=6.8, wall=64929
2022-05-19 11:40:32 - progress_bar.py[line:274] - INFO: epoch 001:   4768 / 7081 loss=-0.005, score=1.434, ntokens=873.8, nsentences=80, sample_size=873.8, wps=100.2, ups=0.11, wpb=873.8, bsz=80, num_updates=4760, lr=4.16206e-06, gnorm=1.681, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=65017
2022-05-19 11:41:59 - progress_bar.py[line:274] - INFO: epoch 001:   4778 / 7081 loss=-0.008, score=1.461, ntokens=888.6, nsentences=80, sample_size=888.6, wps=102, ups=0.11, wpb=888.6, bsz=80, num_updates=4770, lr=4.15966e-06, gnorm=1.539, clip=90, loss_scale=64, train_wall=87, gb_free=6.8, wall=65104
2022-05-19 11:43:26 - progress_bar.py[line:274] - INFO: epoch 001:   4788 / 7081 loss=-0.005, score=1.401, ntokens=880.8, nsentences=80, sample_size=880.8, wps=101.4, ups=0.12, wpb=880.8, bsz=80, num_updates=4780, lr=4.15725e-06, gnorm=1.488, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=65191
2022-05-19 11:44:52 - progress_bar.py[line:274] - INFO: epoch 001:   4798 / 7081 loss=-0.004, score=1.476, ntokens=880.4, nsentences=80, sample_size=880.4, wps=102.2, ups=0.12, wpb=880.4, bsz=80, num_updates=4790, lr=4.15485e-06, gnorm=1.757, clip=100, loss_scale=64, train_wall=86, gb_free=6.8, wall=65277
2022-05-19 11:46:19 - progress_bar.py[line:274] - INFO: epoch 001:   4808 / 7081 loss=-0.005, score=1.363, ntokens=870.9, nsentences=80, sample_size=870.9, wps=100.4, ups=0.12, wpb=870.9, bsz=80, num_updates=4800, lr=4.15245e-06, gnorm=1.281, clip=60, loss_scale=64, train_wall=87, gb_free=6.8, wall=65363
2022-05-19 11:47:45 - progress_bar.py[line:274] - INFO: epoch 001:   4818 / 7081 loss=-0.006, score=1.436, ntokens=880.6, nsentences=80, sample_size=880.6, wps=101.8, ups=0.12, wpb=880.6, bsz=80, num_updates=4810, lr=4.15004e-06, gnorm=1.47, clip=70, loss_scale=64, train_wall=86, gb_free=6.8, wall=65450
2022-05-19 11:49:11 - progress_bar.py[line:274] - INFO: epoch 001:   4828 / 7081 loss=-0.005, score=1.402, ntokens=880.9, nsentences=80, sample_size=880.9, wps=102.2, ups=0.12, wpb=880.9, bsz=80, num_updates=4820, lr=4.14764e-06, gnorm=1.254, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=65536
2022-05-19 11:50:38 - progress_bar.py[line:274] - INFO: epoch 001:   4838 / 7081 loss=-0.004, score=1.449, ntokens=878.1, nsentences=80, sample_size=878.1, wps=101.1, ups=0.12, wpb=878.1, bsz=80, num_updates=4830, lr=4.14524e-06, gnorm=1.366, clip=80, loss_scale=64, train_wall=87, gb_free=6.8, wall=65623
2022-05-19 11:52:05 - progress_bar.py[line:274] - INFO: epoch 001:   4848 / 7081 loss=-0.006, score=1.524, ntokens=879.5, nsentences=80, sample_size=879.5, wps=101.2, ups=0.12, wpb=879.5, bsz=80, num_updates=4840, lr=4.14283e-06, gnorm=1.276, clip=70, loss_scale=64, train_wall=87, gb_free=6.8, wall=65710
2022-05-19 11:53:32 - progress_bar.py[line:274] - INFO: epoch 001:   4858 / 7081 loss=-0.006, score=1.512, ntokens=884.4, nsentences=80, sample_size=884.4, wps=102.2, ups=0.12, wpb=884.4, bsz=80, num_updates=4850, lr=4.14043e-06, gnorm=1.724, clip=80, loss_scale=64, train_wall=86, gb_free=6.8, wall=65796
2022-05-19 11:54:58 - progress_bar.py[line:274] - INFO: epoch 001:   4868 / 7081 loss=-0.004, score=1.439, ntokens=888.7, nsentences=80, sample_size=888.7, wps=102.4, ups=0.12, wpb=888.7, bsz=80, num_updates=4860, lr=4.13802e-06, gnorm=2.168, clip=100, loss_scale=64, train_wall=87, gb_free=6.8, wall=65883
2022-05-19 11:55:24 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-05-19 11:56:34 - progress_bar.py[line:274] - INFO: epoch 001:   4879 / 7081 loss=-0.005, score=1.482, ntokens=879.5, nsentences=80, sample_size=879.5, wps=92.3, ups=0.1, wpb=879.5, bsz=80, num_updates=4870, lr=4.13562e-06, gnorm=1.594, clip=90, loss_scale=32, train_wall=95, gb_free=6.8, wall=65979
2022-05-19 11:58:01 - progress_bar.py[line:274] - INFO: epoch 001:   4889 / 7081 loss=-0.006, score=1.433, ntokens=888.4, nsentences=80, sample_size=888.4, wps=102.1, ups=0.11, wpb=888.4, bsz=80, num_updates=4880, lr=4.13322e-06, gnorm=1.633, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=66066
2022-05-19 11:59:27 - progress_bar.py[line:274] - INFO: epoch 001:   4899 / 7081 loss=-0.003, score=1.431, ntokens=875.5, nsentences=80, sample_size=875.5, wps=101, ups=0.12, wpb=875.5, bsz=80, num_updates=4890, lr=4.13081e-06, gnorm=1.433, clip=60, loss_scale=32, train_wall=87, gb_free=6.8, wall=66152
2022-05-19 12:00:54 - progress_bar.py[line:274] - INFO: epoch 001:   4909 / 7081 loss=-0.004, score=1.449, ntokens=883.1, nsentences=80, sample_size=883.1, wps=102, ups=0.12, wpb=883.1, bsz=80, num_updates=4900, lr=4.12841e-06, gnorm=1.618, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=66239
2022-05-19 12:02:20 - progress_bar.py[line:274] - INFO: epoch 001:   4919 / 7081 loss=-0.006, score=1.464, ntokens=885, nsentences=80, sample_size=885, wps=102.3, ups=0.12, wpb=885, bsz=80, num_updates=4910, lr=4.12601e-06, gnorm=1.499, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=66325
2022-05-19 12:03:47 - progress_bar.py[line:274] - INFO: epoch 001:   4929 / 7081 loss=-0.005, score=1.491, ntokens=887.3, nsentences=80, sample_size=887.3, wps=102.1, ups=0.12, wpb=887.3, bsz=80, num_updates=4920, lr=4.1236e-06, gnorm=1.723, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=66412
2022-05-19 12:05:14 - progress_bar.py[line:274] - INFO: epoch 001:   4939 / 7081 loss=-0.007, score=1.54, ntokens=878.6, nsentences=80, sample_size=878.6, wps=101, ups=0.11, wpb=878.6, bsz=80, num_updates=4930, lr=4.1212e-06, gnorm=1.313, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=66499
2022-05-19 12:06:41 - progress_bar.py[line:274] - INFO: epoch 001:   4949 / 7081 loss=-0.006, score=1.399, ntokens=890.9, nsentences=80, sample_size=890.9, wps=103.2, ups=0.12, wpb=890.9, bsz=80, num_updates=4940, lr=4.11879e-06, gnorm=1.592, clip=80, loss_scale=32, train_wall=86, gb_free=6.8, wall=66586
2022-05-19 12:08:08 - progress_bar.py[line:274] - INFO: epoch 001:   4959 / 7081 loss=-0.006, score=1.431, ntokens=889.8, nsentences=80, sample_size=889.8, wps=102.2, ups=0.11, wpb=889.8, bsz=80, num_updates=4950, lr=4.11639e-06, gnorm=1.568, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=66673
2022-05-19 12:09:35 - progress_bar.py[line:274] - INFO: epoch 001:   4969 / 7081 loss=-0.004, score=1.424, ntokens=892.2, nsentences=80, sample_size=892.2, wps=102.5, ups=0.11, wpb=892.2, bsz=80, num_updates=4960, lr=4.11399e-06, gnorm=1.489, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=66760
2022-05-19 12:11:02 - progress_bar.py[line:274] - INFO: epoch 001:   4979 / 7081 loss=-0.007, score=1.428, ntokens=887.5, nsentences=80, sample_size=887.5, wps=102, ups=0.11, wpb=887.5, bsz=80, num_updates=4970, lr=4.11158e-06, gnorm=1.618, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=66847
2022-05-19 12:12:29 - progress_bar.py[line:274] - INFO: epoch 001:   4989 / 7081 loss=-0.004, score=1.45, ntokens=883.1, nsentences=80, sample_size=883.1, wps=101.8, ups=0.12, wpb=883.1, bsz=80, num_updates=4980, lr=4.10918e-06, gnorm=1.839, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=66933
2022-05-19 12:13:55 - progress_bar.py[line:274] - INFO: epoch 001:   4999 / 7081 loss=-0.007, score=1.417, ntokens=876.5, nsentences=80, sample_size=876.5, wps=101.1, ups=0.12, wpb=876.5, bsz=80, num_updates=4990, lr=4.10678e-06, gnorm=1.798, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=67020
slice_id 1 seek offset 2500
2022-05-19 12:15:22 - progress_bar.py[line:274] - INFO: epoch 001:   5009 / 7081 loss=-0.004, score=1.379, ntokens=886.8, nsentences=80, sample_size=886.8, wps=102.7, ups=0.12, wpb=886.8, bsz=80, num_updates=5000, lr=4.10437e-06, gnorm=1.191, clip=80, loss_scale=32, train_wall=86, gb_free=6.8, wall=67106
2022-05-19 12:15:22 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 0 seek offset 0
2022-05-19 12:57:55 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.358 | ntokens 111.849 | nsentences 10 | sample_size 111.849 | cider 1.456 | wps 109.5 | wpb 111.8 | bsz 10 | num_updates 5000 | best_cider 1.456
2022-05-19 12:57:55 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5000 updates
2022-05-19 12:57:55 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_5000.pt
2022-05-19 12:58:04 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_5000.pt
2022-05-19 12:59:40 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_5000.pt (epoch 1 @ 5000 updates, score 1.456) (writing took 104.56756142689846 seconds)
2022-05-19 13:01:05 - progress_bar.py[line:274] - INFO: epoch 001:   5019 / 7081 loss=-0.006, score=1.304, ntokens=878.7, nsentences=80, sample_size=878.7, wps=3.2, ups=0, wpb=878.7, bsz=80, num_updates=5010, lr=4.10197e-06, gnorm=1.345, clip=70, loss_scale=32, train_wall=85, gb_free=6.8, wall=69850
2022-05-19 13:02:32 - progress_bar.py[line:274] - INFO: epoch 001:   5029 / 7081 loss=-0.006, score=1.445, ntokens=880.5, nsentences=80, sample_size=880.5, wps=101.7, ups=0.12, wpb=880.5, bsz=80, num_updates=5020, lr=4.09956e-06, gnorm=1.566, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=69937
2022-05-19 13:03:58 - progress_bar.py[line:274] - INFO: epoch 001:   5039 / 7081 loss=-0.004, score=1.453, ntokens=880.5, nsentences=80, sample_size=880.5, wps=101.9, ups=0.12, wpb=880.5, bsz=80, num_updates=5030, lr=4.09716e-06, gnorm=1.502, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=70023
2022-05-19 13:05:25 - progress_bar.py[line:274] - INFO: epoch 001:   5049 / 7081 loss=-0.005, score=1.372, ntokens=876.9, nsentences=80, sample_size=876.9, wps=100.6, ups=0.11, wpb=876.9, bsz=80, num_updates=5040, lr=4.09476e-06, gnorm=1.532, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=70110
2022-05-19 13:06:52 - progress_bar.py[line:274] - INFO: epoch 001:   5059 / 7081 loss=-0.007, score=1.441, ntokens=888.8, nsentences=80, sample_size=888.8, wps=102.3, ups=0.12, wpb=888.8, bsz=80, num_updates=5050, lr=4.09235e-06, gnorm=2.403, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=70197
2022-05-19 13:08:18 - progress_bar.py[line:274] - INFO: epoch 001:   5069 / 7081 loss=-0.006, score=1.409, ntokens=873.6, nsentences=80, sample_size=873.6, wps=101.4, ups=0.12, wpb=873.6, bsz=80, num_updates=5060, lr=4.08995e-06, gnorm=1.569, clip=80, loss_scale=32, train_wall=86, gb_free=6.8, wall=70283
2022-05-19 13:09:45 - progress_bar.py[line:274] - INFO: epoch 001:   5079 / 7081 loss=-0.005, score=1.488, ntokens=874.6, nsentences=80, sample_size=874.6, wps=101.2, ups=0.12, wpb=874.6, bsz=80, num_updates=5070, lr=4.08755e-06, gnorm=1.65, clip=70, loss_scale=32, train_wall=86, gb_free=6.8, wall=70370
2022-05-19 13:11:12 - progress_bar.py[line:274] - INFO: epoch 001:   5089 / 7081 loss=-0.001, score=1.289, ntokens=883, nsentences=80, sample_size=883, wps=101.9, ups=0.12, wpb=883, bsz=80, num_updates=5080, lr=4.08514e-06, gnorm=2.119, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=70456
2022-05-19 13:12:37 - progress_bar.py[line:274] - INFO: epoch 001:   5099 / 7081 loss=-0.004, score=1.31, ntokens=877.9, nsentences=80, sample_size=877.9, wps=102.3, ups=0.12, wpb=877.9, bsz=80, num_updates=5090, lr=4.08274e-06, gnorm=1.456, clip=70, loss_scale=32, train_wall=86, gb_free=6.8, wall=70542
2022-05-19 13:14:04 - progress_bar.py[line:274] - INFO: epoch 001:   5109 / 7081 loss=-0.005, score=1.46, ntokens=876.3, nsentences=80, sample_size=876.3, wps=101, ups=0.12, wpb=876.3, bsz=80, num_updates=5100, lr=4.08033e-06, gnorm=1.356, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=70629
2022-05-19 13:15:31 - progress_bar.py[line:274] - INFO: epoch 001:   5119 / 7081 loss=-0.004, score=1.336, ntokens=879.4, nsentences=80, sample_size=879.4, wps=101.4, ups=0.12, wpb=879.4, bsz=80, num_updates=5110, lr=4.07793e-06, gnorm=1.134, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=70716
2022-05-19 13:16:58 - progress_bar.py[line:274] - INFO: epoch 001:   5129 / 7081 loss=-0.004, score=1.515, ntokens=890.2, nsentences=80, sample_size=890.2, wps=102.7, ups=0.12, wpb=890.2, bsz=80, num_updates=5120, lr=4.07553e-06, gnorm=1.565, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=70802
2022-05-19 13:18:24 - progress_bar.py[line:274] - INFO: epoch 001:   5139 / 7081 loss=-0.006, score=1.454, ntokens=876, nsentences=80, sample_size=876, wps=101.5, ups=0.12, wpb=876, bsz=80, num_updates=5130, lr=4.07312e-06, gnorm=1.345, clip=60, loss_scale=32, train_wall=86, gb_free=6.8, wall=70889
2022-05-19 13:19:51 - progress_bar.py[line:274] - INFO: epoch 001:   5149 / 7081 loss=-0.006, score=1.397, ntokens=886.5, nsentences=80, sample_size=886.5, wps=101.7, ups=0.11, wpb=886.5, bsz=80, num_updates=5140, lr=4.07072e-06, gnorm=1.666, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=70976
2022-05-19 13:21:18 - progress_bar.py[line:274] - INFO: epoch 001:   5159 / 7081 loss=-0.004, score=1.336, ntokens=885.2, nsentences=80, sample_size=885.2, wps=102.2, ups=0.12, wpb=885.2, bsz=80, num_updates=5150, lr=4.06832e-06, gnorm=1.601, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=71062
2022-05-19 13:22:43 - progress_bar.py[line:274] - INFO: epoch 001:   5169 / 7081 loss=-0.006, score=1.439, ntokens=872.1, nsentences=80, sample_size=872.1, wps=101.8, ups=0.12, wpb=872.1, bsz=80, num_updates=5160, lr=4.06591e-06, gnorm=1.832, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=71148
2022-05-19 13:24:10 - progress_bar.py[line:274] - INFO: epoch 001:   5179 / 7081 loss=-0.003, score=1.462, ntokens=881.4, nsentences=80, sample_size=881.4, wps=101.6, ups=0.12, wpb=881.4, bsz=80, num_updates=5170, lr=4.06351e-06, gnorm=1.504, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=71235
2022-05-19 13:25:36 - progress_bar.py[line:274] - INFO: epoch 001:   5189 / 7081 loss=-0.007, score=1.472, ntokens=867.4, nsentences=80, sample_size=867.4, wps=100.7, ups=0.12, wpb=867.4, bsz=80, num_updates=5180, lr=4.0611e-06, gnorm=1.425, clip=80, loss_scale=32, train_wall=86, gb_free=6.8, wall=71321
2022-05-19 13:27:03 - progress_bar.py[line:274] - INFO: epoch 001:   5199 / 7081 loss=-0.006, score=1.516, ntokens=874.6, nsentences=80, sample_size=874.6, wps=101.2, ups=0.12, wpb=874.6, bsz=80, num_updates=5190, lr=4.0587e-06, gnorm=2.217, clip=100, loss_scale=32, train_wall=86, gb_free=6.8, wall=71407
2022-05-19 13:28:29 - progress_bar.py[line:274] - INFO: epoch 001:   5209 / 7081 loss=-0.006, score=1.401, ntokens=884.1, nsentences=80, sample_size=884.1, wps=102, ups=0.12, wpb=884.1, bsz=80, num_updates=5200, lr=4.0563e-06, gnorm=1.698, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=71494
2022-05-19 13:29:57 - progress_bar.py[line:274] - INFO: epoch 001:   5219 / 7081 loss=-0.006, score=1.41, ntokens=882.7, nsentences=80, sample_size=882.7, wps=101.1, ups=0.11, wpb=882.7, bsz=80, num_updates=5210, lr=4.05389e-06, gnorm=1.4, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=71581
2022-05-19 13:31:23 - progress_bar.py[line:274] - INFO: epoch 001:   5229 / 7081 loss=-0.003, score=1.449, ntokens=886, nsentences=80, sample_size=886, wps=102.1, ups=0.12, wpb=886, bsz=80, num_updates=5220, lr=4.05149e-06, gnorm=1.969, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=71668
2022-05-19 13:32:50 - progress_bar.py[line:274] - INFO: epoch 001:   5239 / 7081 loss=-0.004, score=1.428, ntokens=875.3, nsentences=80, sample_size=875.3, wps=101.5, ups=0.12, wpb=875.3, bsz=80, num_updates=5230, lr=4.04909e-06, gnorm=1.486, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=71754
2022-05-19 13:34:17 - progress_bar.py[line:274] - INFO: epoch 001:   5249 / 7081 loss=-0.009, score=1.371, ntokens=881, nsentences=80, sample_size=881, wps=101.2, ups=0.11, wpb=881, bsz=80, num_updates=5240, lr=4.04668e-06, gnorm=1.748, clip=90, loss_scale=32, train_wall=87, gb_free=6.8, wall=71841
2022-05-19 13:35:43 - progress_bar.py[line:274] - INFO: epoch 001:   5259 / 7081 loss=-0.006, score=1.416, ntokens=881.4, nsentences=80, sample_size=881.4, wps=101.6, ups=0.12, wpb=881.4, bsz=80, num_updates=5250, lr=4.04428e-06, gnorm=1.597, clip=80, loss_scale=32, train_wall=87, gb_free=6.8, wall=71928
2022-05-19 13:37:11 - progress_bar.py[line:274] - INFO: epoch 001:   5269 / 7081 loss=-0.003, score=1.448, ntokens=880.8, nsentences=80, sample_size=880.8, wps=100.7, ups=0.11, wpb=880.8, bsz=80, num_updates=5260, lr=4.04187e-06, gnorm=1.257, clip=70, loss_scale=32, train_wall=87, gb_free=6.8, wall=72016
2022-05-19 13:38:38 - progress_bar.py[line:274] - INFO: epoch 001:   5279 / 7081 loss=-0.005, score=1.405, ntokens=883.8, nsentences=80, sample_size=883.8, wps=101.9, ups=0.12, wpb=883.8, bsz=80, num_updates=5270, lr=4.03947e-06, gnorm=1.819, clip=100, loss_scale=32, train_wall=87, gb_free=6.8, wall=72102
2022-05-19 13:40:05 - progress_bar.py[line:274] - INFO: epoch 001:   5289 / 7081 loss=-0.005, score=1.42, ntokens=873.7, nsentences=80, sample_size=873.7, wps=100.5, ups=0.12, wpb=873.7, bsz=80, num_updates=5280, lr=4.03707e-06, gnorm=1.719, clip=60, loss_scale=32, train_wall=87, gb_free=6.8, wall=72189
2022-05-19 13:41:31 - progress_bar.py[line:274] - INFO: epoch 001:   5299 / 7081 loss=-0.001, score=1.397, ntokens=882.4, nsentences=80, sample_size=882.4, wps=102.1, ups=0.12, wpb=882.4, bsz=80, num_updates=5290, lr=4.03466e-06, gnorm=1.549, clip=90, loss_scale=32, train_wall=86, gb_free=6.8, wall=72276
2022-05-19 13:42:58 - progress_bar.py[line:274] - INFO: epoch 001:   5309 / 7081 loss=-0.003, score=1.446, ntokens=896.9, nsentences=80, sample_size=896.9, wps=102.8, ups=0.11, wpb=896.9, bsz=80, num_updates=5300, lr=4.03226e-06, gnorm=1.486, clip=50, loss_scale=32, train_wall=87, gb_free=6.8, wall=72363
2022-05-19 13:44:28 - progress_bar.py[line:274] - INFO: epoch 001:   5319 / 7081 loss=-0.005, score=1.33, ntokens=883.4, nsentences=80, sample_size=883.4, wps=98.2, ups=0.11, wpb=883.4, bsz=80, num_updates=5310, lr=4.02986e-06, gnorm=1.577, clip=100, loss_scale=32, train_wall=90, gb_free=6.8, wall=72453
2022-05-19 13:45:47 - progress_bar.py[line:274] - INFO: epoch 001:   5329 / 7081 loss=-0.003, score=1.502, ntokens=883.9, nsentences=80, sample_size=883.9, wps=112.8, ups=0.13, wpb=883.9, bsz=80, num_updates=5320, lr=4.02745e-06, gnorm=1.935, clip=70, loss_scale=32, train_wall=78, gb_free=6.8, wall=72531
2022-05-19 13:47:01 - progress_bar.py[line:274] - INFO: epoch 001:   5339 / 7081 loss=-0.002, score=1.378, ntokens=890.6, nsentences=80, sample_size=890.6, wps=120.2, ups=0.13, wpb=890.6, bsz=80, num_updates=5330, lr=4.02505e-06, gnorm=1.195, clip=80, loss_scale=32, train_wall=74, gb_free=6.8, wall=72606
2022-05-19 13:48:15 - progress_bar.py[line:274] - INFO: epoch 001:   5349 / 7081 loss=-0.008, score=1.395, ntokens=883.7, nsentences=80, sample_size=883.7, wps=118.5, ups=0.13, wpb=883.7, bsz=80, num_updates=5340, lr=4.02265e-06, gnorm=1.596, clip=80, loss_scale=32, train_wall=75, gb_free=6.8, wall=72680
2022-05-19 13:49:30 - progress_bar.py[line:274] - INFO: epoch 001:   5359 / 7081 loss=-0.005, score=1.397, ntokens=887.9, nsentences=80, sample_size=887.9, wps=119.2, ups=0.13, wpb=887.9, bsz=80, num_updates=5350, lr=4.02024e-06, gnorm=1.439, clip=90, loss_scale=32, train_wall=74, gb_free=6.8, wall=72755
2022-05-19 13:50:44 - progress_bar.py[line:274] - INFO: epoch 001:   5369 / 7081 loss=-0.004, score=1.383, ntokens=887.8, nsentences=80, sample_size=887.8, wps=118.9, ups=0.13, wpb=887.8, bsz=80, num_updates=5360, lr=4.01784e-06, gnorm=1.873, clip=80, loss_scale=32, train_wall=75, gb_free=6.8, wall=72829
2022-05-19 13:51:59 - progress_bar.py[line:274] - INFO: epoch 001:   5379 / 7081 loss=-0.005, score=1.428, ntokens=889.1, nsentences=80, sample_size=889.1, wps=119.9, ups=0.13, wpb=889.1, bsz=80, num_updates=5370, lr=4.01543e-06, gnorm=1.495, clip=70, loss_scale=32, train_wall=74, gb_free=6.8, wall=72903
2022-05-19 13:53:17 - progress_bar.py[line:274] - INFO: epoch 001:   5389 / 7081 loss=-0.005, score=1.463, ntokens=895, nsentences=80, sample_size=895, wps=114.8, ups=0.13, wpb=895, bsz=80, num_updates=5380, lr=4.01303e-06, gnorm=1.642, clip=100, loss_scale=64, train_wall=78, gb_free=6.8, wall=72981
2022-05-19 13:54:39 - progress_bar.py[line:274] - INFO: epoch 001:   5399 / 7081 loss=-0.003, score=1.502, ntokens=868.4, nsentences=80, sample_size=868.4, wps=105.1, ups=0.12, wpb=868.4, bsz=80, num_updates=5390, lr=4.01063e-06, gnorm=1.573, clip=90, loss_scale=64, train_wall=83, gb_free=6.8, wall=73064
2022-05-19 13:56:02 - progress_bar.py[line:274] - INFO: epoch 001:   5409 / 7081 loss=-0.006, score=1.504, ntokens=893, nsentences=80, sample_size=893, wps=107.4, ups=0.12, wpb=893, bsz=80, num_updates=5400, lr=4.00822e-06, gnorm=1.774, clip=100, loss_scale=64, train_wall=83, gb_free=6.8, wall=73147
2022-05-19 13:57:21 - progress_bar.py[line:274] - INFO: epoch 001:   5419 / 7081 loss=-0.003, score=1.463, ntokens=885.9, nsentences=80, sample_size=885.9, wps=112.2, ups=0.13, wpb=885.9, bsz=80, num_updates=5410, lr=4.00582e-06, gnorm=1.655, clip=70, loss_scale=64, train_wall=79, gb_free=6.8, wall=73226
2022-05-19 13:58:36 - progress_bar.py[line:274] - INFO: epoch 001:   5429 / 7081 loss=-0.006, score=1.456, ntokens=904.5, nsentences=80, sample_size=904.5, wps=120.5, ups=0.13, wpb=904.5, bsz=80, num_updates=5420, lr=4.00342e-06, gnorm=1.338, clip=80, loss_scale=64, train_wall=75, gb_free=6.8, wall=73301
2022-05-19 13:59:51 - progress_bar.py[line:274] - INFO: epoch 001:   5439 / 7081 loss=-0.005, score=1.382, ntokens=883.3, nsentences=80, sample_size=883.3, wps=118.9, ups=0.13, wpb=883.3, bsz=80, num_updates=5430, lr=4.00101e-06, gnorm=1.498, clip=90, loss_scale=64, train_wall=74, gb_free=6.8, wall=73375
2022-05-19 14:01:05 - progress_bar.py[line:274] - INFO: epoch 001:   5449 / 7081 loss=-0.005, score=1.569, ntokens=894.4, nsentences=80, sample_size=894.4, wps=119.8, ups=0.13, wpb=894.4, bsz=80, num_updates=5440, lr=3.99861e-06, gnorm=1.418, clip=70, loss_scale=64, train_wall=75, gb_free=6.8, wall=73450
2022-05-19 14:02:23 - progress_bar.py[line:274] - INFO: epoch 001:   5459 / 7081 loss=-0.006, score=1.41, ntokens=890.8, nsentences=80, sample_size=890.8, wps=113.9, ups=0.13, wpb=890.8, bsz=80, num_updates=5450, lr=3.9962e-06, gnorm=1.4, clip=90, loss_scale=64, train_wall=78, gb_free=6.8, wall=73528
2022-05-19 14:03:46 - progress_bar.py[line:274] - INFO: epoch 001:   5469 / 7081 loss=-0.006, score=1.445, ntokens=890.2, nsentences=80, sample_size=890.2, wps=107.9, ups=0.12, wpb=890.2, bsz=80, num_updates=5460, lr=3.9938e-06, gnorm=1.271, clip=60, loss_scale=64, train_wall=82, gb_free=6.8, wall=73611
2022-05-19 14:05:09 - progress_bar.py[line:274] - INFO: epoch 001:   5479 / 7081 loss=-0.005, score=1.415, ntokens=894.4, nsentences=80, sample_size=894.4, wps=107.9, ups=0.12, wpb=894.4, bsz=80, num_updates=5470, lr=3.9914e-06, gnorm=1.146, clip=60, loss_scale=64, train_wall=83, gb_free=6.8, wall=73694
2022-05-19 14:06:31 - progress_bar.py[line:274] - INFO: epoch 001:   5489 / 7081 loss=-0.003, score=1.356, ntokens=884.7, nsentences=80, sample_size=884.7, wps=107.2, ups=0.12, wpb=884.7, bsz=80, num_updates=5480, lr=3.98899e-06, gnorm=1.16, clip=60, loss_scale=64, train_wall=82, gb_free=6.8, wall=73776
2022-05-19 14:07:54 - progress_bar.py[line:274] - INFO: epoch 001:   5499 / 7081 loss=-0.006, score=1.408, ntokens=906.2, nsentences=80, sample_size=906.2, wps=109.6, ups=0.12, wpb=906.2, bsz=80, num_updates=5490, lr=3.98659e-06, gnorm=1.237, clip=80, loss_scale=64, train_wall=83, gb_free=6.8, wall=73859
2022-05-19 14:09:17 - progress_bar.py[line:274] - INFO: epoch 001:   5509 / 7081 loss=-0.004, score=1.39, ntokens=885.4, nsentences=80, sample_size=885.4, wps=107.3, ups=0.12, wpb=885.4, bsz=80, num_updates=5500, lr=3.98419e-06, gnorm=1.446, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=73941
2022-05-19 14:09:17 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-19 14:50:52 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.36 | ntokens 112.994 | nsentences 10 | sample_size 112.994 | cider 1.451 | wps 113.2 | wpb 113 | bsz 10 | num_updates 5500 | best_cider 1.456
2022-05-19 14:50:52 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5500 updates
2022-05-19 14:50:52 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_5500.pt
2022-05-19 14:51:00 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_5500.pt
2022-05-19 14:51:54 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_5500.pt (epoch 1 @ 5500 updates, score 1.451) (writing took 62.16790563799441 seconds)
2022-05-19 14:53:11 - progress_bar.py[line:274] - INFO: epoch 001:   5519 / 7081 loss=-0.007, score=1.478, ntokens=894.7, nsentences=80, sample_size=894.7, wps=3.4, ups=0, wpb=894.7, bsz=80, num_updates=5510, lr=3.98178e-06, gnorm=1.429, clip=80, loss_scale=64, train_wall=77, gb_free=6.8, wall=76576
2022-05-19 14:54:29 - progress_bar.py[line:274] - INFO: epoch 001:   5529 / 7081 loss=-0.004, score=1.405, ntokens=879.2, nsentences=80, sample_size=879.2, wps=113.1, ups=0.13, wpb=879.2, bsz=80, num_updates=5520, lr=3.97938e-06, gnorm=1.509, clip=80, loss_scale=64, train_wall=78, gb_free=6.8, wall=76653
2022-05-19 14:55:51 - progress_bar.py[line:274] - INFO: epoch 001:   5539 / 7081 loss=-0.006, score=1.52, ntokens=895.9, nsentences=80, sample_size=895.9, wps=109.1, ups=0.12, wpb=895.9, bsz=80, num_updates=5530, lr=3.97697e-06, gnorm=1.439, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=76736
2022-05-19 14:57:13 - progress_bar.py[line:274] - INFO: epoch 001:   5549 / 7081 loss=-0.003, score=1.356, ntokens=887.7, nsentences=80, sample_size=887.7, wps=108.4, ups=0.12, wpb=887.7, bsz=80, num_updates=5540, lr=3.97457e-06, gnorm=1.838, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=76818
2022-05-19 14:58:36 - progress_bar.py[line:274] - INFO: epoch 001:   5559 / 7081 loss=-0.006, score=1.444, ntokens=893.6, nsentences=80, sample_size=893.6, wps=107.1, ups=0.12, wpb=893.6, bsz=80, num_updates=5550, lr=3.97217e-06, gnorm=1.287, clip=70, loss_scale=64, train_wall=83, gb_free=6.8, wall=76901
2022-05-19 14:59:58 - progress_bar.py[line:274] - INFO: epoch 001:   5569 / 7081 loss=-0.002, score=1.361, ntokens=896.3, nsentences=80, sample_size=896.3, wps=108.9, ups=0.12, wpb=896.3, bsz=80, num_updates=5560, lr=3.96976e-06, gnorm=1.194, clip=50, loss_scale=64, train_wall=82, gb_free=6.8, wall=76983
2022-05-19 15:01:22 - progress_bar.py[line:274] - INFO: epoch 001:   5579 / 7081 loss=-0.003, score=1.507, ntokens=892.7, nsentences=80, sample_size=892.7, wps=107.3, ups=0.12, wpb=892.7, bsz=80, num_updates=5570, lr=3.96736e-06, gnorm=1.363, clip=70, loss_scale=64, train_wall=83, gb_free=6.8, wall=77066
2022-05-19 15:02:44 - progress_bar.py[line:274] - INFO: epoch 001:   5589 / 7081 loss=-0.005, score=1.44, ntokens=891.5, nsentences=80, sample_size=891.5, wps=107.6, ups=0.12, wpb=891.5, bsz=80, num_updates=5580, lr=3.96496e-06, gnorm=1.099, clip=40, loss_scale=64, train_wall=83, gb_free=6.8, wall=77149
2022-05-19 15:04:07 - progress_bar.py[line:274] - INFO: epoch 001:   5599 / 7081 loss=-0.006, score=1.431, ntokens=889.6, nsentences=80, sample_size=889.6, wps=107.9, ups=0.12, wpb=889.6, bsz=80, num_updates=5590, lr=3.96255e-06, gnorm=1.695, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=77232
2022-05-19 15:05:30 - progress_bar.py[line:274] - INFO: epoch 001:   5609 / 7081 loss=-0.005, score=1.416, ntokens=877.7, nsentences=80, sample_size=877.7, wps=106.3, ups=0.12, wpb=877.7, bsz=80, num_updates=5600, lr=3.96015e-06, gnorm=1.695, clip=60, loss_scale=64, train_wall=82, gb_free=6.8, wall=77314
2022-05-19 15:06:53 - progress_bar.py[line:274] - INFO: epoch 001:   5619 / 7081 loss=-0.004, score=1.457, ntokens=893.5, nsentences=80, sample_size=893.5, wps=107.4, ups=0.12, wpb=893.5, bsz=80, num_updates=5610, lr=3.95774e-06, gnorm=1.572, clip=90, loss_scale=64, train_wall=83, gb_free=6.8, wall=77397
2022-05-19 15:08:15 - progress_bar.py[line:274] - INFO: epoch 001:   5629 / 7081 loss=-0.004, score=1.441, ntokens=891.6, nsentences=80, sample_size=891.6, wps=108.3, ups=0.12, wpb=891.6, bsz=80, num_updates=5620, lr=3.95534e-06, gnorm=1.282, clip=70, loss_scale=64, train_wall=82, gb_free=6.8, wall=77480
2022-05-19 15:09:37 - progress_bar.py[line:274] - INFO: epoch 001:   5639 / 7081 loss=-0.005, score=1.411, ntokens=885.4, nsentences=80, sample_size=885.4, wps=107.7, ups=0.12, wpb=885.4, bsz=80, num_updates=5630, lr=3.95294e-06, gnorm=1.57, clip=70, loss_scale=64, train_wall=82, gb_free=6.8, wall=77562
2022-05-19 15:11:00 - progress_bar.py[line:274] - INFO: epoch 001:   5649 / 7081 loss=-0.006, score=1.363, ntokens=885.6, nsentences=80, sample_size=885.6, wps=107.4, ups=0.12, wpb=885.6, bsz=80, num_updates=5640, lr=3.95053e-06, gnorm=1.394, clip=100, loss_scale=64, train_wall=82, gb_free=6.7, wall=77645
2022-05-19 15:12:23 - progress_bar.py[line:274] - INFO: epoch 001:   5659 / 7081 loss=-0.005, score=1.525, ntokens=891.7, nsentences=80, sample_size=891.7, wps=106.8, ups=0.12, wpb=891.7, bsz=80, num_updates=5650, lr=3.94813e-06, gnorm=1.253, clip=70, loss_scale=64, train_wall=83, gb_free=6.8, wall=77728
2022-05-19 15:13:47 - progress_bar.py[line:274] - INFO: epoch 001:   5669 / 7081 loss=-0.006, score=1.39, ntokens=887.5, nsentences=80, sample_size=887.5, wps=106.2, ups=0.12, wpb=887.5, bsz=80, num_updates=5660, lr=3.94573e-06, gnorm=1.417, clip=90, loss_scale=64, train_wall=83, gb_free=6.8, wall=77812
2022-05-19 15:15:09 - progress_bar.py[line:274] - INFO: epoch 001:   5679 / 7081 loss=-0.008, score=1.47, ntokens=893.2, nsentences=80, sample_size=893.2, wps=108.4, ups=0.12, wpb=893.2, bsz=80, num_updates=5670, lr=3.94332e-06, gnorm=1.399, clip=70, loss_scale=64, train_wall=82, gb_free=6.8, wall=77894
2022-05-19 15:16:31 - progress_bar.py[line:274] - INFO: epoch 001:   5689 / 7081 loss=-0.006, score=1.437, ntokens=886.3, nsentences=80, sample_size=886.3, wps=107.7, ups=0.12, wpb=886.3, bsz=80, num_updates=5680, lr=3.94092e-06, gnorm=1.101, clip=60, loss_scale=64, train_wall=82, gb_free=6.8, wall=77976
2022-05-19 15:17:53 - progress_bar.py[line:274] - INFO: epoch 001:   5699 / 7081 loss=-0.004, score=1.391, ntokens=894.7, nsentences=80, sample_size=894.7, wps=109.3, ups=0.12, wpb=894.7, bsz=80, num_updates=5690, lr=3.93851e-06, gnorm=1.154, clip=70, loss_scale=64, train_wall=82, gb_free=6.8, wall=78058
2022-05-19 15:19:11 - progress_bar.py[line:274] - INFO: epoch 001:   5709 / 7081 loss=-0.005, score=1.417, ntokens=892.9, nsentences=80, sample_size=892.9, wps=115.5, ups=0.13, wpb=892.9, bsz=80, num_updates=5700, lr=3.93611e-06, gnorm=1.415, clip=80, loss_scale=64, train_wall=77, gb_free=6.8, wall=78136
2022-05-19 15:20:29 - progress_bar.py[line:274] - INFO: epoch 001:   5719 / 7081 loss=-0.009, score=1.392, ntokens=894.2, nsentences=80, sample_size=894.2, wps=114.8, ups=0.13, wpb=894.2, bsz=80, num_updates=5710, lr=3.93371e-06, gnorm=1.293, clip=60, loss_scale=64, train_wall=78, gb_free=6.8, wall=78213
2022-05-19 15:21:50 - progress_bar.py[line:274] - INFO: epoch 001:   5729 / 7081 loss=-0.005, score=1.375, ntokens=873.1, nsentences=80, sample_size=873.1, wps=107.5, ups=0.12, wpb=873.1, bsz=80, num_updates=5720, lr=3.9313e-06, gnorm=1.804, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=78295
2022-05-19 15:23:13 - progress_bar.py[line:274] - INFO: epoch 001:   5739 / 7081 loss=-0.002, score=1.378, ntokens=897.1, nsentences=80, sample_size=897.1, wps=107.9, ups=0.12, wpb=897.1, bsz=80, num_updates=5730, lr=3.9289e-06, gnorm=1.281, clip=70, loss_scale=64, train_wall=83, gb_free=6.8, wall=78378
2022-05-19 15:24:35 - progress_bar.py[line:274] - INFO: epoch 001:   5749 / 7081 loss=-0.005, score=1.365, ntokens=888.7, nsentences=80, sample_size=888.7, wps=108, ups=0.12, wpb=888.7, bsz=80, num_updates=5740, lr=3.9265e-06, gnorm=1.627, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=78460
2022-05-19 15:25:58 - progress_bar.py[line:274] - INFO: epoch 001:   5759 / 7081 loss=-0.003, score=1.309, ntokens=890.8, nsentences=80, sample_size=890.8, wps=107.6, ups=0.12, wpb=890.8, bsz=80, num_updates=5750, lr=3.92409e-06, gnorm=1.296, clip=80, loss_scale=64, train_wall=83, gb_free=6.8, wall=78543
2022-05-19 15:27:21 - progress_bar.py[line:274] - INFO: epoch 001:   5769 / 7081 loss=-0.006, score=1.435, ntokens=884.4, nsentences=80, sample_size=884.4, wps=106.7, ups=0.12, wpb=884.4, bsz=80, num_updates=5760, lr=3.92169e-06, gnorm=1.708, clip=80, loss_scale=64, train_wall=83, gb_free=6.8, wall=78626
2022-05-19 15:28:44 - progress_bar.py[line:274] - INFO: epoch 001:   5779 / 7081 loss=-0.006, score=1.42, ntokens=888.6, nsentences=80, sample_size=888.6, wps=107.1, ups=0.12, wpb=888.6, bsz=80, num_updates=5770, lr=3.91928e-06, gnorm=1.514, clip=90, loss_scale=64, train_wall=83, gb_free=6.8, wall=78709
2022-05-19 15:30:06 - progress_bar.py[line:274] - INFO: epoch 001:   5789 / 7081 loss=-0.005, score=1.4, ntokens=880.4, nsentences=80, sample_size=880.4, wps=107, ups=0.12, wpb=880.4, bsz=80, num_updates=5780, lr=3.91688e-06, gnorm=1.243, clip=60, loss_scale=64, train_wall=82, gb_free=6.8, wall=78791
2022-05-19 15:31:29 - progress_bar.py[line:274] - INFO: epoch 001:   5799 / 7081 loss=-0.007, score=1.39, ntokens=897.5, nsentences=80, sample_size=897.5, wps=108.5, ups=0.12, wpb=897.5, bsz=80, num_updates=5790, lr=3.91448e-06, gnorm=2.135, clip=80, loss_scale=64, train_wall=83, gb_free=6.8, wall=78874
2022-05-19 15:32:51 - progress_bar.py[line:274] - INFO: epoch 001:   5809 / 7081 loss=-0.006, score=1.45, ntokens=888.3, nsentences=80, sample_size=888.3, wps=108.1, ups=0.12, wpb=888.3, bsz=80, num_updates=5800, lr=3.91207e-06, gnorm=1.36, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=78956
2022-05-19 15:34:13 - progress_bar.py[line:274] - INFO: epoch 001:   5819 / 7081 loss=-0.005, score=1.498, ntokens=887.5, nsentences=80, sample_size=887.5, wps=107.8, ups=0.12, wpb=887.5, bsz=80, num_updates=5810, lr=3.90967e-06, gnorm=1.194, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=79038
2022-05-19 15:35:36 - progress_bar.py[line:274] - INFO: epoch 001:   5829 / 7081 loss=-0.005, score=1.299, ntokens=880.5, nsentences=80, sample_size=880.5, wps=106.8, ups=0.12, wpb=880.5, bsz=80, num_updates=5820, lr=3.90727e-06, gnorm=1.079, clip=50, loss_scale=64, train_wall=82, gb_free=6.8, wall=79121
2022-05-19 15:36:58 - progress_bar.py[line:274] - INFO: epoch 001:   5839 / 7081 loss=-0.006, score=1.464, ntokens=897.2, nsentences=80, sample_size=897.2, wps=108.7, ups=0.12, wpb=897.2, bsz=80, num_updates=5830, lr=3.90486e-06, gnorm=2.103, clip=100, loss_scale=64, train_wall=82, gb_free=6.8, wall=79203
2022-05-19 15:38:21 - progress_bar.py[line:274] - INFO: epoch 001:   5849 / 7081 loss=-0.006, score=1.36, ntokens=890.5, nsentences=80, sample_size=890.5, wps=108.2, ups=0.12, wpb=890.5, bsz=80, num_updates=5840, lr=3.90246e-06, gnorm=1.544, clip=100, loss_scale=64, train_wall=82, gb_free=6.8, wall=79285
2022-05-19 15:39:43 - progress_bar.py[line:274] - INFO: epoch 001:   5859 / 7081 loss=-0.003, score=1.431, ntokens=882.1, nsentences=80, sample_size=882.1, wps=107.3, ups=0.12, wpb=882.1, bsz=80, num_updates=5850, lr=3.90006e-06, gnorm=1.558, clip=70, loss_scale=64, train_wall=82, gb_free=6.8, wall=79368
2022-05-19 15:41:06 - progress_bar.py[line:274] - INFO: epoch 001:   5869 / 7081 loss=-0.003, score=1.448, ntokens=881.7, nsentences=80, sample_size=881.7, wps=106.4, ups=0.12, wpb=881.7, bsz=80, num_updates=5860, lr=3.89765e-06, gnorm=1.785, clip=80, loss_scale=64, train_wall=83, gb_free=6.8, wall=79450
2022-05-19 15:42:29 - progress_bar.py[line:274] - INFO: epoch 001:   5879 / 7081 loss=-0.003, score=1.408, ntokens=887.3, nsentences=80, sample_size=887.3, wps=106.8, ups=0.12, wpb=887.3, bsz=80, num_updates=5870, lr=3.89525e-06, gnorm=1.639, clip=80, loss_scale=64, train_wall=83, gb_free=6.8, wall=79534
2022-05-19 15:43:51 - progress_bar.py[line:274] - INFO: epoch 001:   5889 / 7081 loss=-0.004, score=1.399, ntokens=897.2, nsentences=80, sample_size=897.2, wps=108.8, ups=0.12, wpb=897.2, bsz=80, num_updates=5880, lr=3.89284e-06, gnorm=1.883, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=79616
2022-05-19 15:45:10 - progress_bar.py[line:274] - INFO: epoch 001:   5899 / 7081 loss=-0.007, score=1.401, ntokens=888.5, nsentences=80, sample_size=888.5, wps=112.4, ups=0.13, wpb=888.5, bsz=80, num_updates=5890, lr=3.89044e-06, gnorm=2.069, clip=80, loss_scale=128, train_wall=79, gb_free=6.8, wall=79695
2022-05-19 15:45:34 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-19 15:46:35 - progress_bar.py[line:274] - INFO: epoch 001:   5910 / 7081 loss=-0.005, score=1.436, ntokens=875.5, nsentences=80, sample_size=875.5, wps=102.8, ups=0.12, wpb=875.5, bsz=80, num_updates=5900, lr=3.88804e-06, gnorm=1.446, clip=80, loss_scale=64, train_wall=85, gb_free=6.8, wall=79780
2022-05-19 15:47:55 - progress_bar.py[line:274] - INFO: epoch 001:   5920 / 7081 loss=-0.008, score=1.416, ntokens=892.7, nsentences=80, sample_size=892.7, wps=111.6, ups=0.12, wpb=892.7, bsz=80, num_updates=5910, lr=3.88563e-06, gnorm=1.618, clip=90, loss_scale=64, train_wall=80, gb_free=6.8, wall=79860
2022-05-19 15:49:19 - progress_bar.py[line:274] - INFO: epoch 001:   5930 / 7081 loss=-0.006, score=1.468, ntokens=890.5, nsentences=80, sample_size=890.5, wps=107, ups=0.12, wpb=890.5, bsz=80, num_updates=5920, lr=3.88323e-06, gnorm=1.993, clip=100, loss_scale=64, train_wall=83, gb_free=6.8, wall=79943
2022-05-19 15:50:41 - progress_bar.py[line:274] - INFO: epoch 001:   5940 / 7081 loss=-0.006, score=1.542, ntokens=897.1, nsentences=80, sample_size=897.1, wps=108.7, ups=0.12, wpb=897.1, bsz=80, num_updates=5930, lr=3.88083e-06, gnorm=1.887, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=80026
2022-05-19 15:52:04 - progress_bar.py[line:274] - INFO: epoch 001:   5950 / 7081 loss=-0.004, score=1.407, ntokens=895.8, nsentences=80, sample_size=895.8, wps=108.8, ups=0.12, wpb=895.8, bsz=80, num_updates=5940, lr=3.87842e-06, gnorm=1.525, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=80108
2022-05-19 15:53:26 - progress_bar.py[line:274] - INFO: epoch 001:   5960 / 7081 loss=-0.007, score=1.439, ntokens=882.4, nsentences=80, sample_size=882.4, wps=107.3, ups=0.12, wpb=882.4, bsz=80, num_updates=5950, lr=3.87602e-06, gnorm=1.51, clip=70, loss_scale=64, train_wall=82, gb_free=6.8, wall=80191
2022-05-19 15:54:49 - progress_bar.py[line:274] - INFO: epoch 001:   5970 / 7081 loss=-0.007, score=1.407, ntokens=893.1, nsentences=80, sample_size=893.1, wps=107.8, ups=0.12, wpb=893.1, bsz=80, num_updates=5960, lr=3.87361e-06, gnorm=1.579, clip=80, loss_scale=64, train_wall=83, gb_free=6.8, wall=80273
2022-05-19 15:56:12 - progress_bar.py[line:274] - INFO: epoch 001:   5980 / 7081 loss=-0.003, score=1.29, ntokens=905.3, nsentences=80, sample_size=905.3, wps=108.8, ups=0.12, wpb=905.3, bsz=80, num_updates=5970, lr=3.87121e-06, gnorm=1.371, clip=90, loss_scale=64, train_wall=83, gb_free=6.8, wall=80357
2022-05-19 15:57:35 - progress_bar.py[line:274] - INFO: epoch 001:   5990 / 7081 loss=-0.003, score=1.382, ntokens=889.9, nsentences=80, sample_size=889.9, wps=107.5, ups=0.12, wpb=889.9, bsz=80, num_updates=5980, lr=3.86881e-06, gnorm=1.384, clip=80, loss_scale=64, train_wall=83, gb_free=6.8, wall=80440
2022-05-19 15:58:57 - progress_bar.py[line:274] - INFO: epoch 001:   6000 / 7081 loss=-0.004, score=1.298, ntokens=896.6, nsentences=80, sample_size=896.6, wps=109, ups=0.12, wpb=896.6, bsz=80, num_updates=5990, lr=3.8664e-06, gnorm=1.111, clip=50, loss_scale=64, train_wall=82, gb_free=6.8, wall=80522
slice_id 1 seek offset 2500
2022-05-19 16:00:20 - progress_bar.py[line:274] - INFO: epoch 001:   6010 / 7081 loss=-0.006, score=1.371, ntokens=895.9, nsentences=80, sample_size=895.9, wps=108.4, ups=0.12, wpb=895.9, bsz=80, num_updates=6000, lr=3.864e-06, gnorm=1.973, clip=90, loss_scale=64, train_wall=83, gb_free=6.8, wall=80604
2022-05-19 16:00:20 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0slice_id 1 seek offset 2500

2022-05-19 16:41:46 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.359 | ntokens 112.907 | nsentences 10 | sample_size 112.907 | cider 1.452 | wps 113.5 | wpb 112.9 | bsz 10 | num_updates 6000 | best_cider 1.456
2022-05-19 16:41:46 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2022-05-19 16:41:46 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_6000.pt
2022-05-19 16:41:55 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_6000.pt
2022-05-19 16:42:46 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 1.452) (writing took 59.703582766931504 seconds)
2022-05-19 16:44:06 - progress_bar.py[line:274] - INFO: epoch 001:   6020 / 7081 loss=-0.004, score=1.352, ntokens=886.1, nsentences=80, sample_size=886.1, wps=3.4, ups=0, wpb=886.1, bsz=80, num_updates=6010, lr=3.8616e-06, gnorm=2.01, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=83231
2022-05-19 16:45:28 - progress_bar.py[line:274] - INFO: epoch 001:   6030 / 7081 loss=-0.004, score=1.374, ntokens=877.4, nsentences=80, sample_size=877.4, wps=107.8, ups=0.12, wpb=877.4, bsz=80, num_updates=6020, lr=3.85919e-06, gnorm=1.521, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=83312
2022-05-19 16:46:49 - progress_bar.py[line:274] - INFO: epoch 001:   6040 / 7081 loss=-0.004, score=1.363, ntokens=887.7, nsentences=80, sample_size=887.7, wps=108.8, ups=0.12, wpb=887.7, bsz=80, num_updates=6030, lr=3.85679e-06, gnorm=1.171, clip=60, loss_scale=64, train_wall=82, gb_free=6.7, wall=83394
2022-05-19 16:48:11 - progress_bar.py[line:274] - INFO: epoch 001:   6050 / 7081 loss=-0.007, score=1.493, ntokens=883.1, nsentences=80, sample_size=883.1, wps=108.3, ups=0.12, wpb=883.1, bsz=80, num_updates=6040, lr=3.85438e-06, gnorm=1.543, clip=100, loss_scale=64, train_wall=81, gb_free=6.8, wall=83476
2022-05-19 16:49:32 - progress_bar.py[line:274] - INFO: epoch 001:   6060 / 7081 loss=-0.008, score=1.503, ntokens=887.2, nsentences=80, sample_size=887.2, wps=108.7, ups=0.12, wpb=887.2, bsz=80, num_updates=6050, lr=3.85198e-06, gnorm=1.557, clip=100, loss_scale=64, train_wall=82, gb_free=6.8, wall=83557
2022-05-19 16:50:54 - progress_bar.py[line:274] - INFO: epoch 001:   6070 / 7081 loss=-0.005, score=1.429, ntokens=884.2, nsentences=80, sample_size=884.2, wps=108, ups=0.12, wpb=884.2, bsz=80, num_updates=6060, lr=3.84958e-06, gnorm=1.438, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=83639
2022-05-19 16:52:16 - progress_bar.py[line:274] - INFO: epoch 001:   6080 / 7081 loss=-0.008, score=1.447, ntokens=891.6, nsentences=80, sample_size=891.6, wps=108.5, ups=0.12, wpb=891.6, bsz=80, num_updates=6070, lr=3.84717e-06, gnorm=2.362, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=83721
2022-05-19 16:53:38 - progress_bar.py[line:274] - INFO: epoch 001:   6090 / 7081 loss=-0.011, score=1.319, ntokens=893.5, nsentences=80, sample_size=893.5, wps=109, ups=0.12, wpb=893.5, bsz=80, num_updates=6080, lr=3.84477e-06, gnorm=1.909, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=83803
2022-05-19 16:54:59 - progress_bar.py[line:274] - INFO: epoch 001:   6100 / 7081 loss=-0.002, score=1.333, ntokens=886.3, nsentences=80, sample_size=886.3, wps=109.8, ups=0.12, wpb=886.3, bsz=80, num_updates=6090, lr=3.84237e-06, gnorm=1.784, clip=70, loss_scale=64, train_wall=81, gb_free=6.8, wall=83884
2022-05-19 16:56:22 - progress_bar.py[line:274] - INFO: epoch 001:   6110 / 7081 loss=-0.007, score=1.474, ntokens=902.7, nsentences=80, sample_size=902.7, wps=109.3, ups=0.12, wpb=902.7, bsz=80, num_updates=6100, lr=3.83996e-06, gnorm=1.308, clip=90, loss_scale=64, train_wall=83, gb_free=6.8, wall=83967
2022-05-19 16:57:44 - progress_bar.py[line:274] - INFO: epoch 001:   6120 / 7081 loss=-0.003, score=1.419, ntokens=896.2, nsentences=80, sample_size=896.2, wps=109.3, ups=0.12, wpb=896.2, bsz=80, num_updates=6110, lr=3.83756e-06, gnorm=1.879, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=84049
2022-05-19 16:59:05 - progress_bar.py[line:274] - INFO: epoch 001:   6130 / 7081 loss=-0.007, score=1.483, ntokens=896.6, nsentences=80, sample_size=896.6, wps=109.8, ups=0.12, wpb=896.6, bsz=80, num_updates=6120, lr=3.83515e-06, gnorm=1.63, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=84130
2022-05-19 17:00:27 - progress_bar.py[line:274] - INFO: epoch 001:   6140 / 7081 loss=-0.004, score=1.391, ntokens=901.8, nsentences=80, sample_size=901.8, wps=110.3, ups=0.12, wpb=901.8, bsz=80, num_updates=6130, lr=3.83275e-06, gnorm=1.417, clip=70, loss_scale=64, train_wall=82, gb_free=6.8, wall=84212
2022-05-19 17:01:49 - progress_bar.py[line:274] - INFO: epoch 001:   6150 / 7081 loss=-0.006, score=1.413, ntokens=880.2, nsentences=80, sample_size=880.2, wps=108, ups=0.12, wpb=880.2, bsz=80, num_updates=6140, lr=3.83035e-06, gnorm=1.569, clip=80, loss_scale=64, train_wall=81, gb_free=6.8, wall=84294
2022-05-19 17:03:11 - progress_bar.py[line:274] - INFO: epoch 001:   6160 / 7081 loss=-0.002, score=1.37, ntokens=893.2, nsentences=80, sample_size=893.2, wps=108.7, ups=0.12, wpb=893.2, bsz=80, num_updates=6150, lr=3.82794e-06, gnorm=1.157, clip=60, loss_scale=64, train_wall=82, gb_free=6.8, wall=84376
2022-05-19 17:04:29 - progress_bar.py[line:274] - INFO: epoch 001:   6170 / 7081 loss=-0.007, score=1.468, ntokens=890.5, nsentences=80, sample_size=890.5, wps=113.7, ups=0.13, wpb=890.5, bsz=80, num_updates=6160, lr=3.82554e-06, gnorm=1.862, clip=100, loss_scale=64, train_wall=78, gb_free=6.8, wall=84454
2022-05-19 17:05:46 - progress_bar.py[line:274] - INFO: epoch 001:   6180 / 7081 loss=-0.006, score=1.367, ntokens=888, nsentences=80, sample_size=888, wps=114.9, ups=0.13, wpb=888, bsz=80, num_updates=6170, lr=3.82314e-06, gnorm=1.259, clip=70, loss_scale=64, train_wall=77, gb_free=6.8, wall=84531
2022-05-19 17:07:06 - progress_bar.py[line:274] - INFO: epoch 001:   6190 / 7081 loss=-0.005, score=1.351, ntokens=893.1, nsentences=80, sample_size=893.1, wps=112.9, ups=0.13, wpb=893.1, bsz=80, num_updates=6180, lr=3.82073e-06, gnorm=1.196, clip=70, loss_scale=64, train_wall=79, gb_free=6.8, wall=84610
2022-05-19 17:08:27 - progress_bar.py[line:274] - INFO: epoch 001:   6200 / 7081 loss=-0.005, score=1.348, ntokens=881.8, nsentences=80, sample_size=881.8, wps=108, ups=0.12, wpb=881.8, bsz=80, num_updates=6190, lr=3.81833e-06, gnorm=1.643, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=84692
2022-05-19 17:09:49 - progress_bar.py[line:274] - INFO: epoch 001:   6210 / 7081 loss=-0.008, score=1.486, ntokens=903.8, nsentences=80, sample_size=903.8, wps=109.9, ups=0.12, wpb=903.8, bsz=80, num_updates=6200, lr=3.81592e-06, gnorm=1.374, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=84774
2022-05-19 17:11:11 - progress_bar.py[line:274] - INFO: epoch 001:   6220 / 7081 loss=-0.01, score=1.406, ntokens=892, nsentences=80, sample_size=892, wps=109.4, ups=0.12, wpb=892, bsz=80, num_updates=6210, lr=3.81352e-06, gnorm=1.516, clip=80, loss_scale=64, train_wall=81, gb_free=6.8, wall=84856
2022-05-19 17:12:32 - progress_bar.py[line:274] - INFO: epoch 001:   6230 / 7081 loss=-0.004, score=1.284, ntokens=887.4, nsentences=80, sample_size=887.4, wps=109.3, ups=0.12, wpb=887.4, bsz=80, num_updates=6220, lr=3.81112e-06, gnorm=1.628, clip=80, loss_scale=64, train_wall=81, gb_free=6.8, wall=84937
2022-05-19 17:13:54 - progress_bar.py[line:274] - INFO: epoch 001:   6240 / 7081 loss=-0.006, score=1.524, ntokens=887.9, nsentences=80, sample_size=887.9, wps=108, ups=0.12, wpb=887.9, bsz=80, num_updates=6230, lr=3.80871e-06, gnorm=1.648, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=85019
2022-05-19 17:15:17 - progress_bar.py[line:274] - INFO: epoch 001:   6250 / 7081 loss=-0.005, score=1.443, ntokens=896.2, nsentences=80, sample_size=896.2, wps=109, ups=0.12, wpb=896.2, bsz=80, num_updates=6240, lr=3.80631e-06, gnorm=1.767, clip=60, loss_scale=64, train_wall=82, gb_free=6.8, wall=85101
2022-05-19 17:16:38 - progress_bar.py[line:274] - INFO: epoch 001:   6260 / 7081 loss=-0.006, score=1.47, ntokens=889.6, nsentences=80, sample_size=889.6, wps=108.7, ups=0.12, wpb=889.6, bsz=80, num_updates=6250, lr=3.80391e-06, gnorm=1.409, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=85183
2022-05-19 17:18:00 - progress_bar.py[line:274] - INFO: epoch 001:   6270 / 7081 loss=-0.002, score=1.384, ntokens=884.7, nsentences=80, sample_size=884.7, wps=108.5, ups=0.12, wpb=884.7, bsz=80, num_updates=6260, lr=3.8015e-06, gnorm=1.478, clip=60, loss_scale=64, train_wall=81, gb_free=6.8, wall=85265
2022-05-19 17:19:22 - progress_bar.py[line:274] - INFO: epoch 001:   6280 / 7081 loss=-0.004, score=1.528, ntokens=876.5, nsentences=80, sample_size=876.5, wps=107.3, ups=0.12, wpb=876.5, bsz=80, num_updates=6270, lr=3.7991e-06, gnorm=1.456, clip=70, loss_scale=64, train_wall=82, gb_free=6.8, wall=85346
2022-05-19 17:20:43 - progress_bar.py[line:274] - INFO: epoch 001:   6290 / 7081 loss=-0.006, score=1.441, ntokens=879.7, nsentences=80, sample_size=879.7, wps=108, ups=0.12, wpb=879.7, bsz=80, num_updates=6280, lr=3.79669e-06, gnorm=1.487, clip=80, loss_scale=64, train_wall=81, gb_free=6.8, wall=85428
2022-05-19 17:22:05 - progress_bar.py[line:274] - INFO: epoch 001:   6300 / 7081 loss=-0.004, score=1.369, ntokens=891.5, nsentences=80, sample_size=891.5, wps=109.4, ups=0.12, wpb=891.5, bsz=80, num_updates=6290, lr=3.79429e-06, gnorm=1.962, clip=100, loss_scale=64, train_wall=81, gb_free=6.8, wall=85509
2022-05-19 17:23:27 - progress_bar.py[line:274] - INFO: epoch 001:   6310 / 7081 loss=-0.004, score=1.319, ntokens=897.4, nsentences=80, sample_size=897.4, wps=108.9, ups=0.12, wpb=897.4, bsz=80, num_updates=6300, lr=3.79189e-06, gnorm=1.524, clip=50, loss_scale=64, train_wall=82, gb_free=6.8, wall=85592
2022-05-19 17:24:48 - progress_bar.py[line:274] - INFO: epoch 001:   6320 / 7081 loss=-0.006, score=1.445, ntokens=883.6, nsentences=80, sample_size=883.6, wps=108.9, ups=0.12, wpb=883.6, bsz=80, num_updates=6310, lr=3.78948e-06, gnorm=1.999, clip=100, loss_scale=64, train_wall=81, gb_free=6.8, wall=85673
2022-05-19 17:26:10 - progress_bar.py[line:274] - INFO: epoch 001:   6330 / 7081 loss=-0.003, score=1.436, ntokens=884.7, nsentences=80, sample_size=884.7, wps=107.8, ups=0.12, wpb=884.7, bsz=80, num_updates=6320, lr=3.78708e-06, gnorm=1.88, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=85755
2022-05-19 17:27:32 - progress_bar.py[line:274] - INFO: epoch 001:   6340 / 7081 loss=-0.005, score=1.549, ntokens=896.9, nsentences=80, sample_size=896.9, wps=109.5, ups=0.12, wpb=896.9, bsz=80, num_updates=6330, lr=3.78468e-06, gnorm=1.818, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=85837
2022-05-19 17:28:54 - progress_bar.py[line:274] - INFO: epoch 001:   6350 / 7081 loss=-0.005, score=1.494, ntokens=890.7, nsentences=80, sample_size=890.7, wps=108.7, ups=0.12, wpb=890.7, bsz=80, num_updates=6340, lr=3.78227e-06, gnorm=1.646, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=85919
2022-05-19 17:30:15 - progress_bar.py[line:274] - INFO: epoch 001:   6360 / 7081 loss=-0.006, score=1.307, ntokens=891.5, nsentences=80, sample_size=891.5, wps=110.4, ups=0.12, wpb=891.5, bsz=80, num_updates=6350, lr=3.77987e-06, gnorm=1.447, clip=80, loss_scale=64, train_wall=81, gb_free=6.8, wall=86000
2022-05-19 17:31:31 - progress_bar.py[line:274] - INFO: epoch 001:   6370 / 7081 loss=-0.007, score=1.384, ntokens=883.2, nsentences=80, sample_size=883.2, wps=115.4, ups=0.13, wpb=883.2, bsz=80, num_updates=6360, lr=3.77747e-06, gnorm=1.903, clip=90, loss_scale=64, train_wall=76, gb_free=6.8, wall=86076
2022-05-19 17:32:48 - progress_bar.py[line:274] - INFO: epoch 001:   6380 / 7081 loss=-0.006, score=1.486, ntokens=886.9, nsentences=80, sample_size=886.9, wps=115.3, ups=0.13, wpb=886.9, bsz=80, num_updates=6370, lr=3.77506e-06, gnorm=1.243, clip=60, loss_scale=64, train_wall=77, gb_free=6.8, wall=86153
2022-05-19 17:34:10 - progress_bar.py[line:274] - INFO: epoch 001:   6390 / 7081 loss=-0.006, score=1.438, ntokens=893.2, nsentences=80, sample_size=893.2, wps=109.7, ups=0.12, wpb=893.2, bsz=80, num_updates=6380, lr=3.77266e-06, gnorm=1.371, clip=60, loss_scale=64, train_wall=81, gb_free=6.8, wall=86235
2022-05-19 17:35:31 - progress_bar.py[line:274] - INFO: epoch 001:   6400 / 7081 loss=-0.006, score=1.303, ntokens=879.6, nsentences=80, sample_size=879.6, wps=107.7, ups=0.12, wpb=879.6, bsz=80, num_updates=6390, lr=3.77025e-06, gnorm=1.579, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=86316
2022-05-19 17:36:53 - progress_bar.py[line:274] - INFO: epoch 001:   6410 / 7081 loss=-0.003, score=1.355, ntokens=892.3, nsentences=80, sample_size=892.3, wps=109.3, ups=0.12, wpb=892.3, bsz=80, num_updates=6400, lr=3.76785e-06, gnorm=1.691, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=86398
2022-05-19 17:38:14 - progress_bar.py[line:274] - INFO: epoch 001:   6420 / 7081 loss=-0.006, score=1.458, ntokens=884, nsentences=80, sample_size=884, wps=108.7, ups=0.12, wpb=884, bsz=80, num_updates=6410, lr=3.76545e-06, gnorm=1.687, clip=90, loss_scale=128, train_wall=81, gb_free=6.8, wall=86479
2022-05-19 17:39:35 - progress_bar.py[line:274] - INFO: epoch 001:   6430 / 7081 loss=-0.004, score=1.44, ntokens=886, nsentences=80, sample_size=886, wps=109.2, ups=0.12, wpb=886, bsz=80, num_updates=6420, lr=3.76304e-06, gnorm=1.609, clip=100, loss_scale=128, train_wall=81, gb_free=6.8, wall=86560
2022-05-19 17:40:57 - progress_bar.py[line:274] - INFO: epoch 001:   6440 / 7081 loss=-0.003, score=1.382, ntokens=882.6, nsentences=80, sample_size=882.6, wps=108.5, ups=0.12, wpb=882.6, bsz=80, num_updates=6430, lr=3.76064e-06, gnorm=1.504, clip=80, loss_scale=128, train_wall=81, gb_free=6.8, wall=86642
2022-05-19 17:42:18 - progress_bar.py[line:274] - INFO: epoch 001:   6450 / 7081 loss=-0.007, score=1.406, ntokens=877.7, nsentences=80, sample_size=877.7, wps=108.2, ups=0.12, wpb=877.7, bsz=80, num_updates=6440, lr=3.75824e-06, gnorm=2.078, clip=90, loss_scale=128, train_wall=81, gb_free=6.8, wall=86723
2022-05-19 17:43:39 - progress_bar.py[line:274] - INFO: epoch 001:   6460 / 7081 loss=-0.008, score=1.454, ntokens=896.6, nsentences=80, sample_size=896.6, wps=110.3, ups=0.12, wpb=896.6, bsz=80, num_updates=6450, lr=3.75583e-06, gnorm=1.391, clip=90, loss_scale=128, train_wall=81, gb_free=6.8, wall=86804
2022-05-19 17:45:01 - progress_bar.py[line:274] - INFO: epoch 001:   6470 / 7081 loss=-0.005, score=1.422, ntokens=891.6, nsentences=80, sample_size=891.6, wps=109.3, ups=0.12, wpb=891.6, bsz=80, num_updates=6460, lr=3.75343e-06, gnorm=1.779, clip=90, loss_scale=128, train_wall=81, gb_free=6.8, wall=86886
2022-05-19 17:46:23 - progress_bar.py[line:274] - INFO: epoch 001:   6480 / 7081 loss=-0.006, score=1.451, ntokens=892.6, nsentences=80, sample_size=892.6, wps=108.4, ups=0.12, wpb=892.6, bsz=80, num_updates=6470, lr=3.75102e-06, gnorm=1.405, clip=80, loss_scale=128, train_wall=82, gb_free=6.8, wall=86968
2022-05-19 17:47:45 - progress_bar.py[line:274] - INFO: epoch 001:   6490 / 7081 loss=-0.007, score=1.537, ntokens=890.6, nsentences=80, sample_size=890.6, wps=109.1, ups=0.12, wpb=890.6, bsz=80, num_updates=6480, lr=3.74862e-06, gnorm=1.91, clip=90, loss_scale=128, train_wall=82, gb_free=6.8, wall=87050
2022-05-19 17:49:07 - progress_bar.py[line:274] - INFO: epoch 001:   6500 / 7081 loss=-0.008, score=1.429, ntokens=913.5, nsentences=80, sample_size=913.5, wps=111.2, ups=0.12, wpb=913.5, bsz=80, num_updates=6490, lr=3.74622e-06, gnorm=1.597, clip=80, loss_scale=128, train_wall=82, gb_free=6.8, wall=87132
2022-05-19 17:50:28 - progress_bar.py[line:274] - INFO: epoch 001:   6510 / 7081 loss=-0.007, score=1.508, ntokens=888.3, nsentences=80, sample_size=888.3, wps=109.2, ups=0.12, wpb=888.3, bsz=80, num_updates=6500, lr=3.74381e-06, gnorm=1.897, clip=80, loss_scale=128, train_wall=81, gb_free=6.8, wall=87213
2022-05-19 17:50:28 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-19 18:31:40 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.362 | ntokens 113.12 | nsentences 10 | sample_size 113.12 | cider 1.45 | wps 114.4 | wpb 113.1 | bsz 10 | num_updates 6500 | best_cider 1.456
2022-05-19 18:31:40 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6500 updates
2022-05-19 18:31:40 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_6500.pt
2022-05-19 18:31:49 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_6500.pt
2022-05-19 18:32:21 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_6500.pt (epoch 1 @ 6500 updates, score 1.45) (writing took 40.593599318061024 seconds)
2022-05-19 18:33:41 - progress_bar.py[line:274] - INFO: epoch 001:   6520 / 7081 loss=-0.005, score=1.439, ntokens=887.3, nsentences=80, sample_size=887.3, wps=3.4, ups=0, wpb=887.3, bsz=80, num_updates=6510, lr=3.74141e-06, gnorm=1.392, clip=80, loss_scale=128, train_wall=80, gb_free=6.8, wall=89806
2022-05-19 18:35:02 - progress_bar.py[line:274] - INFO: epoch 001:   6530 / 7081 loss=-0.003, score=1.564, ntokens=875.9, nsentences=80, sample_size=875.9, wps=108.4, ups=0.12, wpb=875.9, bsz=80, num_updates=6520, lr=3.73901e-06, gnorm=1.391, clip=90, loss_scale=128, train_wall=81, gb_free=6.8, wall=89887
2022-05-19 18:36:23 - progress_bar.py[line:274] - INFO: epoch 001:   6540 / 7081 loss=-0.004, score=1.392, ntokens=879.4, nsentences=80, sample_size=879.4, wps=107.9, ups=0.12, wpb=879.4, bsz=80, num_updates=6530, lr=3.7366e-06, gnorm=1.266, clip=70, loss_scale=128, train_wall=81, gb_free=6.8, wall=89968
2022-05-19 18:37:46 - progress_bar.py[line:274] - INFO: epoch 001:   6550 / 7081 loss=-0.004, score=1.306, ntokens=902.3, nsentences=80, sample_size=902.3, wps=109.2, ups=0.12, wpb=902.3, bsz=80, num_updates=6540, lr=3.7342e-06, gnorm=1.55, clip=70, loss_scale=128, train_wall=83, gb_free=6.8, wall=90051
2022-05-19 18:39:07 - progress_bar.py[line:274] - INFO: epoch 001:   6560 / 7081 loss=-0.004, score=1.489, ntokens=878.2, nsentences=80, sample_size=878.2, wps=108.2, ups=0.12, wpb=878.2, bsz=80, num_updates=6550, lr=3.73179e-06, gnorm=1.554, clip=90, loss_scale=128, train_wall=81, gb_free=6.8, wall=90132
2022-05-19 18:40:29 - progress_bar.py[line:274] - INFO: epoch 001:   6570 / 7081 loss=-0.005, score=1.399, ntokens=900.6, nsentences=80, sample_size=900.6, wps=110.2, ups=0.12, wpb=900.6, bsz=80, num_updates=6560, lr=3.72939e-06, gnorm=1.436, clip=60, loss_scale=128, train_wall=82, gb_free=6.8, wall=90214
2022-05-19 18:41:51 - progress_bar.py[line:274] - INFO: epoch 001:   6580 / 7081 loss=-0.006, score=1.439, ntokens=880.1, nsentences=80, sample_size=880.1, wps=107.6, ups=0.12, wpb=880.1, bsz=80, num_updates=6570, lr=3.72699e-06, gnorm=1.322, clip=80, loss_scale=128, train_wall=82, gb_free=6.8, wall=90295
2022-05-19 18:43:12 - progress_bar.py[line:274] - INFO: epoch 001:   6590 / 7081 loss=-0.003, score=1.5, ntokens=891.4, nsentences=80, sample_size=891.4, wps=109.4, ups=0.12, wpb=891.4, bsz=80, num_updates=6580, lr=3.72458e-06, gnorm=1.334, clip=80, loss_scale=128, train_wall=81, gb_free=6.8, wall=90377
2022-05-19 18:44:33 - progress_bar.py[line:274] - INFO: epoch 001:   6600 / 7081 loss=-0.008, score=1.429, ntokens=887.8, nsentences=80, sample_size=887.8, wps=109.2, ups=0.12, wpb=887.8, bsz=80, num_updates=6590, lr=3.72218e-06, gnorm=1.616, clip=90, loss_scale=128, train_wall=81, gb_free=6.8, wall=90458
2022-05-19 18:45:55 - progress_bar.py[line:274] - INFO: epoch 001:   6610 / 7081 loss=-0.007, score=1.407, ntokens=885.4, nsentences=80, sample_size=885.4, wps=108.8, ups=0.12, wpb=885.4, bsz=80, num_updates=6600, lr=3.71978e-06, gnorm=1.536, clip=100, loss_scale=128, train_wall=81, gb_free=6.8, wall=90540
2022-05-19 18:47:17 - progress_bar.py[line:274] - INFO: epoch 001:   6620 / 7081 loss=-0.006, score=1.426, ntokens=888.6, nsentences=80, sample_size=888.6, wps=108.7, ups=0.12, wpb=888.6, bsz=80, num_updates=6610, lr=3.71737e-06, gnorm=1.435, clip=70, loss_scale=128, train_wall=82, gb_free=6.8, wall=90621
2022-05-19 18:48:38 - progress_bar.py[line:274] - INFO: epoch 001:   6630 / 7081 loss=-0.005, score=1.525, ntokens=894.9, nsentences=80, sample_size=894.9, wps=110, ups=0.12, wpb=894.9, bsz=80, num_updates=6620, lr=3.71497e-06, gnorm=1.424, clip=80, loss_scale=128, train_wall=81, gb_free=6.8, wall=90703
2022-05-19 18:49:56 - progress_bar.py[line:274] - INFO: epoch 001:   6640 / 7081 loss=-0.006, score=1.367, ntokens=890.8, nsentences=80, sample_size=890.8, wps=114, ups=0.13, wpb=890.8, bsz=80, num_updates=6630, lr=3.71256e-06, gnorm=1.631, clip=90, loss_scale=128, train_wall=78, gb_free=6.8, wall=90781
2022-05-19 18:51:13 - progress_bar.py[line:274] - INFO: epoch 001:   6650 / 7081 loss=-0.005, score=1.434, ntokens=885.8, nsentences=80, sample_size=885.8, wps=114.6, ups=0.13, wpb=885.8, bsz=80, num_updates=6640, lr=3.71016e-06, gnorm=1.537, clip=100, loss_scale=128, train_wall=77, gb_free=6.8, wall=90858
2022-05-19 18:52:33 - progress_bar.py[line:274] - INFO: epoch 001:   6660 / 7081 loss=-0.007, score=1.444, ntokens=890.8, nsentences=80, sample_size=890.8, wps=111.7, ups=0.13, wpb=890.8, bsz=80, num_updates=6650, lr=3.70776e-06, gnorm=1.816, clip=80, loss_scale=128, train_wall=80, gb_free=6.8, wall=90938
2022-05-19 18:53:55 - progress_bar.py[line:274] - INFO: epoch 001:   6670 / 7081 loss=-0.008, score=1.499, ntokens=891.6, nsentences=80, sample_size=891.6, wps=109.1, ups=0.12, wpb=891.6, bsz=80, num_updates=6660, lr=3.70535e-06, gnorm=1.268, clip=60, loss_scale=128, train_wall=82, gb_free=6.8, wall=91020
2022-05-19 18:54:03 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-19 18:55:24 - progress_bar.py[line:274] - INFO: epoch 001:   6681 / 7081 loss=-0.004, score=1.435, ntokens=885.4, nsentences=80, sample_size=885.4, wps=99.4, ups=0.11, wpb=885.4, bsz=80, num_updates=6670, lr=3.70295e-06, gnorm=1.392, clip=70, loss_scale=64, train_wall=89, gb_free=6.8, wall=91109
2022-05-19 18:56:46 - progress_bar.py[line:274] - INFO: epoch 001:   6691 / 7081 loss=-0.005, score=1.35, ntokens=892.5, nsentences=80, sample_size=892.5, wps=108.8, ups=0.12, wpb=892.5, bsz=80, num_updates=6680, lr=3.70055e-06, gnorm=1.592, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=91191
2022-05-19 18:58:07 - progress_bar.py[line:274] - INFO: epoch 001:   6701 / 7081 loss=-0.007, score=1.367, ntokens=890.5, nsentences=80, sample_size=890.5, wps=109.3, ups=0.12, wpb=890.5, bsz=80, num_updates=6690, lr=3.69814e-06, gnorm=2.199, clip=100, loss_scale=64, train_wall=81, gb_free=6.8, wall=91272
2022-05-19 18:59:29 - progress_bar.py[line:274] - INFO: epoch 001:   6711 / 7081 loss=-0.004, score=1.325, ntokens=891.9, nsentences=80, sample_size=891.9, wps=108.9, ups=0.12, wpb=891.9, bsz=80, num_updates=6700, lr=3.69574e-06, gnorm=1.357, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=91354
2022-05-19 19:00:51 - progress_bar.py[line:274] - INFO: epoch 001:   6721 / 7081 loss=-0.008, score=1.474, ntokens=881.2, nsentences=80, sample_size=881.2, wps=108, ups=0.12, wpb=881.2, bsz=80, num_updates=6710, lr=3.69333e-06, gnorm=1.571, clip=80, loss_scale=64, train_wall=81, gb_free=6.8, wall=91436
2022-05-19 19:02:12 - progress_bar.py[line:274] - INFO: epoch 001:   6731 / 7081 loss=-0.007, score=1.442, ntokens=892.7, nsentences=80, sample_size=892.7, wps=109.6, ups=0.12, wpb=892.7, bsz=80, num_updates=6720, lr=3.69093e-06, gnorm=1.358, clip=80, loss_scale=64, train_wall=81, gb_free=6.8, wall=91517
2022-05-19 19:03:35 - progress_bar.py[line:274] - INFO: epoch 001:   6741 / 7081 loss=-0.006, score=1.472, ntokens=893.2, nsentences=80, sample_size=893.2, wps=108.4, ups=0.12, wpb=893.2, bsz=80, num_updates=6730, lr=3.68853e-06, gnorm=1.612, clip=100, loss_scale=64, train_wall=82, gb_free=6.8, wall=91599
2022-05-19 19:04:57 - progress_bar.py[line:274] - INFO: epoch 001:   6751 / 7081 loss=-0.004, score=1.436, ntokens=888.8, nsentences=80, sample_size=888.8, wps=108.3, ups=0.12, wpb=888.8, bsz=80, num_updates=6740, lr=3.68612e-06, gnorm=1.383, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=91681
2022-05-19 19:06:18 - progress_bar.py[line:274] - INFO: epoch 001:   6761 / 7081 loss=-0.005, score=1.54, ntokens=890, nsentences=80, sample_size=890, wps=109.5, ups=0.12, wpb=890, bsz=80, num_updates=6750, lr=3.68372e-06, gnorm=1.8, clip=100, loss_scale=64, train_wall=81, gb_free=6.8, wall=91763
2022-05-19 19:07:40 - progress_bar.py[line:274] - INFO: epoch 001:   6771 / 7081 loss=-0.005, score=1.41, ntokens=887.9, nsentences=80, sample_size=887.9, wps=108.2, ups=0.12, wpb=887.9, bsz=80, num_updates=6760, lr=3.68132e-06, gnorm=1.371, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=91845
2022-05-19 19:09:02 - progress_bar.py[line:274] - INFO: epoch 001:   6781 / 7081 loss=-0.004, score=1.431, ntokens=891.8, nsentences=80, sample_size=891.8, wps=109.4, ups=0.12, wpb=891.8, bsz=80, num_updates=6770, lr=3.67891e-06, gnorm=1.573, clip=100, loss_scale=64, train_wall=81, gb_free=6.8, wall=91926
2022-05-19 19:10:23 - progress_bar.py[line:274] - INFO: epoch 001:   6791 / 7081 loss=-0.005, score=1.395, ntokens=887.1, nsentences=80, sample_size=887.1, wps=109.3, ups=0.12, wpb=887.1, bsz=80, num_updates=6780, lr=3.67651e-06, gnorm=1.265, clip=60, loss_scale=64, train_wall=81, gb_free=6.8, wall=92008
2022-05-19 19:11:44 - progress_bar.py[line:274] - INFO: epoch 001:   6801 / 7081 loss=-0.007, score=1.41, ntokens=880.6, nsentences=80, sample_size=880.6, wps=107.9, ups=0.12, wpb=880.6, bsz=80, num_updates=6790, lr=3.6741e-06, gnorm=1.564, clip=100, loss_scale=64, train_wall=82, gb_free=6.8, wall=92089
2022-05-19 19:13:06 - progress_bar.py[line:274] - INFO: epoch 001:   6811 / 7081 loss=-0.004, score=1.423, ntokens=896.2, nsentences=80, sample_size=896.2, wps=110.2, ups=0.12, wpb=896.2, bsz=80, num_updates=6800, lr=3.6717e-06, gnorm=1.215, clip=50, loss_scale=64, train_wall=81, gb_free=6.8, wall=92170
2022-05-19 19:14:27 - progress_bar.py[line:274] - INFO: epoch 001:   6821 / 7081 loss=-0.004, score=1.457, ntokens=883.2, nsentences=80, sample_size=883.2, wps=108.9, ups=0.12, wpb=883.2, bsz=80, num_updates=6810, lr=3.6693e-06, gnorm=1.297, clip=70, loss_scale=64, train_wall=81, gb_free=6.8, wall=92252
2022-05-19 19:15:46 - progress_bar.py[line:274] - INFO: epoch 001:   6831 / 7081 loss=-0.003, score=1.331, ntokens=874.2, nsentences=80, sample_size=874.2, wps=110.7, ups=0.13, wpb=874.2, bsz=80, num_updates=6820, lr=3.66689e-06, gnorm=1.513, clip=70, loss_scale=64, train_wall=79, gb_free=6.8, wall=92330
2022-05-19 19:17:03 - progress_bar.py[line:274] - INFO: epoch 001:   6841 / 7081 loss=-0.008, score=1.334, ntokens=885.6, nsentences=80, sample_size=885.6, wps=115.2, ups=0.13, wpb=885.6, bsz=80, num_updates=6830, lr=3.66449e-06, gnorm=1.729, clip=100, loss_scale=64, train_wall=77, gb_free=6.8, wall=92407
2022-05-19 19:18:22 - progress_bar.py[line:274] - INFO: epoch 001:   6851 / 7081 loss=-0.006, score=1.43, ntokens=903.5, nsentences=80, sample_size=903.5, wps=114.4, ups=0.13, wpb=903.5, bsz=80, num_updates=6840, lr=3.66209e-06, gnorm=1.468, clip=100, loss_scale=64, train_wall=79, gb_free=6.8, wall=92486
2022-05-19 19:19:43 - progress_bar.py[line:274] - INFO: epoch 001:   6861 / 7081 loss=-0.006, score=1.502, ntokens=879.1, nsentences=80, sample_size=879.1, wps=108.5, ups=0.12, wpb=879.1, bsz=80, num_updates=6850, lr=3.65968e-06, gnorm=1.74, clip=90, loss_scale=64, train_wall=81, gb_free=6.7, wall=92567
2022-05-19 19:21:05 - progress_bar.py[line:274] - INFO: epoch 001:   6871 / 7081 loss=-0.007, score=1.514, ntokens=888, nsentences=80, sample_size=888, wps=108.2, ups=0.12, wpb=888, bsz=80, num_updates=6860, lr=3.65728e-06, gnorm=1.665, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=92649
2022-05-19 19:22:27 - progress_bar.py[line:274] - INFO: epoch 001:   6881 / 7081 loss=-0.01, score=1.471, ntokens=898.3, nsentences=80, sample_size=898.3, wps=108.8, ups=0.12, wpb=898.3, bsz=80, num_updates=6870, lr=3.65488e-06, gnorm=1.751, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=92732
2022-05-19 19:23:49 - progress_bar.py[line:274] - INFO: epoch 001:   6891 / 7081 loss=-0.007, score=1.397, ntokens=888.2, nsentences=80, sample_size=888.2, wps=109.1, ups=0.12, wpb=888.2, bsz=80, num_updates=6880, lr=3.65247e-06, gnorm=1.751, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=92813
2022-05-19 19:25:10 - progress_bar.py[line:274] - INFO: epoch 001:   6901 / 7081 loss=-0.003, score=1.367, ntokens=889.3, nsentences=80, sample_size=889.3, wps=109, ups=0.12, wpb=889.3, bsz=80, num_updates=6890, lr=3.65007e-06, gnorm=1.677, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=92895
2022-05-19 19:26:31 - progress_bar.py[line:274] - INFO: epoch 001:   6911 / 7081 loss=-0.005, score=1.507, ntokens=886.4, nsentences=80, sample_size=886.4, wps=109.4, ups=0.12, wpb=886.4, bsz=80, num_updates=6900, lr=3.64766e-06, gnorm=2.012, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=92976
2022-05-19 19:27:53 - progress_bar.py[line:274] - INFO: epoch 001:   6921 / 7081 loss=-0.004, score=1.451, ntokens=896.6, nsentences=80, sample_size=896.6, wps=109, ups=0.12, wpb=896.6, bsz=80, num_updates=6910, lr=3.64526e-06, gnorm=1.423, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=93058
2022-05-19 19:29:15 - progress_bar.py[line:274] - INFO: epoch 001:   6931 / 7081 loss=-0.007, score=1.428, ntokens=899.4, nsentences=80, sample_size=899.4, wps=109.7, ups=0.12, wpb=899.4, bsz=80, num_updates=6920, lr=3.64286e-06, gnorm=1.514, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=93140
2022-05-19 19:30:37 - progress_bar.py[line:274] - INFO: epoch 001:   6941 / 7081 loss=-0.004, score=1.39, ntokens=880.5, nsentences=80, sample_size=880.5, wps=108.4, ups=0.12, wpb=880.5, bsz=80, num_updates=6930, lr=3.64045e-06, gnorm=1.481, clip=80, loss_scale=64, train_wall=81, gb_free=6.8, wall=93221
2022-05-19 19:31:57 - progress_bar.py[line:274] - INFO: epoch 001:   6951 / 7081 loss=-0.005, score=1.461, ntokens=883, nsentences=80, sample_size=883, wps=109.4, ups=0.12, wpb=883, bsz=80, num_updates=6940, lr=3.63805e-06, gnorm=1.197, clip=50, loss_scale=64, train_wall=81, gb_free=6.8, wall=93302
2022-05-19 19:33:18 - progress_bar.py[line:274] - INFO: epoch 001:   6961 / 7081 loss=-0.004, score=1.374, ntokens=871.4, nsentences=80, sample_size=871.4, wps=108.4, ups=0.12, wpb=871.4, bsz=80, num_updates=6950, lr=3.63565e-06, gnorm=1.689, clip=80, loss_scale=64, train_wall=80, gb_free=6.8, wall=93383
2022-05-19 19:34:39 - progress_bar.py[line:274] - INFO: epoch 001:   6971 / 7081 loss=-0.003, score=1.469, ntokens=877.7, nsentences=80, sample_size=877.7, wps=108.1, ups=0.12, wpb=877.7, bsz=80, num_updates=6960, lr=3.63324e-06, gnorm=1.599, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=93464
2022-05-19 19:36:01 - progress_bar.py[line:274] - INFO: epoch 001:   6981 / 7081 loss=-0.007, score=1.394, ntokens=887, nsentences=80, sample_size=887, wps=108.1, ups=0.12, wpb=887, bsz=80, num_updates=6970, lr=3.63084e-06, gnorm=1.376, clip=100, loss_scale=64, train_wall=82, gb_free=6.8, wall=93546
2022-05-19 19:37:23 - progress_bar.py[line:274] - INFO: epoch 001:   6991 / 7081 loss=-0.005, score=1.442, ntokens=884.1, nsentences=80, sample_size=884.1, wps=107.9, ups=0.12, wpb=884.1, bsz=80, num_updates=6980, lr=3.62843e-06, gnorm=1.363, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=93628
2022-05-19 19:38:45 - progress_bar.py[line:274] - INFO: epoch 001:   7001 / 7081 loss=-0.008, score=1.452, ntokens=890.2, nsentences=80, sample_size=890.2, wps=108.7, ups=0.12, wpb=890.2, bsz=80, num_updates=6990, lr=3.62603e-06, gnorm=1.672, clip=100, loss_scale=64, train_wall=82, gb_free=6.8, wall=93710
2022-05-19 19:40:07 - progress_bar.py[line:274] - INFO: epoch 001:   7011 / 7081 loss=-0.004, score=1.347, ntokens=899.5, nsentences=80, sample_size=899.5, wps=109.9, ups=0.12, wpb=899.5, bsz=80, num_updates=7000, lr=3.62363e-06, gnorm=1.349, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=93791
2022-05-19 19:40:07 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-05-19 20:21:06 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.004 | score 1.363 | ntokens 112.837 | nsentences 10 | sample_size 112.837 | cider 1.45 | wps 114.7 | wpb 112.8 | bsz 10 | num_updates 7000 | best_cider 1.456
2022-05-19 20:21:06 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 7000 updates
2022-05-19 20:21:06 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_7000.pt
2022-05-19 20:21:15 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_7000.pt
2022-05-19 20:21:48 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_1_7000.pt (epoch 1 @ 7000 updates, score 1.45) (writing took 42.0903338000644 seconds)
2022-05-19 20:23:09 - progress_bar.py[line:274] - INFO: epoch 001:   7021 / 7081 loss=-0.005, score=1.477, ntokens=893.6, nsentences=80, sample_size=893.6, wps=3.5, ups=0, wpb=893.6, bsz=80, num_updates=7010, lr=3.62122e-06, gnorm=1.563, clip=100, loss_scale=64, train_wall=81, gb_free=6.8, wall=96374
2022-05-19 20:24:30 - progress_bar.py[line:274] - INFO: epoch 001:   7031 / 7081 loss=-0.008, score=1.4, ntokens=888.1, nsentences=80, sample_size=888.1, wps=109.4, ups=0.12, wpb=888.1, bsz=80, num_updates=7020, lr=3.61882e-06, gnorm=1.37, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=96455
2022-05-19 20:25:52 - progress_bar.py[line:274] - INFO: epoch 001:   7041 / 7081 loss=-0.005, score=1.346, ntokens=886.6, nsentences=80, sample_size=886.6, wps=108.9, ups=0.12, wpb=886.6, bsz=80, num_updates=7030, lr=3.61642e-06, gnorm=1.404, clip=80, loss_scale=64, train_wall=81, gb_free=6.8, wall=96536
2022-05-19 20:27:13 - progress_bar.py[line:274] - INFO: epoch 001:   7051 / 7081 loss=-0.007, score=1.409, ntokens=893.7, nsentences=80, sample_size=893.7, wps=109.2, ups=0.12, wpb=893.7, bsz=80, num_updates=7040, lr=3.61401e-06, gnorm=1.792, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=96618
2022-05-19 20:28:35 - progress_bar.py[line:274] - INFO: epoch 001:   7061 / 7081 loss=-0.004, score=1.411, ntokens=895.4, nsentences=80, sample_size=895.4, wps=109.8, ups=0.12, wpb=895.4, bsz=80, num_updates=7050, lr=3.61161e-06, gnorm=1.354, clip=100, loss_scale=64, train_wall=81, gb_free=6.8, wall=96700
2022-05-19 20:29:56 - progress_bar.py[line:274] - INFO: epoch 001:   7071 / 7081 loss=-0.006, score=1.555, ntokens=879.1, nsentences=80, sample_size=879.1, wps=108.4, ups=0.12, wpb=879.1, bsz=80, num_updates=7060, lr=3.6092e-06, gnorm=2.093, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=96781
slice_id 1 seek offset 2500
2022-05-19 20:31:13 - progress_bar.py[line:274] - INFO: epoch 001:   7081 / 7081 loss=-0.004, score=1.505, ntokens=842, nsentences=76, sample_size=842, wps=108.9, ups=0.13, wpb=842, bsz=76, num_updates=7070, lr=3.6068e-06, gnorm=1.658, clip=90, loss_scale=64, train_wall=77, gb_free=6.8, wall=96858
2022-05-19 20:31:13 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-05-19 21:12:14 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0.005 | score 1.363 | ntokens 112.776 | nsentences 10 | sample_size 112.776 | cider 1.452 | wps 114.6 | wpb 112.8 | bsz 10 | num_updates 7070 | best_cider 1.456
2022-05-19 21:12:14 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 7070 updates
local datafile ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2022-05-19 21:12:14 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_last.pt
local datafile ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 1 row count 56643 total row count 113287
slice_id 1 seek offset 56644
2022-05-19 21:12:48 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_last.pt
2022-05-19 21:12:48 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints//5e-6_3/checkpoint_last.pt (epoch 1 @ 7070 updates, score 1.452) (writing took 34.013625842984766 seconds)
2022-05-19 21:12:48 - train.py[line:323] - INFO: end of epoch 1 (average epoch stats below)
2022-05-19 21:12:48 - progress_bar.py[line:282] - INFO: epoch 001 | loss -0.005 | score 1.39 | ntokens 883.755 | nsentences 79.994 | sample_size 883.755 | wps 62.9 | ups 0.07 | wpb 883.8 | bsz 80 | num_updates 7070 | lr 3.6068e-06 | gnorm 1.505 | clip 78.9 | loss_scale 64 | train_wall 60193 | gb_free 6.8 | wall 99353
2022-05-19 21:12:48 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage1_train_ct2.tsv slice_id 0 row count 56644 total row count 113287
slice_id 0 seek offset 0
2022-05-19 21:12:51 - trainer.py[line:703] - INFO: begin training epoch 2
2022-05-19 21:12:51 - train.py[line:296] - INFO: Start iterating over samples
2022-05-19 21:14:13 - progress_bar.py[line:274] - INFO: epoch 002:     10 / 7081 loss=-0.003, score=1.313, ntokens=899.1, nsentences=80, sample_size=899.1, wps=3.5, ups=0, wpb=899.1, bsz=80, num_updates=7080, lr=3.6044e-06, gnorm=1.773, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=99437
2022-05-19 21:15:34 - progress_bar.py[line:274] - INFO: epoch 002:     20 / 7081 loss=-0.005, score=1.415, ntokens=882.8, nsentences=80, sample_size=882.8, wps=108.3, ups=0.12, wpb=882.8, bsz=80, num_updates=7090, lr=3.60199e-06, gnorm=1.419, clip=70, loss_scale=64, train_wall=81, gb_free=6.8, wall=99519
2022-05-19 21:16:56 - progress_bar.py[line:274] - INFO: epoch 002:     30 / 7081 loss=-0.008, score=1.418, ntokens=873.6, nsentences=80, sample_size=873.6, wps=107.3, ups=0.12, wpb=873.6, bsz=80, num_updates=7100, lr=3.59959e-06, gnorm=1.747, clip=70, loss_scale=64, train_wall=81, gb_free=6.8, wall=99600
2022-05-19 21:18:17 - progress_bar.py[line:274] - INFO: epoch 002:     40 / 7081 loss=-0.004, score=1.32, ntokens=880.9, nsentences=80, sample_size=880.9, wps=107.7, ups=0.12, wpb=880.9, bsz=80, num_updates=7110, lr=3.59719e-06, gnorm=1.558, clip=80, loss_scale=64, train_wall=82, gb_free=6.8, wall=99682
2022-05-19 21:19:39 - progress_bar.py[line:274] - INFO: epoch 002:     50 / 7081 loss=-0.007, score=1.392, ntokens=882.6, nsentences=80, sample_size=882.6, wps=107.7, ups=0.12, wpb=882.6, bsz=80, num_updates=7120, lr=3.59478e-06, gnorm=1.907, clip=90, loss_scale=64, train_wall=82, gb_free=6.8, wall=99764
2022-05-19 21:21:00 - progress_bar.py[line:274] - INFO: epoch 002:     60 / 7081 loss=-0.004, score=1.413, ntokens=877.7, nsentences=80, sample_size=877.7, wps=108.3, ups=0.12, wpb=877.7, bsz=80, num_updates=7130, lr=3.59238e-06, gnorm=1.711, clip=90, loss_scale=64, train_wall=81, gb_free=6.8, wall=99845
Traceback (most recent call last):
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/wgus5950/miniconda3/envs/ofa2/bin/python3', '-u', '../../train.py', '--local_rank=1', '../../dataset/caption_data/caption_stage1_train_ct2.tsv,../../dataset/caption_data/caption_val.tsv', '--selected-cols=1,4,2', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=../../checkpoints/checkpoint_stage1_best.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=/database/jhkim/stage2_checkpoints//5e-6_3', '--task=caption', '--arch=ofa_large', '--criterion=scst_reward_criterion', '--batch-size=1', '--update-freq=8', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.0', '--decoder-drop-path-rate=0.0', '--dropout=0.0', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-6', '--end-learning-rate=2e-7', '--max-epoch=3', '--warmup-ratio=0.06', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--no-epoch-checkpoints', '--keep-best-checkpoints=1', '--save-interval=1', '--validate-interval=1', '--save-interval-updates=500', '--validate-interval-updates=500', '--eval-cider', '--eval-cider-cached-tokens=../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', '--eval-args={"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', '--best-checkpoint-metric=cider', '--maximize-best-checkpoint-metric', '--max-src-length=80', '--max-tgt-length=20', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--freeze-resnet', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--scst', '--scst-cider-cached-tokens=../../dataset/caption_data/cider_cached_tokens/coco-train-words.p', '--scst-args={"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', '--memory-efficient-fp16', '--fp16-scale-window=512', '--num-workers=0']' died with <Signals.SIGTERM: 15>.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 2766953
Killing subprocess 2766954
