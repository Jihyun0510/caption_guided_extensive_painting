2022-06-28 03:01:26 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2022-06-28 03:01:26 - utils.py[line:261] - INFO: Start init
2022-06-28 03:01:27 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2022-06-28 03:01:27 - utils.py[line:261] - INFO: Start init
2022-06-28 03:01:27 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2022-06-28 03:01:27 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-06-28 03:01:27 - utils.py[line:274] - INFO: initialized host vdsl as rank 0
single-machine distributed training is initialized.
2022-06-28 03:01:27 - utils.py[line:274] - INFO: initialized host vdsl as rank 1
single-machine distributed training is initialized.
2022-06-28 03:01:33 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 500, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/database/jhkim/stage2_checkpoints_sibal//1e-5_5', 'restore_file': '/database/jhkim/caption_large_best_clean.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'cider', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_large', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='ofa_large', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=1, batch_size_valid=1, best_checkpoint_metric='cider', bf16=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='scst_reward_criterion', cross_self_attention=False, curriculum=0, data='../../dataset/caption_data/caption_stage2_train_half.tsv,../../dataset/caption_data/caption_val_half.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=16, decoder_drop_path_rate=0.0, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, dropout=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_drop_path_rate=0.0, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=2e-07, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', eval_bleu=False, eval_cider=True, eval_cider_cached_tokens='../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, freeze_resnet=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_source_positions=1024, max_src_length=80, max_target_positions=1024, max_tgt_length=20, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet152', restore_file='/database/jhkim/caption_large_best_clean.pt', save_dir='/database/jhkim/stage2_checkpoints_sibal//1e-5_5', save_interval=1, save_interval_updates=500, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', scst=True, scst_args='{"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', scst_cider_cached_tokens='../../dataset/caption_data/cider_cached_tokens/coco-train-words.p', seed=1, selected_cols='1,4,2', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, sync_bn=False, task='caption', tensorboard_logdir=None, threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[8], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='../../ofa_module', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=500, wandb_project=None, warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'caption', 'data': '../../dataset/caption_data/caption_stage2_train_half.tsv,../../dataset/caption_data/caption_val_half.tsv', 'selected_cols': '1,4,2', 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 80, 'max_tgt_length': 20, 'code_dict_size': 8192, 'patch_image_size': 480, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_bleu': False, 'eval_cider': True, 'eval_args': '{"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', 'eval_print_samples': False, 'eval_cider_cached_tokens': '../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', 'scst': True, 'scst_args': '{"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}'}, 'criterion': {'_name': 'scst_reward_criterion', 'scst_cider_cached_tokens': '../../dataset/caption_data/cider_cached_tokens/coco-train-words.p', 'ignore_prefix_size': 0, 'sentence_avg': False, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 2e-07, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-06-28 03:01:33 - ofa_task.py[line:102] - INFO: source dictionary: 59457 types
2022-06-28 03:01:33 - ofa_task.py[line:103] - INFO: target dictionary: 59457 types
2022-06-28 03:01:46 - train.py[line:101] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 1024)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (23): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (24): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (25): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (26): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (27): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (28): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (29): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (30): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (31): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (32): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (33): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (34): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (35): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (patch_layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 1024)
    (embed_image_positions): Embedding(1765, 1024)
    (pos_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (pos_k_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 16)
      (1): Embedding(511, 16)
      (2): Embedding(511, 16)
      (3): Embedding(511, 16)
      (4): Embedding(511, 16)
      (5): Embedding(511, 16)
      (6): Embedding(511, 16)
      (7): Embedding(511, 16)
      (8): Embedding(511, 16)
      (9): Embedding(511, 16)
      (10): Embedding(511, 16)
      (11): Embedding(511, 16)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 16)
      (1): Embedding(6892, 16)
      (2): Embedding(6892, 16)
      (3): Embedding(6892, 16)
      (4): Embedding(6892, 16)
      (5): Embedding(6892, 16)
      (6): Embedding(6892, 16)
      (7): Embedding(6892, 16)
      (8): Embedding(6892, 16)
      (9): Embedding(6892, 16)
      (10): Embedding(6892, 16)
      (11): Embedding(6892, 16)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 1024)
    (embed_image_positions): Embedding(1765, 1024)
    (pos_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (self_pos_k_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (cross_pos_q_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (cross_pos_k_linear): Linear(in_features=1024, out_features=1024, bias=True)
    (code_layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (6): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (7): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (8): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (9): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (10): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (11): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 16)
      (1): Embedding(511, 16)
      (2): Embedding(511, 16)
      (3): Embedding(511, 16)
      (4): Embedding(511, 16)
      (5): Embedding(511, 16)
      (6): Embedding(511, 16)
      (7): Embedding(511, 16)
      (8): Embedding(511, 16)
      (9): Embedding(511, 16)
      (10): Embedding(511, 16)
      (11): Embedding(511, 16)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 16)
      (1): Embedding(6892, 16)
      (2): Embedding(6892, 16)
      (3): Embedding(6892, 16)
      (4): Embedding(6892, 16)
      (5): Embedding(6892, 16)
      (6): Embedding(6892, 16)
      (7): Embedding(6892, 16)
      (8): Embedding(6892, 16)
      (9): Embedding(6892, 16)
      (10): Embedding(6892, 16)
      (11): Embedding(6892, 16)
    )
  )
  (classification_heads): ModuleDict()
)
2022-06-28 03:01:46 - train.py[line:102] - INFO: task: CaptionTask
2022-06-28 03:01:46 - train.py[line:103] - INFO: model: OFAModel
2022-06-28 03:01:46 - train.py[line:104] - INFO: criterion: ScstRewardCriterion
2022-06-28 03:01:46 - train.py[line:108] - INFO: num. shared model params: 472,977,152 (num. trained: 411,964,288)
2022-06-28 03:01:46 - train.py[line:115] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../dataset/caption_data/caption_val_half.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_val_half.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_val_half.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_val_half.tsv slice_id 0 row count 2500 total row count 5000
/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
local datafile ../../dataset/caption_data/caption_val_half.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_val_half.tsv slice_id 1 row count 2500 total row count 5000
/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-06-28 03:01:46 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.4.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.4.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.4.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.5.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.5.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.5.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.6.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.6.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.6.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.7.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.7.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.7.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.23.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.23.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.23.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.24.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.24.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.24.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.25.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.25.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.25.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.26.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.26.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.26.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.27.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.27.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.27.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.28.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.28.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.28.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.29.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.29.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.29.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.30.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.30.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.30.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.31.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.31.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.31.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.32.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.32.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.32.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.33.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.33.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.33.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.34.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.34.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.34.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.35.conv1.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.35.conv2.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.35.conv3.bias
2022-06-28 03:01:46 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-06-28 03:01:46 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-06-28 03:01:46 - utils.py[line:765] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-06-28 03:01:46 - utils.py[line:765] - INFO: rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-06-28 03:01:46 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-06-28 03:01:46 - train.py[line:145] - INFO: training on 2 devices (GPUs/TPUs)
2022-06-28 03:01:46 - train.py[line:151] - INFO: max tokens per device = None and max sentences per device = 1
2022-06-28 03:01:46 - trainer.py[line:458] - INFO: Preparing to load checkpoint /database/jhkim/caption_large_best_clean.pt
2022-06-28 03:01:57 - trainer.py[line:619] - INFO: Loaded checkpoint /database/jhkim/caption_large_best_clean.pt (epoch 2 @ 0 updates)
local datafile ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2022-06-28 03:01:57 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 1 row count 56643 total row count 113287
slice_id 1 seek offset 56644
Total steps 35405, warmup steps 2124, warmup_factor 0.00047080979284369113
local datafile ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 0 row count 56644 total row count 113287
slice_id 0 seek offset 0
Total steps 35405, warmup steps 2124, warmup_factor 0.00047080979284369113
2022-06-28 03:02:01 - trainer.py[line:703] - INFO: begin training epoch 1
2022-06-28 03:02:01 - train.py[line:296] - INFO: Start iterating over samples
/home/wgus5950/OFA/fairseq/fairseq/utils.py:373: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
/home/wgus5950/OFA/fairseq/fairseq/utils.py:373: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-06-28 03:03:49 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 7081 loss=-0.001, score=1.138, ntokens=1329.1, nsentences=80, sample_size=1329.1, wps=131, ups=0.1, wpb=1329.1, bsz=80, num_updates=10, lr=4.7081e-08, gnorm=0.234, clip=0, loss_scale=128, train_wall=107, gb_free=6.7, wall=122
2022-06-28 03:05:32 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 7081 loss=-0.002, score=1.162, ntokens=1333.6, nsentences=80, sample_size=1333.6, wps=128.7, ups=0.1, wpb=1333.6, bsz=80, num_updates=20, lr=9.4162e-08, gnorm=0.477, clip=10, loss_scale=128, train_wall=103, gb_free=6.7, wall=226
2022-06-28 03:07:17 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 7081 loss=0, score=1.149, ntokens=1332.1, nsentences=80, sample_size=1332.1, wps=127.5, ups=0.1, wpb=1332.1, bsz=80, num_updates=30, lr=1.41243e-07, gnorm=0.484, clip=10, loss_scale=128, train_wall=104, gb_free=6.7, wall=330
2022-06-28 03:09:01 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 7081 loss=-0, score=1.131, ntokens=1335.6, nsentences=80, sample_size=1335.6, wps=128.3, ups=0.1, wpb=1335.6, bsz=80, num_updates=40, lr=1.88324e-07, gnorm=0.432, clip=10, loss_scale=128, train_wall=104, gb_free=6.7, wall=434
2022-06-28 03:10:45 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 7081 loss=0.001, score=1.152, ntokens=1340.6, nsentences=80, sample_size=1340.6, wps=128.8, ups=0.1, wpb=1340.6, bsz=80, num_updates=50, lr=2.35405e-07, gnorm=0.411, clip=10, loss_scale=128, train_wall=104, gb_free=6.7, wall=538
2022-06-28 03:12:29 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 7081 loss=0, score=1.199, ntokens=1341.7, nsentences=80, sample_size=1341.7, wps=129.3, ups=0.1, wpb=1341.7, bsz=80, num_updates=60, lr=2.82486e-07, gnorm=0.641, clip=30, loss_scale=128, train_wall=104, gb_free=6.7, wall=642
2022-06-28 03:14:12 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 7081 loss=0.001, score=1.157, ntokens=1329.9, nsentences=80, sample_size=1329.9, wps=128.2, ups=0.1, wpb=1329.9, bsz=80, num_updates=70, lr=3.29567e-07, gnorm=0.427, clip=10, loss_scale=128, train_wall=104, gb_free=6.7, wall=746
2022-06-28 03:15:56 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 7081 loss=-0.001, score=1.202, ntokens=1345.6, nsentences=80, sample_size=1345.6, wps=129.2, ups=0.1, wpb=1345.6, bsz=80, num_updates=80, lr=3.76648e-07, gnorm=0.426, clip=10, loss_scale=128, train_wall=104, gb_free=6.7, wall=850
2022-06-28 03:17:42 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 7081 loss=0, score=1.162, ntokens=1330, nsentences=80, sample_size=1330, wps=126.3, ups=0.09, wpb=1330, bsz=80, num_updates=90, lr=4.23729e-07, gnorm=0.713, clip=20, loss_scale=128, train_wall=105, gb_free=6.7, wall=955
2022-06-28 03:19:26 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 7081 loss=-0.001, score=1.108, ntokens=1334.4, nsentences=80, sample_size=1334.4, wps=127.9, ups=0.1, wpb=1334.4, bsz=80, num_updates=100, lr=4.7081e-07, gnorm=0.327, clip=0, loss_scale=128, train_wall=104, gb_free=6.7, wall=1060
2022-06-28 03:21:11 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 7081 loss=0, score=1.222, ntokens=1335.4, nsentences=80, sample_size=1335.4, wps=127.9, ups=0.1, wpb=1335.4, bsz=80, num_updates=110, lr=5.17891e-07, gnorm=0.241, clip=0, loss_scale=128, train_wall=104, gb_free=6.7, wall=1164
2022-06-28 03:22:54 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 7081 loss=0.002, score=1.161, ntokens=1333.2, nsentences=80, sample_size=1333.2, wps=128.6, ups=0.1, wpb=1333.2, bsz=80, num_updates=120, lr=5.64972e-07, gnorm=0.721, clip=20, loss_scale=128, train_wall=104, gb_free=6.7, wall=1268
2022-06-28 03:24:38 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 7081 loss=-0.002, score=1.194, ntokens=1325.9, nsentences=80, sample_size=1325.9, wps=127.7, ups=0.1, wpb=1325.9, bsz=80, num_updates=130, lr=6.12053e-07, gnorm=0.55, clip=10, loss_scale=128, train_wall=104, gb_free=6.7, wall=1372
2022-06-28 03:26:23 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 7081 loss=-0.001, score=1.124, ntokens=1338.2, nsentences=80, sample_size=1338.2, wps=127.3, ups=0.1, wpb=1338.2, bsz=80, num_updates=140, lr=6.59134e-07, gnorm=0.303, clip=0, loss_scale=128, train_wall=105, gb_free=6.7, wall=1477
2022-06-28 03:28:08 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 7081 loss=-0, score=1.285, ntokens=1337.9, nsentences=80, sample_size=1337.9, wps=127.6, ups=0.1, wpb=1337.9, bsz=80, num_updates=150, lr=7.06215e-07, gnorm=0.38, clip=0, loss_scale=128, train_wall=105, gb_free=6.7, wall=1581
2022-06-28 03:29:52 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 7081 loss=-0, score=1.167, ntokens=1331.8, nsentences=80, sample_size=1331.8, wps=128, ups=0.1, wpb=1331.8, bsz=80, num_updates=160, lr=7.53296e-07, gnorm=0.682, clip=0, loss_scale=128, train_wall=104, gb_free=6.7, wall=1686
2022-06-28 03:31:36 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-28 03:31:45 - progress_bar.py[line:274] - INFO: epoch 001:    171 / 7081 loss=-0.002, score=1.256, ntokens=1336.3, nsentences=80, sample_size=1336.3, wps=118.1, ups=0.09, wpb=1336.3, bsz=80, num_updates=170, lr=8.00377e-07, gnorm=0.338, clip=0, loss_scale=64, train_wall=113, gb_free=6.7, wall=1799
2022-06-28 03:33:29 - progress_bar.py[line:274] - INFO: epoch 001:    181 / 7081 loss=-0.001, score=1.175, ntokens=1334.3, nsentences=80, sample_size=1334.3, wps=128.1, ups=0.1, wpb=1334.3, bsz=80, num_updates=180, lr=8.47458e-07, gnorm=0.683, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=1903
2022-06-28 03:35:14 - progress_bar.py[line:274] - INFO: epoch 001:    191 / 7081 loss=-0.001, score=1.133, ntokens=1333.7, nsentences=80, sample_size=1333.7, wps=127.8, ups=0.1, wpb=1333.7, bsz=80, num_updates=190, lr=8.94539e-07, gnorm=0.416, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=2007
2022-06-28 03:36:59 - progress_bar.py[line:274] - INFO: epoch 001:    201 / 7081 loss=0.001, score=1.198, ntokens=1335.5, nsentences=80, sample_size=1335.5, wps=126.8, ups=0.09, wpb=1335.5, bsz=80, num_updates=200, lr=9.4162e-07, gnorm=0.72, clip=10, loss_scale=64, train_wall=105, gb_free=6.7, wall=2113
2022-06-28 03:38:44 - progress_bar.py[line:274] - INFO: epoch 001:    211 / 7081 loss=-0, score=1.177, ntokens=1336.4, nsentences=80, sample_size=1336.4, wps=127.9, ups=0.1, wpb=1336.4, bsz=80, num_updates=210, lr=9.88701e-07, gnorm=0.42, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=2217
2022-06-28 03:40:28 - progress_bar.py[line:274] - INFO: epoch 001:    221 / 7081 loss=-0.001, score=1.165, ntokens=1329.6, nsentences=80, sample_size=1329.6, wps=127.8, ups=0.1, wpb=1329.6, bsz=80, num_updates=220, lr=1.03578e-06, gnorm=0.361, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=2321
2022-06-28 03:42:12 - progress_bar.py[line:274] - INFO: epoch 001:    231 / 7081 loss=-0, score=1.139, ntokens=1336.9, nsentences=80, sample_size=1336.9, wps=128.1, ups=0.1, wpb=1336.9, bsz=80, num_updates=230, lr=1.08286e-06, gnorm=0.429, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=2425
2022-06-28 03:43:57 - progress_bar.py[line:274] - INFO: epoch 001:    241 / 7081 loss=-0.001, score=1.177, ntokens=1334, nsentences=80, sample_size=1334, wps=127.5, ups=0.1, wpb=1334, bsz=80, num_updates=240, lr=1.12994e-06, gnorm=0.359, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=2530
2022-06-28 03:45:41 - progress_bar.py[line:274] - INFO: epoch 001:    251 / 7081 loss=-0.001, score=1.149, ntokens=1332.1, nsentences=80, sample_size=1332.1, wps=127.4, ups=0.1, wpb=1332.1, bsz=80, num_updates=250, lr=1.17702e-06, gnorm=0.371, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=2635
2022-06-28 03:47:26 - progress_bar.py[line:274] - INFO: epoch 001:    261 / 7081 loss=0, score=1.162, ntokens=1333.8, nsentences=80, sample_size=1333.8, wps=127.3, ups=0.1, wpb=1333.8, bsz=80, num_updates=260, lr=1.22411e-06, gnorm=0.335, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=2739
2022-06-28 03:49:11 - progress_bar.py[line:274] - INFO: epoch 001:    271 / 7081 loss=0.001, score=1.26, ntokens=1337.2, nsentences=80, sample_size=1337.2, wps=126.9, ups=0.09, wpb=1337.2, bsz=80, num_updates=270, lr=1.27119e-06, gnorm=0.509, clip=20, loss_scale=64, train_wall=105, gb_free=6.7, wall=2845
2022-06-28 03:50:55 - progress_bar.py[line:274] - INFO: epoch 001:    281 / 7081 loss=0, score=1.173, ntokens=1319, nsentences=80, sample_size=1319, wps=127.2, ups=0.1, wpb=1319, bsz=80, num_updates=280, lr=1.31827e-06, gnorm=0.233, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=2949
2022-06-28 03:52:39 - progress_bar.py[line:274] - INFO: epoch 001:    291 / 7081 loss=-0.002, score=1.156, ntokens=1328, nsentences=80, sample_size=1328, wps=127.3, ups=0.1, wpb=1328, bsz=80, num_updates=290, lr=1.36535e-06, gnorm=0.239, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=3053
2022-06-28 03:54:23 - progress_bar.py[line:274] - INFO: epoch 001:    301 / 7081 loss=-0, score=1.165, ntokens=1329.3, nsentences=80, sample_size=1329.3, wps=128.7, ups=0.1, wpb=1329.3, bsz=80, num_updates=300, lr=1.41243e-06, gnorm=0.321, clip=10, loss_scale=64, train_wall=103, gb_free=6.7, wall=3156
2022-06-28 03:56:05 - progress_bar.py[line:274] - INFO: epoch 001:    311 / 7081 loss=0.001, score=1.244, ntokens=1335, nsentences=80, sample_size=1335, wps=130, ups=0.1, wpb=1335, bsz=80, num_updates=310, lr=1.45951e-06, gnorm=0.193, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=3259
2022-06-28 03:57:48 - progress_bar.py[line:274] - INFO: epoch 001:    321 / 7081 loss=-0.001, score=1.176, ntokens=1333.6, nsentences=80, sample_size=1333.6, wps=129.8, ups=0.1, wpb=1333.6, bsz=80, num_updates=320, lr=1.50659e-06, gnorm=0.306, clip=10, loss_scale=64, train_wall=103, gb_free=6.7, wall=3362
2022-06-28 03:59:32 - progress_bar.py[line:274] - INFO: epoch 001:    331 / 7081 loss=-0, score=1.197, ntokens=1340.9, nsentences=80, sample_size=1340.9, wps=128.7, ups=0.1, wpb=1340.9, bsz=80, num_updates=330, lr=1.55367e-06, gnorm=0.82, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=3466
2022-06-28 04:01:17 - progress_bar.py[line:274] - INFO: epoch 001:    341 / 7081 loss=0.001, score=1.087, ntokens=1330.9, nsentences=80, sample_size=1330.9, wps=127.6, ups=0.1, wpb=1330.9, bsz=80, num_updates=340, lr=1.60075e-06, gnorm=0.245, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=3570
2022-06-28 04:03:00 - progress_bar.py[line:274] - INFO: epoch 001:    351 / 7081 loss=0.001, score=1.22, ntokens=1337.3, nsentences=80, sample_size=1337.3, wps=129.2, ups=0.1, wpb=1337.3, bsz=80, num_updates=350, lr=1.64783e-06, gnorm=0.343, clip=10, loss_scale=64, train_wall=103, gb_free=6.7, wall=3674
2022-06-28 04:04:44 - progress_bar.py[line:274] - INFO: epoch 001:    361 / 7081 loss=0, score=1.171, ntokens=1339.1, nsentences=80, sample_size=1339.1, wps=128.9, ups=0.1, wpb=1339.1, bsz=80, num_updates=360, lr=1.69492e-06, gnorm=0.273, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=3778
2022-06-28 04:06:29 - progress_bar.py[line:274] - INFO: epoch 001:    371 / 7081 loss=0, score=1.359, ntokens=1334.3, nsentences=80, sample_size=1334.3, wps=127.5, ups=0.1, wpb=1334.3, bsz=80, num_updates=370, lr=1.742e-06, gnorm=0.372, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=3882
2022-06-28 04:08:13 - progress_bar.py[line:274] - INFO: epoch 001:    381 / 7081 loss=0, score=1.167, ntokens=1328.7, nsentences=80, sample_size=1328.7, wps=126.7, ups=0.1, wpb=1328.7, bsz=80, num_updates=380, lr=1.78908e-06, gnorm=0.199, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=3987
2022-06-28 04:09:57 - progress_bar.py[line:274] - INFO: epoch 001:    391 / 7081 loss=-0.001, score=1.347, ntokens=1329.7, nsentences=80, sample_size=1329.7, wps=128.1, ups=0.1, wpb=1329.7, bsz=80, num_updates=390, lr=1.83616e-06, gnorm=0.268, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=4091
2022-06-28 04:11:42 - progress_bar.py[line:274] - INFO: epoch 001:    401 / 7081 loss=-0.001, score=1.213, ntokens=1340.5, nsentences=80, sample_size=1340.5, wps=127.8, ups=0.1, wpb=1340.5, bsz=80, num_updates=400, lr=1.88324e-06, gnorm=0.614, clip=20, loss_scale=64, train_wall=105, gb_free=6.7, wall=4196
2022-06-28 04:13:27 - progress_bar.py[line:274] - INFO: epoch 001:    411 / 7081 loss=0, score=1.189, ntokens=1337.1, nsentences=80, sample_size=1337.1, wps=127.9, ups=0.1, wpb=1337.1, bsz=80, num_updates=410, lr=1.93032e-06, gnorm=0.437, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=4300
2022-06-28 04:15:12 - progress_bar.py[line:274] - INFO: epoch 001:    421 / 7081 loss=0.002, score=1.159, ntokens=1341, nsentences=80, sample_size=1341, wps=127.8, ups=0.1, wpb=1341, bsz=80, num_updates=420, lr=1.9774e-06, gnorm=0.425, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=4405
2022-06-28 04:16:56 - progress_bar.py[line:274] - INFO: epoch 001:    431 / 7081 loss=-0.002, score=1.199, ntokens=1333.7, nsentences=80, sample_size=1333.7, wps=127.3, ups=0.1, wpb=1333.7, bsz=80, num_updates=430, lr=2.02448e-06, gnorm=0.472, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=4510
2022-06-28 04:18:40 - progress_bar.py[line:274] - INFO: epoch 001:    441 / 7081 loss=-0, score=1.113, ntokens=1338.6, nsentences=80, sample_size=1338.6, wps=129.1, ups=0.1, wpb=1338.6, bsz=80, num_updates=440, lr=2.07156e-06, gnorm=0.328, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=4614
2022-06-28 04:20:24 - progress_bar.py[line:274] - INFO: epoch 001:    451 / 7081 loss=0, score=1.325, ntokens=1330, nsentences=80, sample_size=1330, wps=128.1, ups=0.1, wpb=1330, bsz=80, num_updates=450, lr=2.11864e-06, gnorm=0.277, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=4717
2022-06-28 04:22:08 - progress_bar.py[line:274] - INFO: epoch 001:    461 / 7081 loss=-0.001, score=1.129, ntokens=1329, nsentences=80, sample_size=1329, wps=127.8, ups=0.1, wpb=1329, bsz=80, num_updates=460, lr=2.16573e-06, gnorm=0.537, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=4821
2022-06-28 04:23:53 - progress_bar.py[line:274] - INFO: epoch 001:    471 / 7081 loss=0, score=1.093, ntokens=1327.7, nsentences=80, sample_size=1327.7, wps=126.7, ups=0.1, wpb=1327.7, bsz=80, num_updates=470, lr=2.21281e-06, gnorm=0.521, clip=10, loss_scale=64, train_wall=105, gb_free=6.7, wall=4926
2022-06-28 04:25:36 - progress_bar.py[line:274] - INFO: epoch 001:    481 / 7081 loss=0, score=1.177, ntokens=1335.7, nsentences=80, sample_size=1335.7, wps=128.8, ups=0.1, wpb=1335.7, bsz=80, num_updates=480, lr=2.25989e-06, gnorm=0.324, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=5030
2022-06-28 04:27:21 - progress_bar.py[line:274] - INFO: epoch 001:    491 / 7081 loss=0.001, score=1.29, ntokens=1339.2, nsentences=80, sample_size=1339.2, wps=128.5, ups=0.1, wpb=1339.2, bsz=80, num_updates=490, lr=2.30697e-06, gnorm=0.656, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=5134
2022-06-28 04:29:05 - progress_bar.py[line:274] - INFO: epoch 001:    501 / 7081 loss=0.002, score=1.104, ntokens=1340.8, nsentences=80, sample_size=1340.8, wps=128.6, ups=0.1, wpb=1340.8, bsz=80, num_updates=500, lr=2.35405e-06, gnorm=1.059, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=5238
slice_id 1 seek offset 2500
2022-06-28 04:29:05 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-28 05:32:33 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0 | score 1.117 | ntokens 166.858 | nsentences 10 | sample_size 166.858 | cider 1.139 | wps 109.5 | wpb 166.9 | bsz 10 | num_updates 500
2022-06-28 05:32:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 500 updates
2022-06-28 05:32:33 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_500.pt
2022-06-28 05:32:47 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_500.pt
2022-06-28 05:33:00 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_500.pt (epoch 1 @ 500 updates, score 1.139) (writing took 26.77282413560897 seconds)
2022-06-28 05:34:44 - progress_bar.py[line:274] - INFO: epoch 001:    511 / 7081 loss=0.001, score=1.108, ntokens=1332.7, nsentences=80, sample_size=1332.7, wps=3.4, ups=0, wpb=1332.7, bsz=80, num_updates=510, lr=2.40113e-06, gnorm=0.629, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=9178
2022-06-28 05:36:28 - progress_bar.py[line:274] - INFO: epoch 001:    521 / 7081 loss=0.002, score=1.178, ntokens=1333, nsentences=80, sample_size=1333, wps=127.6, ups=0.1, wpb=1333, bsz=80, num_updates=520, lr=2.44821e-06, gnorm=1.009, clip=30, loss_scale=64, train_wall=104, gb_free=6.7, wall=9282
2022-06-28 05:38:13 - progress_bar.py[line:274] - INFO: epoch 001:    531 / 7081 loss=-0.001, score=1.085, ntokens=1336.1, nsentences=80, sample_size=1336.1, wps=128.1, ups=0.1, wpb=1336.1, bsz=80, num_updates=530, lr=2.49529e-06, gnorm=0.249, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=9386
2022-06-28 05:39:57 - progress_bar.py[line:274] - INFO: epoch 001:    541 / 7081 loss=0, score=1.11, ntokens=1330.9, nsentences=80, sample_size=1330.9, wps=127.7, ups=0.1, wpb=1330.9, bsz=80, num_updates=540, lr=2.54237e-06, gnorm=0.22, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=9491
2022-06-28 05:41:42 - progress_bar.py[line:274] - INFO: epoch 001:    551 / 7081 loss=-0, score=1.181, ntokens=1334.8, nsentences=80, sample_size=1334.8, wps=127, ups=0.1, wpb=1334.8, bsz=80, num_updates=550, lr=2.58945e-06, gnorm=0.208, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=9596
2022-06-28 05:43:27 - progress_bar.py[line:274] - INFO: epoch 001:    561 / 7081 loss=-0.002, score=1.186, ntokens=1338.3, nsentences=80, sample_size=1338.3, wps=127.2, ups=0.1, wpb=1338.3, bsz=80, num_updates=560, lr=2.63653e-06, gnorm=0.656, clip=30, loss_scale=64, train_wall=105, gb_free=6.7, wall=9701
2022-06-28 05:45:12 - progress_bar.py[line:274] - INFO: epoch 001:    571 / 7081 loss=-0.002, score=1.227, ntokens=1337.2, nsentences=80, sample_size=1337.2, wps=127.4, ups=0.1, wpb=1337.2, bsz=80, num_updates=570, lr=2.68362e-06, gnorm=0.585, clip=10, loss_scale=64, train_wall=105, gb_free=6.7, wall=9806
2022-06-28 05:46:56 - progress_bar.py[line:274] - INFO: epoch 001:    581 / 7081 loss=0.002, score=1.158, ntokens=1338.7, nsentences=80, sample_size=1338.7, wps=128.6, ups=0.1, wpb=1338.7, bsz=80, num_updates=580, lr=2.7307e-06, gnorm=0.334, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=9910
2022-06-28 05:48:41 - progress_bar.py[line:274] - INFO: epoch 001:    591 / 7081 loss=-0.002, score=1.268, ntokens=1331.3, nsentences=80, sample_size=1331.3, wps=127.3, ups=0.1, wpb=1331.3, bsz=80, num_updates=590, lr=2.77778e-06, gnorm=0.732, clip=30, loss_scale=64, train_wall=104, gb_free=6.7, wall=10015
2022-06-28 05:50:25 - progress_bar.py[line:274] - INFO: epoch 001:    601 / 7081 loss=-0, score=1.206, ntokens=1337.1, nsentences=80, sample_size=1337.1, wps=128.5, ups=0.1, wpb=1337.1, bsz=80, num_updates=600, lr=2.82486e-06, gnorm=0.493, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=10119
2022-06-28 05:52:10 - progress_bar.py[line:274] - INFO: epoch 001:    611 / 7081 loss=0, score=1.183, ntokens=1343.9, nsentences=80, sample_size=1343.9, wps=128.2, ups=0.1, wpb=1343.9, bsz=80, num_updates=610, lr=2.87194e-06, gnorm=0.231, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=10223
2022-06-28 05:53:54 - progress_bar.py[line:274] - INFO: epoch 001:    621 / 7081 loss=-0, score=1.241, ntokens=1324.5, nsentences=80, sample_size=1324.5, wps=127.4, ups=0.1, wpb=1324.5, bsz=80, num_updates=620, lr=2.91902e-06, gnorm=0.307, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=10327
2022-06-28 05:55:39 - progress_bar.py[line:274] - INFO: epoch 001:    631 / 7081 loss=-0, score=1.21, ntokens=1332.9, nsentences=80, sample_size=1332.9, wps=127.3, ups=0.1, wpb=1332.9, bsz=80, num_updates=630, lr=2.9661e-06, gnorm=0.363, clip=10, loss_scale=64, train_wall=105, gb_free=6.7, wall=10432
2022-06-28 05:57:23 - progress_bar.py[line:274] - INFO: epoch 001:    641 / 7081 loss=-0.001, score=1.244, ntokens=1337.3, nsentences=80, sample_size=1337.3, wps=127.7, ups=0.1, wpb=1337.3, bsz=80, num_updates=640, lr=3.01318e-06, gnorm=0.432, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=10537
2022-06-28 05:59:08 - progress_bar.py[line:274] - INFO: epoch 001:    651 / 7081 loss=-0, score=1.165, ntokens=1336.6, nsentences=80, sample_size=1336.6, wps=127.8, ups=0.1, wpb=1336.6, bsz=80, num_updates=650, lr=3.06026e-06, gnorm=0.375, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=10641
2022-06-28 06:00:52 - progress_bar.py[line:274] - INFO: epoch 001:    661 / 7081 loss=-0.002, score=1.239, ntokens=1340.9, nsentences=80, sample_size=1340.9, wps=129.3, ups=0.1, wpb=1340.9, bsz=80, num_updates=660, lr=3.10734e-06, gnorm=0.414, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=10745
2022-06-28 06:02:36 - progress_bar.py[line:274] - INFO: epoch 001:    671 / 7081 loss=-0.001, score=1.073, ntokens=1328.5, nsentences=80, sample_size=1328.5, wps=127.7, ups=0.1, wpb=1328.5, bsz=80, num_updates=670, lr=3.15443e-06, gnorm=0.227, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=10849
2022-06-28 06:04:21 - progress_bar.py[line:274] - INFO: epoch 001:    681 / 7081 loss=-0, score=1.28, ntokens=1326.1, nsentences=80, sample_size=1326.1, wps=126.5, ups=0.1, wpb=1326.1, bsz=80, num_updates=680, lr=3.20151e-06, gnorm=0.251, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=10954
2022-06-28 06:06:04 - progress_bar.py[line:274] - INFO: epoch 001:    691 / 7081 loss=-0, score=1.164, ntokens=1333.6, nsentences=80, sample_size=1333.6, wps=128.8, ups=0.1, wpb=1333.6, bsz=80, num_updates=690, lr=3.24859e-06, gnorm=0.324, clip=0, loss_scale=128, train_wall=103, gb_free=6.7, wall=11058
2022-06-28 06:07:49 - progress_bar.py[line:274] - INFO: epoch 001:    701 / 7081 loss=-0.001, score=1.244, ntokens=1342.5, nsentences=80, sample_size=1342.5, wps=128.2, ups=0.1, wpb=1342.5, bsz=80, num_updates=700, lr=3.29567e-06, gnorm=0.242, clip=0, loss_scale=128, train_wall=105, gb_free=6.7, wall=11162
2022-06-28 06:09:33 - progress_bar.py[line:274] - INFO: epoch 001:    711 / 7081 loss=-0, score=1.121, ntokens=1330.6, nsentences=80, sample_size=1330.6, wps=127.4, ups=0.1, wpb=1330.6, bsz=80, num_updates=710, lr=3.34275e-06, gnorm=0.413, clip=0, loss_scale=128, train_wall=104, gb_free=6.7, wall=11267
2022-06-28 06:11:17 - progress_bar.py[line:274] - INFO: epoch 001:    721 / 7081 loss=0.001, score=1.064, ntokens=1342.5, nsentences=80, sample_size=1342.5, wps=129.3, ups=0.1, wpb=1342.5, bsz=80, num_updates=720, lr=3.38983e-06, gnorm=0.435, clip=10, loss_scale=128, train_wall=104, gb_free=6.7, wall=11371
2022-06-28 06:13:01 - progress_bar.py[line:274] - INFO: epoch 001:    731 / 7081 loss=0.001, score=1.151, ntokens=1322.8, nsentences=80, sample_size=1322.8, wps=127.5, ups=0.1, wpb=1322.8, bsz=80, num_updates=730, lr=3.43691e-06, gnorm=0.767, clip=20, loss_scale=128, train_wall=104, gb_free=6.7, wall=11474
2022-06-28 06:14:43 - progress_bar.py[line:274] - INFO: epoch 001:    741 / 7081 loss=-0.001, score=1.21, ntokens=1338.9, nsentences=80, sample_size=1338.9, wps=130.5, ups=0.1, wpb=1338.9, bsz=80, num_updates=740, lr=3.48399e-06, gnorm=0.442, clip=20, loss_scale=128, train_wall=102, gb_free=6.7, wall=11577
2022-06-28 06:15:13 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-28 06:16:39 - progress_bar.py[line:274] - INFO: epoch 001:    752 / 7081 loss=-0, score=1.238, ntokens=1336.8, nsentences=80, sample_size=1336.8, wps=115.9, ups=0.09, wpb=1336.8, bsz=80, num_updates=750, lr=3.53107e-06, gnorm=0.329, clip=10, loss_scale=64, train_wall=115, gb_free=6.7, wall=11692
2022-06-28 06:18:23 - progress_bar.py[line:274] - INFO: epoch 001:    762 / 7081 loss=0, score=1.258, ntokens=1331.9, nsentences=80, sample_size=1331.9, wps=127.3, ups=0.1, wpb=1331.9, bsz=80, num_updates=760, lr=3.57815e-06, gnorm=0.358, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=11797
2022-06-28 06:20:07 - progress_bar.py[line:274] - INFO: epoch 001:    772 / 7081 loss=0.001, score=1.11, ntokens=1335.3, nsentences=80, sample_size=1335.3, wps=128.6, ups=0.1, wpb=1335.3, bsz=80, num_updates=770, lr=3.62524e-06, gnorm=0.3, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=11901
2022-06-28 06:21:52 - progress_bar.py[line:274] - INFO: epoch 001:    782 / 7081 loss=-0.001, score=1.086, ntokens=1346, nsentences=80, sample_size=1346, wps=128.7, ups=0.1, wpb=1346, bsz=80, num_updates=780, lr=3.67232e-06, gnorm=0.502, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=12005
2022-06-28 06:23:37 - progress_bar.py[line:274] - INFO: epoch 001:    792 / 7081 loss=-0.001, score=1.238, ntokens=1331.2, nsentences=80, sample_size=1331.2, wps=127.1, ups=0.1, wpb=1331.2, bsz=80, num_updates=790, lr=3.7194e-06, gnorm=0.339, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=12110
2022-06-28 06:25:21 - progress_bar.py[line:274] - INFO: epoch 001:    802 / 7081 loss=-0.001, score=1.165, ntokens=1341.4, nsentences=80, sample_size=1341.4, wps=128.8, ups=0.1, wpb=1341.4, bsz=80, num_updates=800, lr=3.76648e-06, gnorm=0.432, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=12214
2022-06-28 06:27:06 - progress_bar.py[line:274] - INFO: epoch 001:    812 / 7081 loss=-0, score=1.137, ntokens=1340.5, nsentences=80, sample_size=1340.5, wps=127.8, ups=0.1, wpb=1340.5, bsz=80, num_updates=810, lr=3.81356e-06, gnorm=0.581, clip=20, loss_scale=64, train_wall=105, gb_free=6.7, wall=12319
2022-06-28 06:28:49 - progress_bar.py[line:274] - INFO: epoch 001:    822 / 7081 loss=-0, score=1.13, ntokens=1326, nsentences=80, sample_size=1326, wps=127.8, ups=0.1, wpb=1326, bsz=80, num_updates=820, lr=3.86064e-06, gnorm=0.201, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=12423
2022-06-28 06:30:33 - progress_bar.py[line:274] - INFO: epoch 001:    832 / 7081 loss=-0.001, score=1.247, ntokens=1336.5, nsentences=80, sample_size=1336.5, wps=128.4, ups=0.1, wpb=1336.5, bsz=80, num_updates=830, lr=3.90772e-06, gnorm=0.43, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=12527
2022-06-28 06:32:16 - progress_bar.py[line:274] - INFO: epoch 001:    842 / 7081 loss=-0.001, score=1.225, ntokens=1337.2, nsentences=80, sample_size=1337.2, wps=130, ups=0.1, wpb=1337.2, bsz=80, num_updates=840, lr=3.9548e-06, gnorm=0.549, clip=10, loss_scale=64, train_wall=103, gb_free=6.7, wall=12630
2022-06-28 06:34:00 - progress_bar.py[line:274] - INFO: epoch 001:    852 / 7081 loss=-0, score=1.105, ntokens=1339.8, nsentences=80, sample_size=1339.8, wps=129.4, ups=0.1, wpb=1339.8, bsz=80, num_updates=850, lr=4.00188e-06, gnorm=0.394, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=12733
2022-06-28 06:35:44 - progress_bar.py[line:274] - INFO: epoch 001:    862 / 7081 loss=0, score=1.161, ntokens=1337.4, nsentences=80, sample_size=1337.4, wps=128.6, ups=0.1, wpb=1337.4, bsz=80, num_updates=860, lr=4.04896e-06, gnorm=0.288, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=12837
2022-06-28 06:37:27 - progress_bar.py[line:274] - INFO: epoch 001:    872 / 7081 loss=-0, score=1.143, ntokens=1342.5, nsentences=80, sample_size=1342.5, wps=129.8, ups=0.1, wpb=1342.5, bsz=80, num_updates=870, lr=4.09605e-06, gnorm=0.256, clip=10, loss_scale=64, train_wall=103, gb_free=6.7, wall=12941
2022-06-28 06:39:12 - progress_bar.py[line:274] - INFO: epoch 001:    882 / 7081 loss=0, score=1.191, ntokens=1337.8, nsentences=80, sample_size=1337.8, wps=127.8, ups=0.1, wpb=1337.8, bsz=80, num_updates=880, lr=4.14313e-06, gnorm=0.359, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=13045
2022-06-28 06:40:57 - progress_bar.py[line:274] - INFO: epoch 001:    892 / 7081 loss=-0.001, score=1.187, ntokens=1341.2, nsentences=80, sample_size=1341.2, wps=128.2, ups=0.1, wpb=1341.2, bsz=80, num_updates=890, lr=4.19021e-06, gnorm=0.533, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=13150
2022-06-28 06:42:41 - progress_bar.py[line:274] - INFO: epoch 001:    902 / 7081 loss=-0.001, score=1.194, ntokens=1335.6, nsentences=80, sample_size=1335.6, wps=128.3, ups=0.1, wpb=1335.6, bsz=80, num_updates=900, lr=4.23729e-06, gnorm=0.276, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=13254
2022-06-28 06:44:24 - progress_bar.py[line:274] - INFO: epoch 001:    912 / 7081 loss=-0, score=1.207, ntokens=1327.2, nsentences=80, sample_size=1327.2, wps=128, ups=0.1, wpb=1327.2, bsz=80, num_updates=910, lr=4.28437e-06, gnorm=0.417, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=13358
2022-06-28 06:46:08 - progress_bar.py[line:274] - INFO: epoch 001:    922 / 7081 loss=-0, score=1.215, ntokens=1333.8, nsentences=80, sample_size=1333.8, wps=128.1, ups=0.1, wpb=1333.8, bsz=80, num_updates=920, lr=4.33145e-06, gnorm=0.518, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=13462
2022-06-28 06:47:52 - progress_bar.py[line:274] - INFO: epoch 001:    932 / 7081 loss=-0.002, score=1.063, ntokens=1326.2, nsentences=80, sample_size=1326.2, wps=127.8, ups=0.1, wpb=1326.2, bsz=80, num_updates=930, lr=4.37853e-06, gnorm=0.542, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=13566
2022-06-28 06:49:37 - progress_bar.py[line:274] - INFO: epoch 001:    942 / 7081 loss=0.001, score=1.166, ntokens=1327.3, nsentences=80, sample_size=1327.3, wps=126.9, ups=0.1, wpb=1327.3, bsz=80, num_updates=940, lr=4.42561e-06, gnorm=0.398, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=13670
2022-06-28 06:51:22 - progress_bar.py[line:274] - INFO: epoch 001:    952 / 7081 loss=-0.001, score=1.249, ntokens=1327.2, nsentences=80, sample_size=1327.2, wps=126.8, ups=0.1, wpb=1327.2, bsz=80, num_updates=950, lr=4.47269e-06, gnorm=0.274, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=13775
2022-06-28 06:53:07 - progress_bar.py[line:274] - INFO: epoch 001:    962 / 7081 loss=-0, score=1.083, ntokens=1337.2, nsentences=80, sample_size=1337.2, wps=127, ups=0.09, wpb=1337.2, bsz=80, num_updates=960, lr=4.51977e-06, gnorm=0.379, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=13880
2022-06-28 06:53:19 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-06-28 06:55:01 - progress_bar.py[line:274] - INFO: epoch 001:    973 / 7081 loss=0.001, score=1.249, ntokens=1325.4, nsentences=80, sample_size=1325.4, wps=116, ups=0.09, wpb=1325.4, bsz=80, num_updates=970, lr=4.56685e-06, gnorm=0.475, clip=20, loss_scale=32, train_wall=114, gb_free=6.7, wall=13995
2022-06-28 06:56:45 - progress_bar.py[line:274] - INFO: epoch 001:    983 / 7081 loss=0.001, score=1.256, ntokens=1330.4, nsentences=80, sample_size=1330.4, wps=128.5, ups=0.1, wpb=1330.4, bsz=80, num_updates=980, lr=4.61394e-06, gnorm=0.433, clip=10, loss_scale=32, train_wall=103, gb_free=6.7, wall=14098
2022-06-28 06:58:28 - progress_bar.py[line:274] - INFO: epoch 001:    993 / 7081 loss=-0, score=1.183, ntokens=1329.1, nsentences=80, sample_size=1329.1, wps=128.5, ups=0.1, wpb=1329.1, bsz=80, num_updates=990, lr=4.66102e-06, gnorm=0.222, clip=0, loss_scale=32, train_wall=103, gb_free=6.7, wall=14202
2022-06-28 07:00:13 - progress_bar.py[line:274] - INFO: epoch 001:   1003 / 7081 loss=0, score=1.283, ntokens=1344, nsentences=80, sample_size=1344, wps=128.4, ups=0.1, wpb=1344, bsz=80, num_updates=1000, lr=4.7081e-06, gnorm=0.71, clip=10, loss_scale=32, train_wall=105, gb_free=6.7, wall=14306
slice_id 1 seek offset 2500
2022-06-28 07:00:13 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-28 08:03:49 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0 | score 1.123 | ntokens 167.552 | nsentences 10 | sample_size 167.552 | cider 1.145 | wps 109.8 | wpb 167.6 | bsz 10 | num_updates 1000 | best_cider 1.145
2022-06-28 08:03:49 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 1000 updates
2022-06-28 08:03:49 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_1000.pt
2022-06-28 08:03:59 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_1000.pt
2022-06-28 08:05:35 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 1.145) (writing took 106.05317588150501 seconds)
2022-06-28 08:07:19 - progress_bar.py[line:274] - INFO: epoch 001:   1013 / 7081 loss=0.001, score=1.26, ntokens=1337.7, nsentences=80, sample_size=1337.7, wps=3.3, ups=0, wpb=1337.7, bsz=80, num_updates=1010, lr=4.75518e-06, gnorm=0.237, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=18333
2022-06-28 08:09:04 - progress_bar.py[line:274] - INFO: epoch 001:   1023 / 7081 loss=-0, score=1.162, ntokens=1338.1, nsentences=80, sample_size=1338.1, wps=127.9, ups=0.1, wpb=1338.1, bsz=80, num_updates=1020, lr=4.80226e-06, gnorm=0.163, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=18438
2022-06-28 08:10:48 - progress_bar.py[line:274] - INFO: epoch 001:   1033 / 7081 loss=0, score=1.101, ntokens=1345.7, nsentences=80, sample_size=1345.7, wps=128.9, ups=0.1, wpb=1345.7, bsz=80, num_updates=1030, lr=4.84934e-06, gnorm=0.503, clip=20, loss_scale=32, train_wall=104, gb_free=6.7, wall=18542
2022-06-28 08:12:33 - progress_bar.py[line:274] - INFO: epoch 001:   1043 / 7081 loss=-0, score=1.191, ntokens=1344.2, nsentences=80, sample_size=1344.2, wps=128.9, ups=0.1, wpb=1344.2, bsz=80, num_updates=1040, lr=4.89642e-06, gnorm=1.262, clip=10, loss_scale=32, train_wall=104, gb_free=6.7, wall=18646
2022-06-28 08:14:17 - progress_bar.py[line:274] - INFO: epoch 001:   1053 / 7081 loss=-0, score=1.279, ntokens=1344.4, nsentences=80, sample_size=1344.4, wps=128.7, ups=0.1, wpb=1344.4, bsz=80, num_updates=1050, lr=4.9435e-06, gnorm=0.302, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=18751
2022-06-28 08:16:01 - progress_bar.py[line:274] - INFO: epoch 001:   1063 / 7081 loss=0, score=1.256, ntokens=1352, nsentences=80, sample_size=1352, wps=130.1, ups=0.1, wpb=1352, bsz=80, num_updates=1060, lr=4.99058e-06, gnorm=0.315, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=18855
2022-06-28 08:17:46 - progress_bar.py[line:274] - INFO: epoch 001:   1073 / 7081 loss=-0, score=1.236, ntokens=1339.8, nsentences=80, sample_size=1339.8, wps=128.4, ups=0.1, wpb=1339.8, bsz=80, num_updates=1070, lr=5.03766e-06, gnorm=0.175, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=18959
2022-06-28 08:19:32 - progress_bar.py[line:274] - INFO: epoch 001:   1083 / 7081 loss=-0, score=1.143, ntokens=1347.9, nsentences=80, sample_size=1347.9, wps=127, ups=0.09, wpb=1347.9, bsz=80, num_updates=1080, lr=5.08475e-06, gnorm=0.46, clip=20, loss_scale=32, train_wall=106, gb_free=6.7, wall=19065
2022-06-28 08:21:16 - progress_bar.py[line:274] - INFO: epoch 001:   1093 / 7081 loss=-0, score=1.222, ntokens=1346.3, nsentences=80, sample_size=1346.3, wps=129.2, ups=0.1, wpb=1346.3, bsz=80, num_updates=1090, lr=5.13183e-06, gnorm=0.406, clip=20, loss_scale=32, train_wall=104, gb_free=6.7, wall=19169
2022-06-28 08:22:59 - progress_bar.py[line:274] - INFO: epoch 001:   1103 / 7081 loss=0, score=1.308, ntokens=1347.2, nsentences=80, sample_size=1347.2, wps=130.3, ups=0.1, wpb=1347.2, bsz=80, num_updates=1100, lr=5.17891e-06, gnorm=0.965, clip=30, loss_scale=32, train_wall=103, gb_free=6.7, wall=19273
2022-06-28 08:24:44 - progress_bar.py[line:274] - INFO: epoch 001:   1113 / 7081 loss=-0.001, score=1.199, ntokens=1348.5, nsentences=80, sample_size=1348.5, wps=129, ups=0.1, wpb=1348.5, bsz=80, num_updates=1110, lr=5.22599e-06, gnorm=0.382, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=19377
2022-06-28 08:26:27 - progress_bar.py[line:274] - INFO: epoch 001:   1123 / 7081 loss=-0.001, score=1.159, ntokens=1342.6, nsentences=80, sample_size=1342.6, wps=130.5, ups=0.1, wpb=1342.6, bsz=80, num_updates=1120, lr=5.27307e-06, gnorm=0.278, clip=10, loss_scale=32, train_wall=103, gb_free=6.7, wall=19480
2022-06-28 08:28:12 - progress_bar.py[line:274] - INFO: epoch 001:   1133 / 7081 loss=0, score=1.319, ntokens=1345.1, nsentences=80, sample_size=1345.1, wps=128, ups=0.1, wpb=1345.1, bsz=80, num_updates=1130, lr=5.32015e-06, gnorm=0.78, clip=40, loss_scale=32, train_wall=105, gb_free=6.7, wall=19585
2022-06-28 08:29:56 - progress_bar.py[line:274] - INFO: epoch 001:   1143 / 7081 loss=0, score=1.145, ntokens=1343.8, nsentences=80, sample_size=1343.8, wps=128.2, ups=0.1, wpb=1343.8, bsz=80, num_updates=1140, lr=5.36723e-06, gnorm=0.36, clip=0, loss_scale=32, train_wall=105, gb_free=6.7, wall=19690
2022-06-28 08:31:42 - progress_bar.py[line:274] - INFO: epoch 001:   1153 / 7081 loss=0, score=1.23, ntokens=1345.6, nsentences=80, sample_size=1345.6, wps=127.4, ups=0.09, wpb=1345.6, bsz=80, num_updates=1150, lr=5.41431e-06, gnorm=0.498, clip=20, loss_scale=32, train_wall=106, gb_free=6.7, wall=19796
2022-06-28 08:33:26 - progress_bar.py[line:274] - INFO: epoch 001:   1163 / 7081 loss=0.001, score=1.302, ntokens=1339.5, nsentences=80, sample_size=1339.5, wps=128.9, ups=0.1, wpb=1339.5, bsz=80, num_updates=1160, lr=5.46139e-06, gnorm=0.298, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=19900
2022-06-28 08:35:10 - progress_bar.py[line:274] - INFO: epoch 001:   1173 / 7081 loss=-0, score=1.256, ntokens=1341.5, nsentences=80, sample_size=1341.5, wps=128.5, ups=0.1, wpb=1341.5, bsz=80, num_updates=1170, lr=5.50847e-06, gnorm=0.289, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=20004
2022-06-28 08:36:55 - progress_bar.py[line:274] - INFO: epoch 001:   1183 / 7081 loss=-0.001, score=1.286, ntokens=1338.4, nsentences=80, sample_size=1338.4, wps=128.2, ups=0.1, wpb=1338.4, bsz=80, num_updates=1180, lr=5.55556e-06, gnorm=0.392, clip=10, loss_scale=32, train_wall=104, gb_free=6.7, wall=20108
2022-06-28 08:38:39 - progress_bar.py[line:274] - INFO: epoch 001:   1193 / 7081 loss=0, score=1.182, ntokens=1349.6, nsentences=80, sample_size=1349.6, wps=129.5, ups=0.1, wpb=1349.6, bsz=80, num_updates=1190, lr=5.60264e-06, gnorm=0.431, clip=10, loss_scale=32, train_wall=104, gb_free=6.7, wall=20213
2022-06-28 08:40:24 - progress_bar.py[line:274] - INFO: epoch 001:   1203 / 7081 loss=-0.002, score=1.097, ntokens=1345.2, nsentences=80, sample_size=1345.2, wps=128.7, ups=0.1, wpb=1345.2, bsz=80, num_updates=1200, lr=5.64972e-06, gnorm=0.412, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=20317
2022-06-28 08:42:08 - progress_bar.py[line:274] - INFO: epoch 001:   1213 / 7081 loss=0.001, score=1.155, ntokens=1349.8, nsentences=80, sample_size=1349.8, wps=128.9, ups=0.1, wpb=1349.8, bsz=80, num_updates=1210, lr=5.6968e-06, gnorm=0.3, clip=0, loss_scale=32, train_wall=105, gb_free=6.7, wall=20422
2022-06-28 08:43:53 - progress_bar.py[line:274] - INFO: epoch 001:   1223 / 7081 loss=0, score=1.355, ntokens=1343.5, nsentences=80, sample_size=1343.5, wps=128.9, ups=0.1, wpb=1343.5, bsz=80, num_updates=1220, lr=5.74388e-06, gnorm=0.447, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=20526
2022-06-28 08:45:38 - progress_bar.py[line:274] - INFO: epoch 001:   1233 / 7081 loss=-0.001, score=1.19, ntokens=1352, nsentences=80, sample_size=1352, wps=128.8, ups=0.1, wpb=1352, bsz=80, num_updates=1230, lr=5.79096e-06, gnorm=0.511, clip=10, loss_scale=32, train_wall=105, gb_free=6.7, wall=20631
2022-06-28 08:47:22 - progress_bar.py[line:274] - INFO: epoch 001:   1243 / 7081 loss=-0, score=1.094, ntokens=1352.2, nsentences=80, sample_size=1352.2, wps=129.7, ups=0.1, wpb=1352.2, bsz=80, num_updates=1240, lr=5.83804e-06, gnorm=0.559, clip=20, loss_scale=32, train_wall=104, gb_free=6.7, wall=20735
2022-06-28 08:49:06 - progress_bar.py[line:274] - INFO: epoch 001:   1253 / 7081 loss=-0, score=1.188, ntokens=1347.2, nsentences=80, sample_size=1347.2, wps=129.3, ups=0.1, wpb=1347.2, bsz=80, num_updates=1250, lr=5.88512e-06, gnorm=0.469, clip=10, loss_scale=32, train_wall=104, gb_free=6.7, wall=20840
2022-06-28 08:50:51 - progress_bar.py[line:274] - INFO: epoch 001:   1263 / 7081 loss=-0.001, score=1.277, ntokens=1334.7, nsentences=80, sample_size=1334.7, wps=127.3, ups=0.1, wpb=1334.7, bsz=80, num_updates=1260, lr=5.9322e-06, gnorm=0.457, clip=10, loss_scale=32, train_wall=105, gb_free=6.7, wall=20944
2022-06-28 08:52:35 - progress_bar.py[line:274] - INFO: epoch 001:   1273 / 7081 loss=-0.001, score=1.149, ntokens=1334.8, nsentences=80, sample_size=1334.8, wps=127.7, ups=0.1, wpb=1334.8, bsz=80, num_updates=1270, lr=5.97928e-06, gnorm=0.564, clip=20, loss_scale=32, train_wall=104, gb_free=6.7, wall=21049
2022-06-28 08:54:20 - progress_bar.py[line:274] - INFO: epoch 001:   1283 / 7081 loss=0, score=1.274, ntokens=1329.2, nsentences=80, sample_size=1329.2, wps=127, ups=0.1, wpb=1329.2, bsz=80, num_updates=1280, lr=6.02637e-06, gnorm=0.316, clip=0, loss_scale=32, train_wall=105, gb_free=6.7, wall=21154
2022-06-28 08:56:05 - progress_bar.py[line:274] - INFO: epoch 001:   1293 / 7081 loss=-0, score=1.226, ntokens=1327.3, nsentences=80, sample_size=1327.3, wps=126.2, ups=0.1, wpb=1327.3, bsz=80, num_updates=1290, lr=6.07345e-06, gnorm=0.404, clip=10, loss_scale=32, train_wall=105, gb_free=6.7, wall=21259
2022-06-28 08:57:50 - progress_bar.py[line:274] - INFO: epoch 001:   1303 / 7081 loss=-0, score=1.385, ntokens=1324.1, nsentences=80, sample_size=1324.1, wps=126.9, ups=0.1, wpb=1324.1, bsz=80, num_updates=1300, lr=6.12053e-06, gnorm=0.4, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=21363
2022-06-28 08:59:34 - progress_bar.py[line:274] - INFO: epoch 001:   1313 / 7081 loss=-0.001, score=1.173, ntokens=1330, nsentences=80, sample_size=1330, wps=127.8, ups=0.1, wpb=1330, bsz=80, num_updates=1310, lr=6.16761e-06, gnorm=0.207, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=21467
2022-06-28 09:01:18 - progress_bar.py[line:274] - INFO: epoch 001:   1323 / 7081 loss=0, score=1.148, ntokens=1333.4, nsentences=80, sample_size=1333.4, wps=127.3, ups=0.1, wpb=1333.4, bsz=80, num_updates=1320, lr=6.21469e-06, gnorm=0.352, clip=0, loss_scale=32, train_wall=105, gb_free=6.7, wall=21572
2022-06-28 09:03:02 - progress_bar.py[line:274] - INFO: epoch 001:   1333 / 7081 loss=0, score=1.234, ntokens=1334.1, nsentences=80, sample_size=1334.1, wps=128.5, ups=0.1, wpb=1334.1, bsz=80, num_updates=1330, lr=6.26177e-06, gnorm=0.312, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=21676
2022-06-28 09:04:46 - progress_bar.py[line:274] - INFO: epoch 001:   1343 / 7081 loss=-0.001, score=1.179, ntokens=1329.1, nsentences=80, sample_size=1329.1, wps=127.9, ups=0.1, wpb=1329.1, bsz=80, num_updates=1340, lr=6.30885e-06, gnorm=0.695, clip=10, loss_scale=32, train_wall=104, gb_free=6.7, wall=21780
2022-06-28 09:06:32 - progress_bar.py[line:274] - INFO: epoch 001:   1353 / 7081 loss=-0, score=1.212, ntokens=1337, nsentences=80, sample_size=1337, wps=126.1, ups=0.09, wpb=1337, bsz=80, num_updates=1350, lr=6.35593e-06, gnorm=0.391, clip=0, loss_scale=32, train_wall=106, gb_free=6.7, wall=21886
2022-06-28 09:08:17 - progress_bar.py[line:274] - INFO: epoch 001:   1363 / 7081 loss=-0.001, score=1.116, ntokens=1316.9, nsentences=80, sample_size=1316.9, wps=125.6, ups=0.1, wpb=1316.9, bsz=80, num_updates=1360, lr=6.40301e-06, gnorm=0.284, clip=0, loss_scale=32, train_wall=105, gb_free=6.7, wall=21991
2022-06-28 09:10:01 - progress_bar.py[line:274] - INFO: epoch 001:   1373 / 7081 loss=-0.001, score=1.21, ntokens=1313, nsentences=80, sample_size=1313, wps=125.9, ups=0.1, wpb=1313, bsz=80, num_updates=1370, lr=6.45009e-06, gnorm=0.374, clip=10, loss_scale=32, train_wall=104, gb_free=6.7, wall=22095
2022-06-28 09:11:46 - progress_bar.py[line:274] - INFO: epoch 001:   1383 / 7081 loss=-0.001, score=1.261, ntokens=1322, nsentences=80, sample_size=1322, wps=126.3, ups=0.1, wpb=1322, bsz=80, num_updates=1380, lr=6.49718e-06, gnorm=0.566, clip=10, loss_scale=32, train_wall=104, gb_free=6.7, wall=22200
2022-06-28 09:13:31 - progress_bar.py[line:274] - INFO: epoch 001:   1393 / 7081 loss=-0.001, score=1.212, ntokens=1329.3, nsentences=80, sample_size=1329.3, wps=127, ups=0.1, wpb=1329.3, bsz=80, num_updates=1390, lr=6.54426e-06, gnorm=0.325, clip=0, loss_scale=32, train_wall=105, gb_free=6.7, wall=22304
2022-06-28 09:15:15 - progress_bar.py[line:274] - INFO: epoch 001:   1403 / 7081 loss=-0, score=1.14, ntokens=1329.8, nsentences=80, sample_size=1329.8, wps=127.4, ups=0.1, wpb=1329.8, bsz=80, num_updates=1400, lr=6.59134e-06, gnorm=0.613, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=22409
2022-06-28 09:16:59 - progress_bar.py[line:274] - INFO: epoch 001:   1413 / 7081 loss=0, score=1.145, ntokens=1324.4, nsentences=80, sample_size=1324.4, wps=128, ups=0.1, wpb=1324.4, bsz=80, num_updates=1410, lr=6.63842e-06, gnorm=0.26, clip=0, loss_scale=32, train_wall=103, gb_free=6.7, wall=22512
2022-06-28 09:18:44 - progress_bar.py[line:274] - INFO: epoch 001:   1423 / 7081 loss=-0.001, score=1.265, ntokens=1319.5, nsentences=80, sample_size=1319.5, wps=125.4, ups=0.1, wpb=1319.5, bsz=80, num_updates=1420, lr=6.6855e-06, gnorm=0.621, clip=10, loss_scale=32, train_wall=105, gb_free=6.7, wall=22617
2022-06-28 09:20:28 - progress_bar.py[line:274] - INFO: epoch 001:   1433 / 7081 loss=-0, score=1.278, ntokens=1300.8, nsentences=80, sample_size=1300.8, wps=124.4, ups=0.1, wpb=1300.8, bsz=80, num_updates=1430, lr=6.73258e-06, gnorm=0.415, clip=0, loss_scale=32, train_wall=104, gb_free=6.7, wall=22722
2022-06-28 09:22:12 - progress_bar.py[line:274] - INFO: epoch 001:   1443 / 7081 loss=0.001, score=1.199, ntokens=1290.4, nsentences=80, sample_size=1290.4, wps=124.6, ups=0.1, wpb=1290.4, bsz=80, num_updates=1440, lr=6.77966e-06, gnorm=0.264, clip=10, loss_scale=32, train_wall=103, gb_free=6.7, wall=22825
2022-06-28 09:23:55 - progress_bar.py[line:274] - INFO: epoch 001:   1453 / 7081 loss=-0, score=1.133, ntokens=1297.5, nsentences=80, sample_size=1297.5, wps=126.1, ups=0.1, wpb=1297.5, bsz=80, num_updates=1450, lr=6.82674e-06, gnorm=0.254, clip=0, loss_scale=32, train_wall=103, gb_free=6.7, wall=22928
2022-06-28 09:25:40 - progress_bar.py[line:274] - INFO: epoch 001:   1463 / 7081 loss=-0.002, score=1.273, ntokens=1287.8, nsentences=80, sample_size=1287.8, wps=122.6, ups=0.1, wpb=1287.8, bsz=80, num_updates=1460, lr=6.87382e-06, gnorm=0.52, clip=10, loss_scale=32, train_wall=105, gb_free=6.7, wall=23033
2022-06-28 09:27:24 - progress_bar.py[line:274] - INFO: epoch 001:   1473 / 7081 loss=0.001, score=1.181, ntokens=1302.8, nsentences=80, sample_size=1302.8, wps=125.4, ups=0.1, wpb=1302.8, bsz=80, num_updates=1470, lr=6.9209e-06, gnorm=0.496, clip=10, loss_scale=32, train_wall=104, gb_free=6.7, wall=23137
2022-06-28 09:29:06 - progress_bar.py[line:274] - INFO: epoch 001:   1483 / 7081 loss=-0.001, score=1.295, ntokens=1302.3, nsentences=80, sample_size=1302.3, wps=127.5, ups=0.1, wpb=1302.3, bsz=80, num_updates=1480, lr=6.96798e-06, gnorm=0.318, clip=0, loss_scale=64, train_wall=102, gb_free=6.7, wall=23239
2022-06-28 09:30:49 - progress_bar.py[line:274] - INFO: epoch 001:   1493 / 7081 loss=0.001, score=1.214, ntokens=1297.1, nsentences=80, sample_size=1297.1, wps=125.2, ups=0.1, wpb=1297.1, bsz=80, num_updates=1490, lr=7.01507e-06, gnorm=0.677, clip=20, loss_scale=64, train_wall=103, gb_free=6.7, wall=23343
2022-06-28 09:32:33 - progress_bar.py[line:274] - INFO: epoch 001:   1503 / 7081 loss=-0.001, score=1.364, ntokens=1311.3, nsentences=80, sample_size=1311.3, wps=126.5, ups=0.1, wpb=1311.3, bsz=80, num_updates=1500, lr=7.06215e-06, gnorm=0.275, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=23447
slice_id 1 seek offset 2500
2022-06-28 09:32:33 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 0 seek offset 0
2022-06-28 10:34:25 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0 | score 1.14 | ntokens 162.921 | nsentences 10 | sample_size 162.921 | cider 1.161 | wps 109.7 | wpb 162.9 | bsz 10 | num_updates 1500 | best_cider 1.161
2022-06-28 10:34:25 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 1500 updates
2022-06-28 10:34:25 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_1500.pt
2022-06-28 10:34:35 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_1500.pt
2022-06-28 10:36:23 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_1500.pt (epoch 1 @ 1500 updates, score 1.161) (writing took 117.7597120301798 seconds)
2022-06-28 10:38:03 - progress_bar.py[line:274] - INFO: epoch 001:   1513 / 7081 loss=-0.001, score=1.185, ntokens=1308, nsentences=80, sample_size=1308, wps=3.3, ups=0, wpb=1308, bsz=80, num_updates=1510, lr=7.10923e-06, gnorm=0.677, clip=20, loss_scale=64, train_wall=99, gb_free=6.7, wall=27377
2022-06-28 10:39:43 - progress_bar.py[line:274] - INFO: epoch 001:   1523 / 7081 loss=0.001, score=1.115, ntokens=1292.2, nsentences=80, sample_size=1292.2, wps=129.6, ups=0.1, wpb=1292.2, bsz=80, num_updates=1520, lr=7.15631e-06, gnorm=0.476, clip=20, loss_scale=64, train_wall=100, gb_free=6.7, wall=27476
2022-06-28 10:41:21 - progress_bar.py[line:274] - INFO: epoch 001:   1533 / 7081 loss=0.001, score=1.278, ntokens=1290.5, nsentences=80, sample_size=1290.5, wps=131.2, ups=0.1, wpb=1290.5, bsz=80, num_updates=1530, lr=7.20339e-06, gnorm=0.431, clip=10, loss_scale=64, train_wall=98, gb_free=6.7, wall=27575
2022-06-28 10:43:01 - progress_bar.py[line:274] - INFO: epoch 001:   1543 / 7081 loss=0, score=1.337, ntokens=1297.2, nsentences=80, sample_size=1297.2, wps=130.1, ups=0.1, wpb=1297.2, bsz=80, num_updates=1540, lr=7.25047e-06, gnorm=0.49, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=27674
2022-06-28 10:44:40 - progress_bar.py[line:274] - INFO: epoch 001:   1553 / 7081 loss=0.001, score=1.098, ntokens=1301.6, nsentences=80, sample_size=1301.6, wps=131.4, ups=0.1, wpb=1301.6, bsz=80, num_updates=1550, lr=7.29755e-06, gnorm=0.632, clip=20, loss_scale=64, train_wall=99, gb_free=6.7, wall=27774
2022-06-28 10:46:20 - progress_bar.py[line:274] - INFO: epoch 001:   1563 / 7081 loss=0, score=1.209, ntokens=1304.6, nsentences=80, sample_size=1304.6, wps=130.9, ups=0.1, wpb=1304.6, bsz=80, num_updates=1560, lr=7.34463e-06, gnorm=0.333, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=27873
2022-06-28 10:47:59 - progress_bar.py[line:274] - INFO: epoch 001:   1573 / 7081 loss=0, score=1.201, ntokens=1302.9, nsentences=80, sample_size=1302.9, wps=130.6, ups=0.1, wpb=1302.9, bsz=80, num_updates=1570, lr=7.39171e-06, gnorm=0.16, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=27973
2022-06-28 10:49:39 - progress_bar.py[line:274] - INFO: epoch 001:   1583 / 7081 loss=0.001, score=1.328, ntokens=1311.3, nsentences=80, sample_size=1311.3, wps=131.5, ups=0.1, wpb=1311.3, bsz=80, num_updates=1580, lr=7.43879e-06, gnorm=0.333, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=28073
2022-06-28 10:51:19 - progress_bar.py[line:274] - INFO: epoch 001:   1593 / 7081 loss=-0.001, score=1.221, ntokens=1306.4, nsentences=80, sample_size=1306.4, wps=130.9, ups=0.1, wpb=1306.4, bsz=80, num_updates=1590, lr=7.48588e-06, gnorm=0.239, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=28172
2022-06-28 10:52:59 - progress_bar.py[line:274] - INFO: epoch 001:   1603 / 7081 loss=-0.001, score=1.204, ntokens=1297.8, nsentences=80, sample_size=1297.8, wps=130, ups=0.1, wpb=1297.8, bsz=80, num_updates=1600, lr=7.53296e-06, gnorm=0.584, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=28272
2022-06-28 10:54:38 - progress_bar.py[line:274] - INFO: epoch 001:   1613 / 7081 loss=0.001, score=1.209, ntokens=1305.8, nsentences=80, sample_size=1305.8, wps=131.1, ups=0.1, wpb=1305.8, bsz=80, num_updates=1610, lr=7.58004e-06, gnorm=0.465, clip=10, loss_scale=64, train_wall=99, gb_free=6.7, wall=28372
2022-06-28 10:56:18 - progress_bar.py[line:274] - INFO: epoch 001:   1623 / 7081 loss=-0, score=1.218, ntokens=1268.6, nsentences=80, sample_size=1268.6, wps=127.8, ups=0.1, wpb=1268.6, bsz=80, num_updates=1620, lr=7.62712e-06, gnorm=0.211, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=28471
2022-06-28 10:57:57 - progress_bar.py[line:274] - INFO: epoch 001:   1633 / 7081 loss=0.001, score=1.136, ntokens=1268.1, nsentences=80, sample_size=1268.1, wps=127.9, ups=0.1, wpb=1268.1, bsz=80, num_updates=1630, lr=7.6742e-06, gnorm=0.274, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=28570
2022-06-28 10:59:36 - progress_bar.py[line:274] - INFO: epoch 001:   1643 / 7081 loss=-0.001, score=1.142, ntokens=1254.6, nsentences=80, sample_size=1254.6, wps=125.9, ups=0.1, wpb=1254.6, bsz=80, num_updates=1640, lr=7.72128e-06, gnorm=0.504, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=28670
2022-06-28 11:01:15 - progress_bar.py[line:274] - INFO: epoch 001:   1653 / 7081 loss=-0.001, score=1.256, ntokens=1246, nsentences=80, sample_size=1246, wps=126.2, ups=0.1, wpb=1246, bsz=80, num_updates=1650, lr=7.76836e-06, gnorm=0.498, clip=20, loss_scale=64, train_wall=99, gb_free=6.7, wall=28769
2022-06-28 11:02:54 - progress_bar.py[line:274] - INFO: epoch 001:   1663 / 7081 loss=-0.001, score=1.252, ntokens=1256.3, nsentences=80, sample_size=1256.3, wps=127.5, ups=0.1, wpb=1256.3, bsz=80, num_updates=1660, lr=7.81544e-06, gnorm=0.303, clip=0, loss_scale=64, train_wall=98, gb_free=6.7, wall=28867
2022-06-28 11:04:34 - progress_bar.py[line:274] - INFO: epoch 001:   1673 / 7081 loss=-0, score=1.075, ntokens=1277.9, nsentences=80, sample_size=1277.9, wps=127.9, ups=0.1, wpb=1277.9, bsz=80, num_updates=1670, lr=7.86252e-06, gnorm=0.361, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=28967
2022-06-28 11:06:12 - progress_bar.py[line:274] - INFO: epoch 001:   1683 / 7081 loss=-0.001, score=1.231, ntokens=1289.1, nsentences=80, sample_size=1289.1, wps=130.9, ups=0.1, wpb=1289.1, bsz=80, num_updates=1680, lr=7.9096e-06, gnorm=0.196, clip=0, loss_scale=64, train_wall=98, gb_free=6.7, wall=29066
2022-06-28 11:07:52 - progress_bar.py[line:274] - INFO: epoch 001:   1693 / 7081 loss=-0, score=1.133, ntokens=1284.8, nsentences=80, sample_size=1284.8, wps=129, ups=0.1, wpb=1284.8, bsz=80, num_updates=1690, lr=7.95669e-06, gnorm=0.263, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=29165
2022-06-28 11:09:31 - progress_bar.py[line:274] - INFO: epoch 001:   1703 / 7081 loss=-0, score=1.171, ntokens=1304.7, nsentences=80, sample_size=1304.7, wps=131.1, ups=0.1, wpb=1304.7, bsz=80, num_updates=1700, lr=8.00377e-06, gnorm=0.537, clip=20, loss_scale=64, train_wall=99, gb_free=6.7, wall=29265
2022-06-28 11:11:10 - progress_bar.py[line:274] - INFO: epoch 001:   1713 / 7081 loss=-0, score=1.212, ntokens=1288.6, nsentences=80, sample_size=1288.6, wps=129.8, ups=0.1, wpb=1288.6, bsz=80, num_updates=1710, lr=8.05085e-06, gnorm=0.259, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=29364
2022-06-28 11:12:48 - progress_bar.py[line:274] - INFO: epoch 001:   1723 / 7081 loss=-0.001, score=1.177, ntokens=1289.6, nsentences=80, sample_size=1289.6, wps=132.5, ups=0.1, wpb=1289.6, bsz=80, num_updates=1720, lr=8.09793e-06, gnorm=0.412, clip=10, loss_scale=64, train_wall=97, gb_free=6.7, wall=29461
2022-06-28 11:14:26 - progress_bar.py[line:274] - INFO: epoch 001:   1733 / 7081 loss=0.001, score=1.173, ntokens=1297.8, nsentences=80, sample_size=1297.8, wps=132, ups=0.1, wpb=1297.8, bsz=80, num_updates=1730, lr=8.14501e-06, gnorm=0.211, clip=0, loss_scale=64, train_wall=98, gb_free=6.7, wall=29560
2022-06-28 11:16:05 - progress_bar.py[line:274] - INFO: epoch 001:   1743 / 7081 loss=-0.001, score=1.228, ntokens=1301.6, nsentences=80, sample_size=1301.6, wps=131.9, ups=0.1, wpb=1301.6, bsz=80, num_updates=1740, lr=8.19209e-06, gnorm=0.435, clip=20, loss_scale=64, train_wall=99, gb_free=6.7, wall=29658
2022-06-28 11:17:44 - progress_bar.py[line:274] - INFO: epoch 001:   1753 / 7081 loss=-0.001, score=1.311, ntokens=1284.8, nsentences=80, sample_size=1284.8, wps=130.1, ups=0.1, wpb=1284.8, bsz=80, num_updates=1750, lr=8.23917e-06, gnorm=0.303, clip=10, loss_scale=64, train_wall=99, gb_free=6.7, wall=29757
2022-06-28 11:19:23 - progress_bar.py[line:274] - INFO: epoch 001:   1763 / 7081 loss=-0.001, score=1.293, ntokens=1271, nsentences=80, sample_size=1271, wps=128.3, ups=0.1, wpb=1271, bsz=80, num_updates=1760, lr=8.28625e-06, gnorm=0.246, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=29856
2022-06-28 11:21:05 - progress_bar.py[line:274] - INFO: epoch 001:   1773 / 7081 loss=-0.002, score=1.247, ntokens=1273.9, nsentences=80, sample_size=1273.9, wps=125, ups=0.1, wpb=1273.9, bsz=80, num_updates=1770, lr=8.33333e-06, gnorm=0.368, clip=10, loss_scale=64, train_wall=102, gb_free=6.7, wall=29958
2022-06-28 11:22:48 - progress_bar.py[line:274] - INFO: epoch 001:   1783 / 7081 loss=-0, score=1.203, ntokens=1278, nsentences=80, sample_size=1278, wps=123.7, ups=0.1, wpb=1278, bsz=80, num_updates=1780, lr=8.38041e-06, gnorm=0.229, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=30061
2022-06-28 11:24:32 - progress_bar.py[line:274] - INFO: epoch 001:   1793 / 7081 loss=0.001, score=1.202, ntokens=1274.7, nsentences=80, sample_size=1274.7, wps=122.4, ups=0.1, wpb=1274.7, bsz=80, num_updates=1790, lr=8.4275e-06, gnorm=0.472, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=30166
2022-06-28 11:26:16 - progress_bar.py[line:274] - INFO: epoch 001:   1803 / 7081 loss=0.001, score=1.188, ntokens=1274, nsentences=80, sample_size=1274, wps=122.2, ups=0.1, wpb=1274, bsz=80, num_updates=1800, lr=8.47458e-06, gnorm=0.361, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=30270
2022-06-28 11:28:00 - progress_bar.py[line:274] - INFO: epoch 001:   1813 / 7081 loss=-0.001, score=1.14, ntokens=1255.5, nsentences=80, sample_size=1255.5, wps=120.8, ups=0.1, wpb=1255.5, bsz=80, num_updates=1810, lr=8.52166e-06, gnorm=0.429, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=30374
2022-06-28 11:29:45 - progress_bar.py[line:274] - INFO: epoch 001:   1823 / 7081 loss=-0, score=1.237, ntokens=1232.4, nsentences=80, sample_size=1232.4, wps=117.6, ups=0.1, wpb=1232.4, bsz=80, num_updates=1820, lr=8.56874e-06, gnorm=0.399, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=30479
2022-06-28 11:31:31 - progress_bar.py[line:274] - INFO: epoch 001:   1833 / 7081 loss=0, score=1.209, ntokens=1210.3, nsentences=80, sample_size=1210.3, wps=114.2, ups=0.09, wpb=1210.3, bsz=80, num_updates=1830, lr=8.61582e-06, gnorm=0.27, clip=10, loss_scale=64, train_wall=106, gb_free=6.7, wall=30585
2022-06-28 11:33:14 - progress_bar.py[line:274] - INFO: epoch 001:   1843 / 7081 loss=-0.001, score=1.304, ntokens=1226.3, nsentences=80, sample_size=1226.3, wps=118.8, ups=0.1, wpb=1226.3, bsz=80, num_updates=1840, lr=8.6629e-06, gnorm=0.266, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=30688
2022-06-28 11:34:57 - progress_bar.py[line:274] - INFO: epoch 001:   1853 / 7081 loss=-0, score=1.222, ntokens=1226.5, nsentences=80, sample_size=1226.5, wps=119, ups=0.1, wpb=1226.5, bsz=80, num_updates=1850, lr=8.70998e-06, gnorm=0.387, clip=10, loss_scale=64, train_wall=103, gb_free=6.7, wall=30791
2022-06-28 11:36:40 - progress_bar.py[line:274] - INFO: epoch 001:   1863 / 7081 loss=0, score=1.388, ntokens=1199.1, nsentences=80, sample_size=1199.1, wps=116.4, ups=0.1, wpb=1199.1, bsz=80, num_updates=1860, lr=8.75706e-06, gnorm=0.308, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=30894
2022-06-28 11:38:24 - progress_bar.py[line:274] - INFO: epoch 001:   1873 / 7081 loss=-0.001, score=1.258, ntokens=1238.6, nsentences=80, sample_size=1238.6, wps=119.5, ups=0.1, wpb=1238.6, bsz=80, num_updates=1870, lr=8.80414e-06, gnorm=0.302, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=30998
2022-06-28 11:40:08 - progress_bar.py[line:274] - INFO: epoch 001:   1883 / 7081 loss=0, score=1.238, ntokens=1236.4, nsentences=80, sample_size=1236.4, wps=119.3, ups=0.1, wpb=1236.4, bsz=80, num_updates=1880, lr=8.85122e-06, gnorm=0.391, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=31101
2022-06-28 11:41:51 - progress_bar.py[line:274] - INFO: epoch 001:   1893 / 7081 loss=-0, score=1.151, ntokens=1223.3, nsentences=80, sample_size=1223.3, wps=118.5, ups=0.1, wpb=1223.3, bsz=80, num_updates=1890, lr=8.89831e-06, gnorm=0.347, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=31204
2022-06-28 11:43:35 - progress_bar.py[line:274] - INFO: epoch 001:   1903 / 7081 loss=0, score=1.192, ntokens=1232.8, nsentences=80, sample_size=1232.8, wps=118.5, ups=0.1, wpb=1232.8, bsz=80, num_updates=1900, lr=8.94539e-06, gnorm=0.349, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=31308
2022-06-28 11:45:18 - progress_bar.py[line:274] - INFO: epoch 001:   1913 / 7081 loss=0.001, score=1.182, ntokens=1222.7, nsentences=80, sample_size=1222.7, wps=118.4, ups=0.1, wpb=1222.7, bsz=80, num_updates=1910, lr=8.99247e-06, gnorm=0.33, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=31412
2022-06-28 11:47:02 - progress_bar.py[line:274] - INFO: epoch 001:   1923 / 7081 loss=-0, score=1.124, ntokens=1234.4, nsentences=80, sample_size=1234.4, wps=118.9, ups=0.1, wpb=1234.4, bsz=80, num_updates=1920, lr=9.03955e-06, gnorm=0.314, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=31516
2022-06-28 11:48:46 - progress_bar.py[line:274] - INFO: epoch 001:   1933 / 7081 loss=0.001, score=1.294, ntokens=1231.6, nsentences=80, sample_size=1231.6, wps=118.5, ups=0.1, wpb=1231.6, bsz=80, num_updates=1930, lr=9.08663e-06, gnorm=0.255, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=31619
2022-06-28 11:50:30 - progress_bar.py[line:274] - INFO: epoch 001:   1943 / 7081 loss=-0.001, score=1.176, ntokens=1220.4, nsentences=80, sample_size=1220.4, wps=117.7, ups=0.1, wpb=1220.4, bsz=80, num_updates=1940, lr=9.13371e-06, gnorm=0.281, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=31723
2022-06-28 11:52:13 - progress_bar.py[line:274] - INFO: epoch 001:   1953 / 7081 loss=-0.002, score=1.167, ntokens=1234.3, nsentences=80, sample_size=1234.3, wps=119.5, ups=0.1, wpb=1234.3, bsz=80, num_updates=1950, lr=9.18079e-06, gnorm=0.232, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=31826
2022-06-28 11:53:54 - progress_bar.py[line:274] - INFO: epoch 001:   1963 / 7081 loss=0, score=1.116, ntokens=1221.9, nsentences=80, sample_size=1221.9, wps=120.9, ups=0.1, wpb=1221.9, bsz=80, num_updates=1960, lr=9.22787e-06, gnorm=0.402, clip=20, loss_scale=64, train_wall=101, gb_free=6.7, wall=31928
2022-06-28 11:55:36 - progress_bar.py[line:274] - INFO: epoch 001:   1973 / 7081 loss=0, score=1.159, ntokens=1242.7, nsentences=80, sample_size=1242.7, wps=122, ups=0.1, wpb=1242.7, bsz=80, num_updates=1970, lr=9.27495e-06, gnorm=0.328, clip=0, loss_scale=64, train_wall=102, gb_free=6.7, wall=32029
2022-06-28 11:57:20 - progress_bar.py[line:274] - INFO: epoch 001:   1983 / 7081 loss=0, score=1.291, ntokens=1246.2, nsentences=80, sample_size=1246.2, wps=120, ups=0.1, wpb=1246.2, bsz=80, num_updates=1980, lr=9.32203e-06, gnorm=0.528, clip=10, loss_scale=64, train_wall=104, gb_free=6.7, wall=32133
2022-06-28 11:59:03 - progress_bar.py[line:274] - INFO: epoch 001:   1993 / 7081 loss=-0, score=1.226, ntokens=1189.7, nsentences=80, sample_size=1189.7, wps=115.2, ups=0.1, wpb=1189.7, bsz=80, num_updates=1990, lr=9.36911e-06, gnorm=0.299, clip=0, loss_scale=128, train_wall=103, gb_free=6.7, wall=32237
2022-06-28 12:00:46 - progress_bar.py[line:274] - INFO: epoch 001:   2003 / 7081 loss=-0.002, score=1.193, ntokens=1202.8, nsentences=80, sample_size=1202.8, wps=116.7, ups=0.1, wpb=1202.8, bsz=80, num_updates=2000, lr=9.4162e-06, gnorm=0.455, clip=10, loss_scale=128, train_wall=103, gb_free=6.7, wall=32340
slice_id 1 seek offset 2500
2022-06-28 12:00:46 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-28 13:04:00 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0 | score 1.142 | ntokens 148.688 | nsentences 10 | sample_size 148.688 | cider 1.163 | wps 98 | wpb 148.7 | bsz 10 | num_updates 2000 | best_cider 1.163
2022-06-28 13:04:00 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2022-06-28 13:04:00 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_2000.pt
2022-06-28 13:04:11 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_2000.pt
2022-06-28 13:05:32 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 1.163) (writing took 92.29784856270999 seconds)
2022-06-28 13:07:15 - progress_bar.py[line:274] - INFO: epoch 001:   2013 / 7081 loss=-0.001, score=1.128, ntokens=1206, nsentences=80, sample_size=1206, wps=3, ups=0, wpb=1206, bsz=80, num_updates=2010, lr=9.46328e-06, gnorm=0.254, clip=0, loss_scale=128, train_wall=103, gb_free=6.7, wall=36329
2022-06-28 13:08:59 - progress_bar.py[line:274] - INFO: epoch 001:   2023 / 7081 loss=0, score=1.242, ntokens=1225.4, nsentences=80, sample_size=1225.4, wps=117.8, ups=0.1, wpb=1225.4, bsz=80, num_updates=2020, lr=9.51036e-06, gnorm=0.407, clip=0, loss_scale=128, train_wall=104, gb_free=6.7, wall=36433
2022-06-28 13:10:43 - progress_bar.py[line:274] - INFO: epoch 001:   2033 / 7081 loss=0, score=1.087, ntokens=1235.3, nsentences=80, sample_size=1235.3, wps=118.6, ups=0.1, wpb=1235.3, bsz=80, num_updates=2030, lr=9.55744e-06, gnorm=0.339, clip=0, loss_scale=128, train_wall=104, gb_free=6.7, wall=36537
2022-06-28 13:12:28 - progress_bar.py[line:274] - INFO: epoch 001:   2043 / 7081 loss=0, score=1.178, ntokens=1245.2, nsentences=80, sample_size=1245.2, wps=119.4, ups=0.1, wpb=1245.2, bsz=80, num_updates=2040, lr=9.60452e-06, gnorm=0.365, clip=10, loss_scale=128, train_wall=104, gb_free=6.7, wall=36641
2022-06-28 13:14:11 - progress_bar.py[line:274] - INFO: epoch 001:   2053 / 7081 loss=-0.001, score=1.239, ntokens=1249.5, nsentences=80, sample_size=1249.5, wps=120.7, ups=0.1, wpb=1249.5, bsz=80, num_updates=2050, lr=9.6516e-06, gnorm=0.401, clip=10, loss_scale=128, train_wall=103, gb_free=6.7, wall=36745
2022-06-28 13:15:56 - progress_bar.py[line:274] - INFO: epoch 001:   2063 / 7081 loss=-0.001, score=1.148, ntokens=1270.1, nsentences=80, sample_size=1270.1, wps=121.5, ups=0.1, wpb=1270.1, bsz=80, num_updates=2060, lr=9.69868e-06, gnorm=0.342, clip=0, loss_scale=128, train_wall=104, gb_free=6.7, wall=36849
2022-06-28 13:17:40 - progress_bar.py[line:274] - INFO: epoch 001:   2073 / 7081 loss=-0.001, score=1.21, ntokens=1282.4, nsentences=80, sample_size=1282.4, wps=123.4, ups=0.1, wpb=1282.4, bsz=80, num_updates=2070, lr=9.74576e-06, gnorm=0.446, clip=10, loss_scale=128, train_wall=104, gb_free=6.7, wall=36953
2022-06-28 13:19:21 - progress_bar.py[line:274] - INFO: epoch 001:   2083 / 7081 loss=-0, score=1.236, ntokens=1292.9, nsentences=80, sample_size=1292.9, wps=127.5, ups=0.1, wpb=1292.9, bsz=80, num_updates=2080, lr=9.79284e-06, gnorm=0.306, clip=10, loss_scale=128, train_wall=101, gb_free=6.7, wall=37055
2022-06-28 13:21:05 - progress_bar.py[line:274] - INFO: epoch 001:   2093 / 7081 loss=0.001, score=1.191, ntokens=1308.5, nsentences=80, sample_size=1308.5, wps=126.4, ups=0.1, wpb=1308.5, bsz=80, num_updates=2090, lr=9.83992e-06, gnorm=0.242, clip=0, loss_scale=128, train_wall=103, gb_free=6.7, wall=37158
2022-06-28 13:23:15 - progress_bar.py[line:274] - INFO: epoch 001:   2103 / 7081 loss=0, score=1.191, ntokens=1309.6, nsentences=80, sample_size=1309.6, wps=100.8, ups=0.08, wpb=1309.6, bsz=80, num_updates=2100, lr=9.88701e-06, gnorm=0.294, clip=0, loss_scale=128, train_wall=130, gb_free=6.7, wall=37288
2022-06-28 13:24:56 - progress_bar.py[line:274] - INFO: epoch 001:   2113 / 7081 loss=0.002, score=1.246, ntokens=1312.1, nsentences=80, sample_size=1312.1, wps=129.7, ups=0.1, wpb=1312.1, bsz=80, num_updates=2110, lr=9.93409e-06, gnorm=0.282, clip=0, loss_scale=128, train_wall=101, gb_free=6.7, wall=37389
2022-06-28 13:26:35 - progress_bar.py[line:274] - INFO: epoch 001:   2123 / 7081 loss=0.001, score=1.207, ntokens=1303.1, nsentences=80, sample_size=1303.1, wps=130.7, ups=0.1, wpb=1303.1, bsz=80, num_updates=2120, lr=9.98117e-06, gnorm=0.289, clip=10, loss_scale=128, train_wall=100, gb_free=6.7, wall=37489
2022-06-28 13:28:15 - progress_bar.py[line:274] - INFO: epoch 001:   2133 / 7081 loss=-0.001, score=1.193, ntokens=1298.1, nsentences=80, sample_size=1298.1, wps=130, ups=0.1, wpb=1298.1, bsz=80, num_updates=2130, lr=9.99823e-06, gnorm=0.248, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=37589
2022-06-28 13:29:55 - progress_bar.py[line:274] - INFO: epoch 001:   2143 / 7081 loss=0.001, score=1.215, ntokens=1281.4, nsentences=80, sample_size=1281.4, wps=128.3, ups=0.1, wpb=1281.4, bsz=80, num_updates=2140, lr=9.99529e-06, gnorm=0.212, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=37689
2022-06-28 13:31:33 - progress_bar.py[line:274] - INFO: epoch 001:   2153 / 7081 loss=-0.002, score=1.183, ntokens=1265.9, nsentences=80, sample_size=1265.9, wps=129.9, ups=0.1, wpb=1265.9, bsz=80, num_updates=2150, lr=9.99234e-06, gnorm=0.276, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=37786
2022-06-28 13:33:11 - progress_bar.py[line:274] - INFO: epoch 001:   2163 / 7081 loss=-0.001, score=1.056, ntokens=1260.9, nsentences=80, sample_size=1260.9, wps=128.7, ups=0.1, wpb=1260.9, bsz=80, num_updates=2160, lr=9.9894e-06, gnorm=0.183, clip=0, loss_scale=128, train_wall=98, gb_free=6.7, wall=37884
2022-06-28 13:34:49 - progress_bar.py[line:274] - INFO: epoch 001:   2173 / 7081 loss=-0.002, score=1.286, ntokens=1277.6, nsentences=80, sample_size=1277.6, wps=129.8, ups=0.1, wpb=1277.6, bsz=80, num_updates=2170, lr=9.98645e-06, gnorm=0.235, clip=0, loss_scale=128, train_wall=98, gb_free=6.7, wall=37983
2022-06-28 13:36:28 - progress_bar.py[line:274] - INFO: epoch 001:   2183 / 7081 loss=-0.001, score=1.168, ntokens=1293.4, nsentences=80, sample_size=1293.4, wps=130.1, ups=0.1, wpb=1293.4, bsz=80, num_updates=2180, lr=9.98351e-06, gnorm=0.244, clip=0, loss_scale=128, train_wall=99, gb_free=6.7, wall=38082
2022-06-28 13:38:08 - progress_bar.py[line:274] - INFO: epoch 001:   2193 / 7081 loss=-0.001, score=1.216, ntokens=1306.3, nsentences=80, sample_size=1306.3, wps=131.3, ups=0.1, wpb=1306.3, bsz=80, num_updates=2190, lr=9.98057e-06, gnorm=0.352, clip=0, loss_scale=128, train_wall=99, gb_free=6.7, wall=38181
2022-06-28 13:39:49 - progress_bar.py[line:274] - INFO: epoch 001:   2203 / 7081 loss=0.001, score=1.253, ntokens=1295.7, nsentences=80, sample_size=1295.7, wps=128.7, ups=0.1, wpb=1295.7, bsz=80, num_updates=2200, lr=9.97762e-06, gnorm=0.129, clip=0, loss_scale=128, train_wall=101, gb_free=6.7, wall=38282
2022-06-28 13:41:29 - progress_bar.py[line:274] - INFO: epoch 001:   2213 / 7081 loss=0.003, score=1.233, ntokens=1310.4, nsentences=80, sample_size=1310.4, wps=130.6, ups=0.1, wpb=1310.4, bsz=80, num_updates=2210, lr=9.97468e-06, gnorm=0.229, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=38382
2022-06-28 13:43:10 - progress_bar.py[line:274] - INFO: epoch 001:   2223 / 7081 loss=-0, score=1.209, ntokens=1307.6, nsentences=80, sample_size=1307.6, wps=129.4, ups=0.1, wpb=1307.6, bsz=80, num_updates=2220, lr=9.97173e-06, gnorm=0.433, clip=20, loss_scale=128, train_wall=101, gb_free=6.7, wall=38483
2022-06-28 13:44:51 - progress_bar.py[line:274] - INFO: epoch 001:   2233 / 7081 loss=-0.001, score=1.215, ntokens=1314.6, nsentences=80, sample_size=1314.6, wps=130.2, ups=0.1, wpb=1314.6, bsz=80, num_updates=2230, lr=9.96879e-06, gnorm=0.573, clip=20, loss_scale=128, train_wall=101, gb_free=6.7, wall=38584
2022-06-28 13:46:32 - progress_bar.py[line:274] - INFO: epoch 001:   2243 / 7081 loss=0, score=1.139, ntokens=1317.6, nsentences=80, sample_size=1317.6, wps=130.8, ups=0.1, wpb=1317.6, bsz=80, num_updates=2240, lr=9.96584e-06, gnorm=0.151, clip=0, loss_scale=128, train_wall=101, gb_free=6.7, wall=38685
2022-06-28 13:48:12 - progress_bar.py[line:274] - INFO: epoch 001:   2253 / 7081 loss=0, score=1.243, ntokens=1329, nsentences=80, sample_size=1329, wps=132, ups=0.1, wpb=1329, bsz=80, num_updates=2250, lr=9.9629e-06, gnorm=0.301, clip=10, loss_scale=128, train_wall=101, gb_free=6.7, wall=38786
2022-06-28 13:49:52 - progress_bar.py[line:274] - INFO: epoch 001:   2263 / 7081 loss=-0.001, score=1.362, ntokens=1330.1, nsentences=80, sample_size=1330.1, wps=133.4, ups=0.1, wpb=1330.1, bsz=80, num_updates=2260, lr=9.95995e-06, gnorm=0.289, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=38886
2022-06-28 13:51:32 - progress_bar.py[line:274] - INFO: epoch 001:   2273 / 7081 loss=0, score=1.101, ntokens=1324.3, nsentences=80, sample_size=1324.3, wps=132, ups=0.1, wpb=1324.3, bsz=80, num_updates=2270, lr=9.95701e-06, gnorm=0.356, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=38986
2022-06-28 13:53:13 - progress_bar.py[line:274] - INFO: epoch 001:   2283 / 7081 loss=0, score=1.271, ntokens=1318.9, nsentences=80, sample_size=1318.9, wps=131, ups=0.1, wpb=1318.9, bsz=80, num_updates=2280, lr=9.95406e-06, gnorm=0.339, clip=10, loss_scale=128, train_wall=101, gb_free=6.7, wall=39087
2022-06-28 13:54:53 - progress_bar.py[line:274] - INFO: epoch 001:   2293 / 7081 loss=-0.003, score=1.331, ntokens=1315, nsentences=80, sample_size=1315, wps=131.1, ups=0.1, wpb=1315, bsz=80, num_updates=2290, lr=9.95112e-06, gnorm=0.695, clip=30, loss_scale=128, train_wall=100, gb_free=6.7, wall=39187
2022-06-28 13:56:33 - progress_bar.py[line:274] - INFO: epoch 001:   2303 / 7081 loss=-0, score=1.239, ntokens=1311.3, nsentences=80, sample_size=1311.3, wps=130.9, ups=0.1, wpb=1311.3, bsz=80, num_updates=2300, lr=9.94817e-06, gnorm=0.209, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=39287
2022-06-28 13:58:14 - progress_bar.py[line:274] - INFO: epoch 001:   2313 / 7081 loss=-0, score=1.25, ntokens=1313.4, nsentences=80, sample_size=1313.4, wps=130.4, ups=0.1, wpb=1313.4, bsz=80, num_updates=2310, lr=9.94523e-06, gnorm=0.206, clip=0, loss_scale=128, train_wall=101, gb_free=6.7, wall=39388
2022-06-28 13:59:54 - progress_bar.py[line:274] - INFO: epoch 001:   2323 / 7081 loss=0.001, score=1.14, ntokens=1316.6, nsentences=80, sample_size=1316.6, wps=131.8, ups=0.1, wpb=1316.6, bsz=80, num_updates=2320, lr=9.94229e-06, gnorm=0.296, clip=10, loss_scale=128, train_wall=100, gb_free=6.7, wall=39488
2022-06-28 14:01:34 - progress_bar.py[line:274] - INFO: epoch 001:   2333 / 7081 loss=0, score=1.263, ntokens=1291.7, nsentences=80, sample_size=1291.7, wps=129, ups=0.1, wpb=1291.7, bsz=80, num_updates=2330, lr=9.93934e-06, gnorm=0.164, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=39588
2022-06-28 14:03:15 - progress_bar.py[line:274] - INFO: epoch 001:   2343 / 7081 loss=-0.001, score=1.201, ntokens=1294.9, nsentences=80, sample_size=1294.9, wps=128.7, ups=0.1, wpb=1294.9, bsz=80, num_updates=2340, lr=9.9364e-06, gnorm=0.484, clip=10, loss_scale=128, train_wall=100, gb_free=6.7, wall=39688
2022-06-28 14:04:55 - progress_bar.py[line:274] - INFO: epoch 001:   2353 / 7081 loss=-0, score=1.189, ntokens=1306.1, nsentences=80, sample_size=1306.1, wps=130.2, ups=0.1, wpb=1306.1, bsz=80, num_updates=2350, lr=9.93345e-06, gnorm=0.197, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=39789
2022-06-28 14:06:35 - progress_bar.py[line:274] - INFO: epoch 001:   2363 / 7081 loss=0, score=1.233, ntokens=1294.1, nsentences=80, sample_size=1294.1, wps=129.3, ups=0.1, wpb=1294.1, bsz=80, num_updates=2360, lr=9.93051e-06, gnorm=0.344, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=39889
2022-06-28 14:08:13 - progress_bar.py[line:274] - INFO: epoch 001:   2373 / 7081 loss=-0, score=1.178, ntokens=1310.2, nsentences=80, sample_size=1310.2, wps=133.5, ups=0.1, wpb=1310.2, bsz=80, num_updates=2370, lr=9.92756e-06, gnorm=0.325, clip=0, loss_scale=128, train_wall=98, gb_free=6.7, wall=39987
2022-06-28 14:09:53 - progress_bar.py[line:274] - INFO: epoch 001:   2383 / 7081 loss=-0.001, score=1.248, ntokens=1298.8, nsentences=80, sample_size=1298.8, wps=130.6, ups=0.1, wpb=1298.8, bsz=80, num_updates=2380, lr=9.92462e-06, gnorm=0.294, clip=0, loss_scale=128, train_wall=99, gb_free=6.7, wall=40086
2022-06-28 14:11:32 - progress_bar.py[line:274] - INFO: epoch 001:   2393 / 7081 loss=-0.001, score=1.22, ntokens=1281, nsentences=80, sample_size=1281, wps=129.3, ups=0.1, wpb=1281, bsz=80, num_updates=2390, lr=9.92167e-06, gnorm=0.416, clip=10, loss_scale=128, train_wall=99, gb_free=6.7, wall=40185
2022-06-28 14:13:10 - progress_bar.py[line:274] - INFO: epoch 001:   2403 / 7081 loss=0, score=1.22, ntokens=1254.9, nsentences=80, sample_size=1254.9, wps=127.5, ups=0.1, wpb=1254.9, bsz=80, num_updates=2400, lr=9.91873e-06, gnorm=0.198, clip=0, loss_scale=128, train_wall=98, gb_free=6.7, wall=40284
2022-06-28 14:14:51 - progress_bar.py[line:274] - INFO: epoch 001:   2413 / 7081 loss=0.002, score=1.073, ntokens=1254.5, nsentences=80, sample_size=1254.5, wps=125, ups=0.1, wpb=1254.5, bsz=80, num_updates=2410, lr=9.91578e-06, gnorm=0.297, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=40384
2022-06-28 14:16:31 - progress_bar.py[line:274] - INFO: epoch 001:   2423 / 7081 loss=-0, score=1.3, ntokens=1266.6, nsentences=80, sample_size=1266.6, wps=125.8, ups=0.1, wpb=1266.6, bsz=80, num_updates=2420, lr=9.91284e-06, gnorm=0.382, clip=10, loss_scale=128, train_wall=101, gb_free=6.7, wall=40485
2022-06-28 14:18:11 - progress_bar.py[line:274] - INFO: epoch 001:   2433 / 7081 loss=0, score=1.25, ntokens=1256.6, nsentences=80, sample_size=1256.6, wps=125.5, ups=0.1, wpb=1256.6, bsz=80, num_updates=2430, lr=9.90989e-06, gnorm=0.692, clip=20, loss_scale=128, train_wall=100, gb_free=6.7, wall=40585
2022-06-28 14:19:51 - progress_bar.py[line:274] - INFO: epoch 001:   2443 / 7081 loss=0.001, score=1.159, ntokens=1242.4, nsentences=80, sample_size=1242.4, wps=124.3, ups=0.1, wpb=1242.4, bsz=80, num_updates=2440, lr=9.90695e-06, gnorm=0.333, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=40685
2022-06-28 14:21:32 - progress_bar.py[line:274] - INFO: epoch 001:   2453 / 7081 loss=-0, score=1.194, ntokens=1252.1, nsentences=80, sample_size=1252.1, wps=123.9, ups=0.1, wpb=1252.1, bsz=80, num_updates=2450, lr=9.90401e-06, gnorm=0.329, clip=0, loss_scale=128, train_wall=101, gb_free=6.7, wall=40786
2022-06-28 14:23:13 - progress_bar.py[line:274] - INFO: epoch 001:   2463 / 7081 loss=-0.001, score=1.22, ntokens=1262.7, nsentences=80, sample_size=1262.7, wps=125.5, ups=0.1, wpb=1262.7, bsz=80, num_updates=2460, lr=9.90106e-06, gnorm=0.722, clip=10, loss_scale=128, train_wall=101, gb_free=6.7, wall=40887
2022-06-28 14:24:52 - progress_bar.py[line:274] - INFO: epoch 001:   2473 / 7081 loss=-0, score=1.239, ntokens=1258.5, nsentences=80, sample_size=1258.5, wps=126.8, ups=0.1, wpb=1258.5, bsz=80, num_updates=2470, lr=9.89812e-06, gnorm=0.252, clip=0, loss_scale=128, train_wall=99, gb_free=6.7, wall=40986
2022-06-28 14:26:32 - progress_bar.py[line:274] - INFO: epoch 001:   2483 / 7081 loss=0.001, score=1.218, ntokens=1274.6, nsentences=80, sample_size=1274.6, wps=127.8, ups=0.1, wpb=1274.6, bsz=80, num_updates=2480, lr=9.89517e-06, gnorm=0.268, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=41086
2022-06-28 14:28:11 - progress_bar.py[line:274] - INFO: epoch 001:   2493 / 7081 loss=-0.001, score=1.149, ntokens=1270.1, nsentences=80, sample_size=1270.1, wps=128, ups=0.1, wpb=1270.1, bsz=80, num_updates=2490, lr=9.89223e-06, gnorm=0.35, clip=0, loss_scale=128, train_wall=99, gb_free=6.7, wall=41185
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-06-28 14:29:51 - progress_bar.py[line:274] - INFO: epoch 001:   2503 / 7081 loss=0, score=1.304, ntokens=1273.7, nsentences=80, sample_size=1273.7, wps=127.7, ups=0.1, wpb=1273.7, bsz=80, num_updates=2500, lr=9.88928e-06, gnorm=0.268, clip=0, loss_scale=256, train_wall=100, gb_free=6.7, wall=41285
2022-06-28 14:29:51 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-06-28 15:29:14 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0 | score 1.148 | ntokens 158.164 | nsentences 10 | sample_size 158.164 | cider 1.17 | wps 111 | wpb 158.2 | bsz 10 | num_updates 2500 | best_cider 1.17
2022-06-28 15:29:14 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2500 updates
2022-06-28 15:29:14 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_2500.pt
2022-06-28 15:29:31 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_2500.pt
2022-06-28 15:31:04 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_2500.pt (epoch 1 @ 2500 updates, score 1.17) (writing took 109.37769170384854 seconds)
2022-06-28 15:32:43 - progress_bar.py[line:274] - INFO: epoch 001:   2513 / 7081 loss=-0, score=1.167, ntokens=1257, nsentences=80, sample_size=1257, wps=3.3, ups=0, wpb=1257, bsz=80, num_updates=2510, lr=9.88634e-06, gnorm=0.298, clip=0, loss_scale=256, train_wall=98, gb_free=6.7, wall=45056
2022-06-28 15:34:21 - progress_bar.py[line:274] - INFO: epoch 001:   2523 / 7081 loss=0.002, score=1.209, ntokens=1262.6, nsentences=80, sample_size=1262.6, wps=128.9, ups=0.1, wpb=1262.6, bsz=80, num_updates=2520, lr=9.88339e-06, gnorm=0.459, clip=10, loss_scale=256, train_wall=98, gb_free=6.7, wall=45154
2022-06-28 15:36:00 - progress_bar.py[line:274] - INFO: epoch 001:   2533 / 7081 loss=0, score=1.25, ntokens=1248.9, nsentences=80, sample_size=1248.9, wps=126.2, ups=0.1, wpb=1248.9, bsz=80, num_updates=2530, lr=9.88045e-06, gnorm=0.238, clip=0, loss_scale=256, train_wall=99, gb_free=6.7, wall=45253
2022-06-28 15:37:39 - progress_bar.py[line:274] - INFO: epoch 001:   2543 / 7081 loss=0.001, score=1.191, ntokens=1269.8, nsentences=80, sample_size=1269.8, wps=128.1, ups=0.1, wpb=1269.8, bsz=80, num_updates=2540, lr=9.8775e-06, gnorm=0.374, clip=0, loss_scale=256, train_wall=99, gb_free=6.7, wall=45352
2022-06-28 15:39:18 - progress_bar.py[line:274] - INFO: epoch 001:   2553 / 7081 loss=0.001, score=1.4, ntokens=1276.7, nsentences=80, sample_size=1276.7, wps=128.4, ups=0.1, wpb=1276.7, bsz=80, num_updates=2550, lr=9.87456e-06, gnorm=0.53, clip=10, loss_scale=256, train_wall=99, gb_free=6.7, wall=45452
2022-06-28 15:40:58 - progress_bar.py[line:274] - INFO: epoch 001:   2563 / 7081 loss=0, score=1.096, ntokens=1280.6, nsentences=80, sample_size=1280.6, wps=128.3, ups=0.1, wpb=1280.6, bsz=80, num_updates=2560, lr=9.87161e-06, gnorm=0.305, clip=0, loss_scale=256, train_wall=100, gb_free=6.7, wall=45552
2022-06-28 15:42:38 - progress_bar.py[line:274] - INFO: epoch 001:   2573 / 7081 loss=0.001, score=1.209, ntokens=1294.6, nsentences=80, sample_size=1294.6, wps=129.5, ups=0.1, wpb=1294.6, bsz=80, num_updates=2570, lr=9.86867e-06, gnorm=0.309, clip=0, loss_scale=256, train_wall=100, gb_free=6.7, wall=45652
2022-06-28 15:43:36 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-06-28 15:44:27 - progress_bar.py[line:274] - INFO: epoch 001:   2584 / 7081 loss=-0.002, score=1.208, ntokens=1279.8, nsentences=80, sample_size=1279.8, wps=117.5, ups=0.09, wpb=1279.8, bsz=80, num_updates=2580, lr=9.86573e-06, gnorm=0.435, clip=10, loss_scale=128, train_wall=109, gb_free=6.7, wall=45761
2022-06-28 15:46:07 - progress_bar.py[line:274] - INFO: epoch 001:   2594 / 7081 loss=-0.001, score=1.357, ntokens=1281.8, nsentences=80, sample_size=1281.8, wps=128.8, ups=0.1, wpb=1281.8, bsz=80, num_updates=2590, lr=9.86278e-06, gnorm=0.235, clip=0, loss_scale=128, train_wall=99, gb_free=6.7, wall=45860
2022-06-28 15:47:46 - progress_bar.py[line:274] - INFO: epoch 001:   2604 / 7081 loss=-0, score=1.299, ntokens=1315.1, nsentences=80, sample_size=1315.1, wps=132.5, ups=0.1, wpb=1315.1, bsz=80, num_updates=2600, lr=9.85984e-06, gnorm=0.424, clip=0, loss_scale=128, train_wall=99, gb_free=6.7, wall=45959
2022-06-28 15:49:26 - progress_bar.py[line:274] - INFO: epoch 001:   2614 / 7081 loss=-0.001, score=1.262, ntokens=1312.1, nsentences=80, sample_size=1312.1, wps=131.3, ups=0.1, wpb=1312.1, bsz=80, num_updates=2610, lr=9.85689e-06, gnorm=0.334, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=46059
2022-06-28 15:51:06 - progress_bar.py[line:274] - INFO: epoch 001:   2624 / 7081 loss=-0.001, score=1.239, ntokens=1302, nsentences=80, sample_size=1302, wps=130.5, ups=0.1, wpb=1302, bsz=80, num_updates=2620, lr=9.85395e-06, gnorm=0.419, clip=10, loss_scale=128, train_wall=100, gb_free=6.7, wall=46159
2022-06-28 15:52:44 - progress_bar.py[line:274] - INFO: epoch 001:   2634 / 7081 loss=0.001, score=1.374, ntokens=1265.3, nsentences=80, sample_size=1265.3, wps=128, ups=0.1, wpb=1265.3, bsz=80, num_updates=2630, lr=9.851e-06, gnorm=0.784, clip=20, loss_scale=128, train_wall=99, gb_free=6.7, wall=46258
2022-06-28 15:54:23 - progress_bar.py[line:274] - INFO: epoch 001:   2644 / 7081 loss=-0.001, score=1.205, ntokens=1205.4, nsentences=80, sample_size=1205.4, wps=122.3, ups=0.1, wpb=1205.4, bsz=80, num_updates=2640, lr=9.84806e-06, gnorm=0.486, clip=10, loss_scale=128, train_wall=98, gb_free=6.7, wall=46357
2022-06-28 15:56:00 - progress_bar.py[line:274] - INFO: epoch 001:   2654 / 7081 loss=0, score=1.274, ntokens=1154.7, nsentences=80, sample_size=1154.7, wps=119.5, ups=0.1, wpb=1154.7, bsz=80, num_updates=2650, lr=9.84511e-06, gnorm=0.445, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=46453
2022-06-28 15:57:37 - progress_bar.py[line:274] - INFO: epoch 001:   2664 / 7081 loss=-0.001, score=1.192, ntokens=1170.2, nsentences=80, sample_size=1170.2, wps=119.9, ups=0.1, wpb=1170.2, bsz=80, num_updates=2660, lr=9.84217e-06, gnorm=0.285, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=46551
2022-06-28 15:59:14 - progress_bar.py[line:274] - INFO: epoch 001:   2674 / 7081 loss=-0, score=1.142, ntokens=1179.4, nsentences=80, sample_size=1179.4, wps=122.2, ups=0.1, wpb=1179.4, bsz=80, num_updates=2670, lr=9.83922e-06, gnorm=0.571, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=46647
2022-06-28 16:00:51 - progress_bar.py[line:274] - INFO: epoch 001:   2684 / 7081 loss=0, score=1.247, ntokens=1225.5, nsentences=80, sample_size=1225.5, wps=126.3, ups=0.1, wpb=1225.5, bsz=80, num_updates=2680, lr=9.83628e-06, gnorm=0.316, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=46744
2022-06-28 16:02:27 - progress_bar.py[line:274] - INFO: epoch 001:   2694 / 7081 loss=-0.001, score=1.245, ntokens=1214.5, nsentences=80, sample_size=1214.5, wps=126.3, ups=0.1, wpb=1214.5, bsz=80, num_updates=2690, lr=9.83333e-06, gnorm=0.514, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=46841
2022-06-28 16:04:03 - progress_bar.py[line:274] - INFO: epoch 001:   2704 / 7081 loss=0.001, score=1.207, ntokens=1211.7, nsentences=80, sample_size=1211.7, wps=125.9, ups=0.1, wpb=1211.7, bsz=80, num_updates=2700, lr=9.83039e-06, gnorm=0.689, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=46937
2022-06-28 16:05:42 - progress_bar.py[line:274] - INFO: epoch 001:   2714 / 7081 loss=0.001, score=1.252, ntokens=1218.7, nsentences=80, sample_size=1218.7, wps=123.7, ups=0.1, wpb=1218.7, bsz=80, num_updates=2710, lr=9.82745e-06, gnorm=0.896, clip=20, loss_scale=128, train_wall=98, gb_free=6.7, wall=47035
2022-06-28 16:07:25 - progress_bar.py[line:274] - INFO: epoch 001:   2724 / 7081 loss=-0.001, score=1.102, ntokens=1197, nsentences=80, sample_size=1197, wps=115.5, ups=0.1, wpb=1197, bsz=80, num_updates=2720, lr=9.8245e-06, gnorm=0.615, clip=20, loss_scale=128, train_wall=104, gb_free=6.7, wall=47139
2022-06-28 16:09:09 - progress_bar.py[line:274] - INFO: epoch 001:   2734 / 7081 loss=0.001, score=1.372, ntokens=1193.2, nsentences=80, sample_size=1193.2, wps=115.7, ups=0.1, wpb=1193.2, bsz=80, num_updates=2730, lr=9.82156e-06, gnorm=0.454, clip=10, loss_scale=128, train_wall=103, gb_free=6.7, wall=47242
2022-06-28 16:10:47 - progress_bar.py[line:274] - INFO: epoch 001:   2744 / 7081 loss=-0.001, score=1.23, ntokens=1191.7, nsentences=80, sample_size=1191.7, wps=120.4, ups=0.1, wpb=1191.7, bsz=80, num_updates=2740, lr=9.81861e-06, gnorm=0.699, clip=20, loss_scale=128, train_wall=99, gb_free=6.7, wall=47341
2022-06-28 16:12:27 - progress_bar.py[line:274] - INFO: epoch 001:   2754 / 7081 loss=0.001, score=1.162, ntokens=1246.8, nsentences=80, sample_size=1246.8, wps=124.9, ups=0.1, wpb=1246.8, bsz=80, num_updates=2750, lr=9.81567e-06, gnorm=0.445, clip=10, loss_scale=128, train_wall=100, gb_free=6.7, wall=47441
2022-06-28 16:14:07 - progress_bar.py[line:274] - INFO: epoch 001:   2764 / 7081 loss=0.002, score=1.182, ntokens=1263.6, nsentences=80, sample_size=1263.6, wps=126.8, ups=0.1, wpb=1263.6, bsz=80, num_updates=2760, lr=9.81272e-06, gnorm=0.477, clip=0, loss_scale=128, train_wall=100, gb_free=6.7, wall=47540
2022-06-28 16:15:46 - progress_bar.py[line:274] - INFO: epoch 001:   2774 / 7081 loss=-0, score=1.303, ntokens=1252.6, nsentences=80, sample_size=1252.6, wps=125.9, ups=0.1, wpb=1252.6, bsz=80, num_updates=2770, lr=9.80978e-06, gnorm=0.699, clip=20, loss_scale=128, train_wall=99, gb_free=6.7, wall=47640
2022-06-28 16:17:25 - progress_bar.py[line:274] - INFO: epoch 001:   2784 / 7081 loss=-0.001, score=1.191, ntokens=1263.9, nsentences=80, sample_size=1263.9, wps=127.7, ups=0.1, wpb=1263.9, bsz=80, num_updates=2780, lr=9.80683e-06, gnorm=0.659, clip=30, loss_scale=128, train_wall=99, gb_free=6.7, wall=47739
2022-06-28 16:19:04 - progress_bar.py[line:274] - INFO: epoch 001:   2794 / 7081 loss=-0.002, score=1.294, ntokens=1243, nsentences=80, sample_size=1243, wps=126.1, ups=0.1, wpb=1243, bsz=80, num_updates=2790, lr=9.80389e-06, gnorm=0.664, clip=20, loss_scale=128, train_wall=98, gb_free=6.7, wall=47838
2022-06-28 16:20:41 - progress_bar.py[line:274] - INFO: epoch 001:   2804 / 7081 loss=-0.001, score=1.18, ntokens=1212.9, nsentences=80, sample_size=1212.9, wps=125.1, ups=0.1, wpb=1212.9, bsz=80, num_updates=2800, lr=9.80094e-06, gnorm=0.421, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=47934
2022-06-28 16:22:18 - progress_bar.py[line:274] - INFO: epoch 001:   2814 / 7081 loss=0.002, score=1.32, ntokens=1217.8, nsentences=80, sample_size=1217.8, wps=124.9, ups=0.1, wpb=1217.8, bsz=80, num_updates=2810, lr=9.798e-06, gnorm=0.358, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=48032
2022-06-28 16:23:57 - progress_bar.py[line:274] - INFO: epoch 001:   2824 / 7081 loss=0.001, score=1.324, ntokens=1223.7, nsentences=80, sample_size=1223.7, wps=124.6, ups=0.1, wpb=1223.7, bsz=80, num_updates=2820, lr=9.79505e-06, gnorm=0.384, clip=10, loss_scale=128, train_wall=98, gb_free=6.7, wall=48130
2022-06-28 16:25:34 - progress_bar.py[line:274] - INFO: epoch 001:   2834 / 7081 loss=-0.001, score=1.141, ntokens=1229.8, nsentences=80, sample_size=1229.8, wps=126.4, ups=0.1, wpb=1229.8, bsz=80, num_updates=2830, lr=9.79211e-06, gnorm=0.411, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=48227
2022-06-28 16:27:12 - progress_bar.py[line:274] - INFO: epoch 001:   2844 / 7081 loss=0, score=1.249, ntokens=1233.1, nsentences=80, sample_size=1233.1, wps=125.5, ups=0.1, wpb=1233.1, bsz=80, num_updates=2840, lr=9.78916e-06, gnorm=0.719, clip=20, loss_scale=128, train_wall=98, gb_free=6.7, wall=48326
2022-06-28 16:28:52 - progress_bar.py[line:274] - INFO: epoch 001:   2854 / 7081 loss=-0, score=1.259, ntokens=1265.4, nsentences=80, sample_size=1265.4, wps=126.8, ups=0.1, wpb=1265.4, bsz=80, num_updates=2850, lr=9.78622e-06, gnorm=0.504, clip=10, loss_scale=128, train_wall=100, gb_free=6.7, wall=48425
2022-06-28 16:30:31 - progress_bar.py[line:274] - INFO: epoch 001:   2864 / 7081 loss=-0.001, score=1.221, ntokens=1236.6, nsentences=80, sample_size=1236.6, wps=124.7, ups=0.1, wpb=1236.6, bsz=80, num_updates=2860, lr=9.78328e-06, gnorm=0.31, clip=0, loss_scale=128, train_wall=99, gb_free=6.7, wall=48525
2022-06-28 16:32:10 - progress_bar.py[line:274] - INFO: epoch 001:   2874 / 7081 loss=0.001, score=1.164, ntokens=1229.1, nsentences=80, sample_size=1229.1, wps=124.8, ups=0.1, wpb=1229.1, bsz=80, num_updates=2870, lr=9.78033e-06, gnorm=0.337, clip=0, loss_scale=128, train_wall=98, gb_free=6.7, wall=48623
2022-06-28 16:33:55 - progress_bar.py[line:274] - INFO: epoch 001:   2884 / 7081 loss=0.001, score=1.219, ntokens=1239.2, nsentences=80, sample_size=1239.2, wps=117.8, ups=0.1, wpb=1239.2, bsz=80, num_updates=2880, lr=9.77739e-06, gnorm=0.735, clip=20, loss_scale=128, train_wall=105, gb_free=6.7, wall=48728
2022-06-28 16:35:57 - progress_bar.py[line:274] - INFO: epoch 001:   2894 / 7081 loss=0, score=1.294, ntokens=1260.5, nsentences=80, sample_size=1260.5, wps=103.5, ups=0.08, wpb=1260.5, bsz=80, num_updates=2890, lr=9.77444e-06, gnorm=0.617, clip=10, loss_scale=128, train_wall=122, gb_free=6.7, wall=48850
2022-06-28 16:36:16 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-28 16:37:55 - progress_bar.py[line:274] - INFO: epoch 001:   2905 / 7081 loss=0.001, score=1.228, ntokens=1252.2, nsentences=80, sample_size=1252.2, wps=106, ups=0.08, wpb=1252.2, bsz=80, num_updates=2900, lr=9.7715e-06, gnorm=0.335, clip=0, loss_scale=64, train_wall=118, gb_free=6.7, wall=48968
2022-06-28 16:39:45 - progress_bar.py[line:274] - INFO: epoch 001:   2915 / 7081 loss=-0, score=1.296, ntokens=1271.2, nsentences=80, sample_size=1271.2, wps=115.5, ups=0.09, wpb=1271.2, bsz=80, num_updates=2910, lr=9.76855e-06, gnorm=0.233, clip=0, loss_scale=64, train_wall=110, gb_free=6.7, wall=49078
2022-06-28 16:41:33 - progress_bar.py[line:274] - INFO: epoch 001:   2925 / 7081 loss=0, score=1.221, ntokens=1272, nsentences=80, sample_size=1272, wps=117.7, ups=0.09, wpb=1272, bsz=80, num_updates=2920, lr=9.76561e-06, gnorm=0.346, clip=10, loss_scale=64, train_wall=108, gb_free=6.7, wall=49186
2022-06-28 16:43:15 - progress_bar.py[line:274] - INFO: epoch 001:   2935 / 7081 loss=-0.001, score=1.211, ntokens=1247.4, nsentences=80, sample_size=1247.4, wps=122.6, ups=0.1, wpb=1247.4, bsz=80, num_updates=2930, lr=9.76266e-06, gnorm=0.687, clip=10, loss_scale=64, train_wall=102, gb_free=6.7, wall=49288
2022-06-28 16:44:59 - progress_bar.py[line:274] - INFO: epoch 001:   2945 / 7081 loss=0, score=1.201, ntokens=1221.8, nsentences=80, sample_size=1221.8, wps=116.9, ups=0.1, wpb=1221.8, bsz=80, num_updates=2940, lr=9.75972e-06, gnorm=0.547, clip=20, loss_scale=64, train_wall=104, gb_free=6.7, wall=49393
2022-06-28 16:46:46 - progress_bar.py[line:274] - INFO: epoch 001:   2955 / 7081 loss=0.001, score=1.186, ntokens=1235.2, nsentences=80, sample_size=1235.2, wps=115.2, ups=0.09, wpb=1235.2, bsz=80, num_updates=2950, lr=9.75677e-06, gnorm=0.51, clip=20, loss_scale=64, train_wall=107, gb_free=6.7, wall=49500
2022-06-28 16:48:31 - progress_bar.py[line:274] - INFO: epoch 001:   2965 / 7081 loss=-0, score=1.239, ntokens=1260.1, nsentences=80, sample_size=1260.1, wps=120.8, ups=0.1, wpb=1260.1, bsz=80, num_updates=2960, lr=9.75383e-06, gnorm=0.212, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=49604
2022-06-28 16:50:11 - progress_bar.py[line:274] - INFO: epoch 001:   2975 / 7081 loss=0, score=1.18, ntokens=1271.5, nsentences=80, sample_size=1271.5, wps=126.9, ups=0.1, wpb=1271.5, bsz=80, num_updates=2970, lr=9.75088e-06, gnorm=0.34, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=49704
2022-06-28 16:51:51 - progress_bar.py[line:274] - INFO: epoch 001:   2985 / 7081 loss=-0, score=1.231, ntokens=1286.2, nsentences=80, sample_size=1286.2, wps=128.9, ups=0.1, wpb=1286.2, bsz=80, num_updates=2980, lr=9.74794e-06, gnorm=0.511, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=49804
2022-06-28 16:53:30 - progress_bar.py[line:274] - INFO: epoch 001:   2995 / 7081 loss=0.001, score=1.166, ntokens=1282.1, nsentences=80, sample_size=1282.1, wps=129.5, ups=0.1, wpb=1282.1, bsz=80, num_updates=2990, lr=9.745e-06, gnorm=0.379, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=49903
2022-06-28 16:55:09 - progress_bar.py[line:274] - INFO: epoch 001:   3005 / 7081 loss=-0, score=1.278, ntokens=1293.9, nsentences=80, sample_size=1293.9, wps=130.6, ups=0.1, wpb=1293.9, bsz=80, num_updates=3000, lr=9.74205e-06, gnorm=0.338, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=50002
slice_id 1 seek offset 2500
2022-06-28 16:55:09 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-28 18:00:39 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0 | score 1.145 | ntokens 162.289 | nsentences 10 | sample_size 162.289 | cider 1.166 | wps 103.2 | wpb 162.3 | bsz 10 | num_updates 3000 | best_cider 1.17
2022-06-28 18:00:39 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 3000 updates
2022-06-28 18:00:39 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_3000.pt
2022-06-28 18:00:53 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_3000.pt
2022-06-28 18:01:48 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_3000.pt (epoch 1 @ 3000 updates, score 1.166) (writing took 69.11174460873008 seconds)
2022-06-28 18:03:26 - progress_bar.py[line:274] - INFO: epoch 001:   3015 / 7081 loss=-0, score=1.136, ntokens=1303.3, nsentences=80, sample_size=1303.3, wps=3.2, ups=0, wpb=1303.3, bsz=80, num_updates=3010, lr=9.73911e-06, gnorm=0.32, clip=0, loss_scale=64, train_wall=98, gb_free=6.7, wall=54100
2022-06-28 18:05:04 - progress_bar.py[line:274] - INFO: epoch 001:   3025 / 7081 loss=0.001, score=1.33, ntokens=1301.9, nsentences=80, sample_size=1301.9, wps=133, ups=0.1, wpb=1301.9, bsz=80, num_updates=3020, lr=9.73616e-06, gnorm=0.267, clip=0, loss_scale=64, train_wall=98, gb_free=6.7, wall=54198
2022-06-28 18:06:43 - progress_bar.py[line:274] - INFO: epoch 001:   3035 / 7081 loss=-0, score=1.201, ntokens=1317.7, nsentences=80, sample_size=1317.7, wps=132.5, ups=0.1, wpb=1317.7, bsz=80, num_updates=3030, lr=9.73322e-06, gnorm=0.281, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=54297
2022-06-28 18:08:23 - progress_bar.py[line:274] - INFO: epoch 001:   3045 / 7081 loss=0, score=1.2, ntokens=1315.2, nsentences=80, sample_size=1315.2, wps=132.1, ups=0.1, wpb=1315.2, bsz=80, num_updates=3040, lr=9.73027e-06, gnorm=0.251, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=54397
2022-06-28 18:10:01 - progress_bar.py[line:274] - INFO: epoch 001:   3055 / 7081 loss=-0.001, score=1.248, ntokens=1322.2, nsentences=80, sample_size=1322.2, wps=134.5, ups=0.1, wpb=1322.2, bsz=80, num_updates=3050, lr=9.72733e-06, gnorm=0.456, clip=10, loss_scale=64, train_wall=98, gb_free=6.7, wall=54495
2022-06-28 18:11:41 - progress_bar.py[line:274] - INFO: epoch 001:   3065 / 7081 loss=0, score=1.188, ntokens=1321.6, nsentences=80, sample_size=1321.6, wps=132.4, ups=0.1, wpb=1321.6, bsz=80, num_updates=3060, lr=9.72438e-06, gnorm=0.313, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=54595
2022-06-28 18:13:22 - progress_bar.py[line:274] - INFO: epoch 001:   3075 / 7081 loss=-0, score=1.238, ntokens=1336.3, nsentences=80, sample_size=1336.3, wps=132.9, ups=0.1, wpb=1336.3, bsz=80, num_updates=3070, lr=9.72144e-06, gnorm=0.263, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=54695
2022-06-28 18:15:09 - progress_bar.py[line:274] - INFO: epoch 001:   3085 / 7081 loss=0.001, score=1.24, ntokens=1341, nsentences=80, sample_size=1341, wps=125.2, ups=0.09, wpb=1341, bsz=80, num_updates=3080, lr=9.71849e-06, gnorm=0.505, clip=20, loss_scale=64, train_wall=107, gb_free=6.7, wall=54802
2022-06-28 18:16:49 - progress_bar.py[line:274] - INFO: epoch 001:   3095 / 7081 loss=0.001, score=1.16, ntokens=1337.9, nsentences=80, sample_size=1337.9, wps=133.3, ups=0.1, wpb=1337.9, bsz=80, num_updates=3090, lr=9.71555e-06, gnorm=0.447, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=54903
2022-06-28 18:18:39 - progress_bar.py[line:274] - INFO: epoch 001:   3105 / 7081 loss=-0, score=1.32, ntokens=1341.7, nsentences=80, sample_size=1341.7, wps=121.9, ups=0.09, wpb=1341.7, bsz=80, num_updates=3100, lr=9.7126e-06, gnorm=0.304, clip=0, loss_scale=64, train_wall=110, gb_free=6.7, wall=55013
2022-06-28 18:20:29 - progress_bar.py[line:274] - INFO: epoch 001:   3115 / 7081 loss=0.001, score=1.143, ntokens=1344.9, nsentences=80, sample_size=1344.9, wps=122.7, ups=0.09, wpb=1344.9, bsz=80, num_updates=3110, lr=9.70966e-06, gnorm=0.354, clip=10, loss_scale=64, train_wall=109, gb_free=6.7, wall=55122
2022-06-28 18:22:25 - progress_bar.py[line:274] - INFO: epoch 001:   3125 / 7081 loss=-0.001, score=1.206, ntokens=1350.6, nsentences=80, sample_size=1350.6, wps=116.5, ups=0.09, wpb=1350.6, bsz=80, num_updates=3120, lr=9.70672e-06, gnorm=0.322, clip=0, loss_scale=64, train_wall=116, gb_free=6.7, wall=55238
2022-06-28 18:24:12 - progress_bar.py[line:274] - INFO: epoch 001:   3135 / 7081 loss=-0.002, score=1.124, ntokens=1351.1, nsentences=80, sample_size=1351.1, wps=126.2, ups=0.09, wpb=1351.1, bsz=80, num_updates=3130, lr=9.70377e-06, gnorm=0.413, clip=10, loss_scale=64, train_wall=107, gb_free=6.7, wall=55345
2022-06-28 18:25:52 - progress_bar.py[line:274] - INFO: epoch 001:   3145 / 7081 loss=-0.001, score=1.286, ntokens=1340.7, nsentences=80, sample_size=1340.7, wps=134, ups=0.1, wpb=1340.7, bsz=80, num_updates=3140, lr=9.70083e-06, gnorm=0.425, clip=20, loss_scale=64, train_wall=100, gb_free=6.7, wall=55445
2022-06-28 18:27:34 - progress_bar.py[line:274] - INFO: epoch 001:   3155 / 7081 loss=0, score=1.214, ntokens=1337.5, nsentences=80, sample_size=1337.5, wps=131.1, ups=0.1, wpb=1337.5, bsz=80, num_updates=3150, lr=9.69788e-06, gnorm=0.351, clip=0, loss_scale=64, train_wall=102, gb_free=6.7, wall=55547
2022-06-28 18:29:13 - progress_bar.py[line:274] - INFO: epoch 001:   3165 / 7081 loss=0, score=1.284, ntokens=1336.1, nsentences=80, sample_size=1336.1, wps=134.4, ups=0.1, wpb=1336.1, bsz=80, num_updates=3160, lr=9.69494e-06, gnorm=0.433, clip=10, loss_scale=64, train_wall=99, gb_free=6.7, wall=55647
2022-06-28 18:30:51 - progress_bar.py[line:274] - INFO: epoch 001:   3175 / 7081 loss=-0.001, score=1.255, ntokens=1325, nsentences=80, sample_size=1325, wps=136, ups=0.1, wpb=1325, bsz=80, num_updates=3170, lr=9.69199e-06, gnorm=0.188, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=55744
2022-06-28 18:32:29 - progress_bar.py[line:274] - INFO: epoch 001:   3185 / 7081 loss=-0.001, score=1.274, ntokens=1330.9, nsentences=80, sample_size=1330.9, wps=136.1, ups=0.1, wpb=1330.9, bsz=80, num_updates=3180, lr=9.68905e-06, gnorm=0.441, clip=10, loss_scale=64, train_wall=98, gb_free=6.7, wall=55842
2022-06-28 18:34:09 - progress_bar.py[line:274] - INFO: epoch 001:   3195 / 7081 loss=-0.002, score=1.221, ntokens=1320, nsentences=80, sample_size=1320, wps=132.1, ups=0.1, wpb=1320, bsz=80, num_updates=3190, lr=9.6861e-06, gnorm=0.498, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=55942
2022-06-28 18:35:46 - progress_bar.py[line:274] - INFO: epoch 001:   3205 / 7081 loss=0, score=1.289, ntokens=1322.7, nsentences=80, sample_size=1322.7, wps=135.8, ups=0.1, wpb=1322.7, bsz=80, num_updates=3200, lr=9.68316e-06, gnorm=0.495, clip=10, loss_scale=64, train_wall=97, gb_free=6.7, wall=56039
2022-06-28 18:37:22 - progress_bar.py[line:274] - INFO: epoch 001:   3215 / 7081 loss=-0.001, score=1.189, ntokens=1312.9, nsentences=80, sample_size=1312.9, wps=136.5, ups=0.1, wpb=1312.9, bsz=80, num_updates=3210, lr=9.68021e-06, gnorm=0.266, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=56136
2022-06-28 18:39:03 - progress_bar.py[line:274] - INFO: epoch 001:   3225 / 7081 loss=-0, score=1.28, ntokens=1321, nsentences=80, sample_size=1321, wps=131.3, ups=0.1, wpb=1321, bsz=80, num_updates=3220, lr=9.67727e-06, gnorm=0.402, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=56236
2022-06-28 18:40:41 - progress_bar.py[line:274] - INFO: epoch 001:   3235 / 7081 loss=-0.001, score=1.349, ntokens=1313.9, nsentences=80, sample_size=1313.9, wps=133, ups=0.1, wpb=1313.9, bsz=80, num_updates=3230, lr=9.67432e-06, gnorm=0.557, clip=20, loss_scale=64, train_wall=99, gb_free=6.7, wall=56335
2022-06-28 18:42:19 - progress_bar.py[line:274] - INFO: epoch 001:   3245 / 7081 loss=-0, score=1.173, ntokens=1335.7, nsentences=80, sample_size=1335.7, wps=136.8, ups=0.1, wpb=1335.7, bsz=80, num_updates=3240, lr=9.67138e-06, gnorm=0.232, clip=0, loss_scale=64, train_wall=98, gb_free=6.7, wall=56433
2022-06-28 18:43:58 - progress_bar.py[line:274] - INFO: epoch 001:   3255 / 7081 loss=0.001, score=1.309, ntokens=1336.4, nsentences=80, sample_size=1336.4, wps=135.3, ups=0.1, wpb=1336.4, bsz=80, num_updates=3250, lr=9.66844e-06, gnorm=0.345, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=56531
2022-06-28 18:45:37 - progress_bar.py[line:274] - INFO: epoch 001:   3265 / 7081 loss=-0, score=1.222, ntokens=1343.9, nsentences=80, sample_size=1343.9, wps=136.3, ups=0.1, wpb=1343.9, bsz=80, num_updates=3260, lr=9.66549e-06, gnorm=0.936, clip=20, loss_scale=64, train_wall=98, gb_free=6.7, wall=56630
2022-06-28 18:47:15 - progress_bar.py[line:274] - INFO: epoch 001:   3275 / 7081 loss=0.001, score=1.254, ntokens=1344.2, nsentences=80, sample_size=1344.2, wps=136.1, ups=0.1, wpb=1344.2, bsz=80, num_updates=3270, lr=9.66255e-06, gnorm=0.29, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=56729
2022-06-28 18:48:58 - progress_bar.py[line:274] - INFO: epoch 001:   3285 / 7081 loss=0.001, score=1.244, ntokens=1347.2, nsentences=80, sample_size=1347.2, wps=130.6, ups=0.1, wpb=1347.2, bsz=80, num_updates=3280, lr=9.6596e-06, gnorm=0.323, clip=10, loss_scale=64, train_wall=103, gb_free=6.7, wall=56832
2022-06-28 18:50:36 - progress_bar.py[line:274] - INFO: epoch 001:   3295 / 7081 loss=-0, score=1.236, ntokens=1346.1, nsentences=80, sample_size=1346.1, wps=137.6, ups=0.1, wpb=1346.1, bsz=80, num_updates=3290, lr=9.65666e-06, gnorm=0.357, clip=10, loss_scale=64, train_wall=98, gb_free=6.7, wall=56930
2022-06-28 18:52:13 - progress_bar.py[line:274] - INFO: epoch 001:   3305 / 7081 loss=-0, score=1.211, ntokens=1345.9, nsentences=80, sample_size=1345.9, wps=138.9, ups=0.1, wpb=1345.9, bsz=80, num_updates=3300, lr=9.65371e-06, gnorm=0.451, clip=10, loss_scale=64, train_wall=97, gb_free=6.7, wall=57027
2022-06-28 18:53:50 - progress_bar.py[line:274] - INFO: epoch 001:   3315 / 7081 loss=-0.002, score=1.259, ntokens=1346.2, nsentences=80, sample_size=1346.2, wps=139.1, ups=0.1, wpb=1346.2, bsz=80, num_updates=3310, lr=9.65077e-06, gnorm=0.555, clip=10, loss_scale=64, train_wall=97, gb_free=6.7, wall=57123
2022-06-28 18:55:26 - progress_bar.py[line:274] - INFO: epoch 001:   3325 / 7081 loss=-0.001, score=1.196, ntokens=1337.5, nsentences=80, sample_size=1337.5, wps=139, ups=0.1, wpb=1337.5, bsz=80, num_updates=3320, lr=9.64782e-06, gnorm=0.388, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=57220
2022-06-28 18:57:13 - progress_bar.py[line:274] - INFO: epoch 001:   3335 / 7081 loss=0.001, score=1.142, ntokens=1336.6, nsentences=80, sample_size=1336.6, wps=125.4, ups=0.09, wpb=1336.6, bsz=80, num_updates=3330, lr=9.64488e-06, gnorm=0.344, clip=0, loss_scale=64, train_wall=106, gb_free=6.7, wall=57326
2022-06-28 18:59:01 - progress_bar.py[line:274] - INFO: epoch 001:   3345 / 7081 loss=0, score=1.262, ntokens=1339.5, nsentences=80, sample_size=1339.5, wps=124.2, ups=0.09, wpb=1339.5, bsz=80, num_updates=3340, lr=9.64193e-06, gnorm=0.367, clip=10, loss_scale=64, train_wall=108, gb_free=6.7, wall=57434
2022-06-28 19:00:37 - progress_bar.py[line:274] - INFO: epoch 001:   3355 / 7081 loss=-0, score=1.376, ntokens=1340.5, nsentences=80, sample_size=1340.5, wps=138.4, ups=0.1, wpb=1340.5, bsz=80, num_updates=3350, lr=9.63899e-06, gnorm=0.271, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=57531
2022-06-28 19:02:16 - progress_bar.py[line:274] - INFO: epoch 001:   3365 / 7081 loss=0, score=1.3, ntokens=1338.4, nsentences=80, sample_size=1338.4, wps=136.2, ups=0.1, wpb=1338.4, bsz=80, num_updates=3360, lr=9.63604e-06, gnorm=0.345, clip=0, loss_scale=64, train_wall=98, gb_free=6.7, wall=57629
2022-06-28 19:03:54 - progress_bar.py[line:274] - INFO: epoch 001:   3375 / 7081 loss=-0, score=1.197, ntokens=1341.8, nsentences=80, sample_size=1341.8, wps=136, ups=0.1, wpb=1341.8, bsz=80, num_updates=3370, lr=9.6331e-06, gnorm=0.26, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=57728
2022-06-28 19:05:34 - progress_bar.py[line:274] - INFO: epoch 001:   3385 / 7081 loss=-0, score=1.165, ntokens=1332.8, nsentences=80, sample_size=1332.8, wps=134.3, ups=0.1, wpb=1332.8, bsz=80, num_updates=3380, lr=9.63016e-06, gnorm=0.498, clip=10, loss_scale=64, train_wall=99, gb_free=6.7, wall=57827
2022-06-28 19:07:13 - progress_bar.py[line:274] - INFO: epoch 001:   3395 / 7081 loss=-0, score=1.378, ntokens=1338.9, nsentences=80, sample_size=1338.9, wps=135.4, ups=0.1, wpb=1338.9, bsz=80, num_updates=3390, lr=9.62721e-06, gnorm=0.424, clip=0, loss_scale=64, train_wall=99, gb_free=6.7, wall=57926
2022-06-28 19:08:50 - progress_bar.py[line:274] - INFO: epoch 001:   3405 / 7081 loss=-0.002, score=1.194, ntokens=1327.6, nsentences=80, sample_size=1327.6, wps=135.6, ups=0.1, wpb=1327.6, bsz=80, num_updates=3400, lr=9.62427e-06, gnorm=0.338, clip=0, loss_scale=64, train_wall=98, gb_free=6.7, wall=58024
2022-06-28 19:10:28 - progress_bar.py[line:274] - INFO: epoch 001:   3415 / 7081 loss=0, score=1.237, ntokens=1325.5, nsentences=80, sample_size=1325.5, wps=136.5, ups=0.1, wpb=1325.5, bsz=80, num_updates=3410, lr=9.62132e-06, gnorm=0.428, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=58121
2022-06-28 19:12:04 - progress_bar.py[line:274] - INFO: epoch 001:   3425 / 7081 loss=-0.001, score=1.208, ntokens=1332.6, nsentences=80, sample_size=1332.6, wps=138.3, ups=0.1, wpb=1332.6, bsz=80, num_updates=3420, lr=9.61838e-06, gnorm=0.388, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=58217
2022-06-28 19:13:40 - progress_bar.py[line:274] - INFO: epoch 001:   3435 / 7081 loss=-0.001, score=1.169, ntokens=1319.9, nsentences=80, sample_size=1319.9, wps=137.8, ups=0.1, wpb=1319.9, bsz=80, num_updates=3430, lr=9.61543e-06, gnorm=0.298, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=58313
2022-06-28 19:15:16 - progress_bar.py[line:274] - INFO: epoch 001:   3445 / 7081 loss=-0.001, score=1.277, ntokens=1309.9, nsentences=80, sample_size=1309.9, wps=135.4, ups=0.1, wpb=1309.9, bsz=80, num_updates=3440, lr=9.61249e-06, gnorm=0.65, clip=30, loss_scale=128, train_wall=97, gb_free=6.7, wall=58410
2022-06-28 19:16:53 - progress_bar.py[line:274] - INFO: epoch 001:   3455 / 7081 loss=0, score=1.204, ntokens=1295.3, nsentences=80, sample_size=1295.3, wps=134.8, ups=0.1, wpb=1295.3, bsz=80, num_updates=3450, lr=9.60954e-06, gnorm=0.306, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=58506
2022-06-28 19:18:25 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-28 19:18:44 - progress_bar.py[line:274] - INFO: epoch 001:   3466 / 7081 loss=0.001, score=1.174, ntokens=1281.3, nsentences=80, sample_size=1281.3, wps=115.3, ups=0.09, wpb=1281.3, bsz=80, num_updates=3460, lr=9.6066e-06, gnorm=0.526, clip=20, loss_scale=64, train_wall=111, gb_free=6.7, wall=58617
2022-06-28 19:20:26 - progress_bar.py[line:274] - INFO: epoch 001:   3476 / 7081 loss=0, score=1.31, ntokens=1285.4, nsentences=80, sample_size=1285.4, wps=125, ups=0.1, wpb=1285.4, bsz=80, num_updates=3470, lr=9.60365e-06, gnorm=0.311, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=58720
2022-06-28 19:22:05 - progress_bar.py[line:274] - INFO: epoch 001:   3486 / 7081 loss=0.002, score=1.245, ntokens=1272.4, nsentences=80, sample_size=1272.4, wps=129.3, ups=0.1, wpb=1272.4, bsz=80, num_updates=3480, lr=9.60071e-06, gnorm=0.503, clip=0, loss_scale=64, train_wall=98, gb_free=6.7, wall=58818
2022-06-28 19:23:43 - progress_bar.py[line:274] - INFO: epoch 001:   3496 / 7081 loss=0, score=1.311, ntokens=1248.6, nsentences=80, sample_size=1248.6, wps=127.1, ups=0.1, wpb=1248.6, bsz=80, num_updates=3490, lr=9.59776e-06, gnorm=0.502, clip=10, loss_scale=64, train_wall=98, gb_free=6.7, wall=58917
slice_id 1 seek offset 2500
2022-06-28 19:25:21 - progress_bar.py[line:274] - INFO: epoch 001:   3506 / 7081 loss=-0.001, score=1.207, ntokens=1233.3, nsentences=80, sample_size=1233.3, wps=126.4, ups=0.1, wpb=1233.3, bsz=80, num_updates=3500, lr=9.59482e-06, gnorm=0.505, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=59014
2022-06-28 19:25:21 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 0 seek offset 0
2022-06-28 20:21:55 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0 | score 1.146 | ntokens 152.838 | nsentences 10 | sample_size 152.838 | cider 1.17 | wps 112.5 | wpb 152.8 | bsz 10 | num_updates 3500 | best_cider 1.17
2022-06-28 20:21:55 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 3500 updates
2022-06-28 20:21:55 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_3500.pt
2022-06-28 20:22:05 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_3500.pt
2022-06-28 20:23:58 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_3500.pt (epoch 1 @ 3500 updates, score 1.17) (writing took 122.69657184462994 seconds)
2022-06-28 20:25:33 - progress_bar.py[line:274] - INFO: epoch 001:   3516 / 7081 loss=-0.001, score=1.264, ntokens=1223.9, nsentences=80, sample_size=1223.9, wps=3.4, ups=0, wpb=1223.9, bsz=80, num_updates=3510, lr=9.59188e-06, gnorm=0.367, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=62627
2022-06-28 20:27:09 - progress_bar.py[line:274] - INFO: epoch 001:   3526 / 7081 loss=0, score=1.268, ntokens=1230.5, nsentences=80, sample_size=1230.5, wps=128, ups=0.1, wpb=1230.5, bsz=80, num_updates=3520, lr=9.58893e-06, gnorm=0.341, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=62723
2022-06-28 20:28:45 - progress_bar.py[line:274] - INFO: epoch 001:   3536 / 7081 loss=0, score=1.232, ntokens=1247.6, nsentences=80, sample_size=1247.6, wps=130.1, ups=0.1, wpb=1247.6, bsz=80, num_updates=3530, lr=9.58599e-06, gnorm=0.278, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=62819
2022-06-28 20:30:21 - progress_bar.py[line:274] - INFO: epoch 001:   3546 / 7081 loss=-0.001, score=1.28, ntokens=1211.9, nsentences=80, sample_size=1211.9, wps=127, ups=0.1, wpb=1211.9, bsz=80, num_updates=3540, lr=9.58304e-06, gnorm=1.347, clip=30, loss_scale=64, train_wall=95, gb_free=6.7, wall=62914
2022-06-28 20:31:57 - progress_bar.py[line:274] - INFO: epoch 001:   3556 / 7081 loss=-0, score=1.245, ntokens=1247.5, nsentences=80, sample_size=1247.5, wps=129.9, ups=0.1, wpb=1247.5, bsz=80, num_updates=3550, lr=9.5801e-06, gnorm=0.341, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=63010
2022-06-28 20:33:33 - progress_bar.py[line:274] - INFO: epoch 001:   3566 / 7081 loss=0, score=1.12, ntokens=1280.4, nsentences=80, sample_size=1280.4, wps=132.6, ups=0.1, wpb=1280.4, bsz=80, num_updates=3560, lr=9.57715e-06, gnorm=0.23, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=63107
2022-06-28 20:35:09 - progress_bar.py[line:274] - INFO: epoch 001:   3576 / 7081 loss=-0.001, score=1.189, ntokens=1261.9, nsentences=80, sample_size=1261.9, wps=131.6, ups=0.1, wpb=1261.9, bsz=80, num_updates=3570, lr=9.57421e-06, gnorm=0.209, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=63203
2022-06-28 20:36:45 - progress_bar.py[line:274] - INFO: epoch 001:   3586 / 7081 loss=-0.003, score=1.307, ntokens=1259.8, nsentences=80, sample_size=1259.8, wps=131.5, ups=0.1, wpb=1259.8, bsz=80, num_updates=3580, lr=9.57126e-06, gnorm=0.689, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=63299
2022-06-28 20:38:21 - progress_bar.py[line:274] - INFO: epoch 001:   3596 / 7081 loss=0.001, score=1.249, ntokens=1263.7, nsentences=80, sample_size=1263.7, wps=131.9, ups=0.1, wpb=1263.7, bsz=80, num_updates=3590, lr=9.56832e-06, gnorm=0.629, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=63394
2022-06-28 20:39:57 - progress_bar.py[line:274] - INFO: epoch 001:   3606 / 7081 loss=-0, score=1.31, ntokens=1272.1, nsentences=80, sample_size=1272.1, wps=132.6, ups=0.1, wpb=1272.1, bsz=80, num_updates=3600, lr=9.56537e-06, gnorm=0.208, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=63490
2022-06-28 20:41:33 - progress_bar.py[line:274] - INFO: epoch 001:   3616 / 7081 loss=-0.001, score=1.267, ntokens=1261.3, nsentences=80, sample_size=1261.3, wps=130.9, ups=0.1, wpb=1261.3, bsz=80, num_updates=3610, lr=9.56243e-06, gnorm=0.383, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=63587
2022-06-28 20:43:10 - progress_bar.py[line:274] - INFO: epoch 001:   3626 / 7081 loss=0.001, score=1.248, ntokens=1267.6, nsentences=80, sample_size=1267.6, wps=130.6, ups=0.1, wpb=1267.6, bsz=80, num_updates=3620, lr=9.55948e-06, gnorm=0.202, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=63684
2022-06-28 20:44:46 - progress_bar.py[line:274] - INFO: epoch 001:   3636 / 7081 loss=0.001, score=1.231, ntokens=1265.7, nsentences=80, sample_size=1265.7, wps=131.5, ups=0.1, wpb=1265.7, bsz=80, num_updates=3630, lr=9.55654e-06, gnorm=0.625, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=63780
2022-06-28 20:46:23 - progress_bar.py[line:274] - INFO: epoch 001:   3646 / 7081 loss=-0.001, score=1.376, ntokens=1256.5, nsentences=80, sample_size=1256.5, wps=130.6, ups=0.1, wpb=1256.5, bsz=80, num_updates=3640, lr=9.5536e-06, gnorm=0.606, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=63876
2022-06-28 20:47:59 - progress_bar.py[line:274] - INFO: epoch 001:   3656 / 7081 loss=0.002, score=1.381, ntokens=1276.4, nsentences=80, sample_size=1276.4, wps=132, ups=0.1, wpb=1276.4, bsz=80, num_updates=3650, lr=9.55065e-06, gnorm=0.239, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=63973
2022-06-28 20:49:36 - progress_bar.py[line:274] - INFO: epoch 001:   3666 / 7081 loss=-0.002, score=1.156, ntokens=1290.1, nsentences=80, sample_size=1290.1, wps=133, ups=0.1, wpb=1290.1, bsz=80, num_updates=3660, lr=9.54771e-06, gnorm=0.287, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=64070
2022-06-28 20:51:13 - progress_bar.py[line:274] - INFO: epoch 001:   3676 / 7081 loss=-0.001, score=1.147, ntokens=1298.6, nsentences=80, sample_size=1298.6, wps=134.5, ups=0.1, wpb=1298.6, bsz=80, num_updates=3670, lr=9.54476e-06, gnorm=0.304, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=64166
2022-06-28 20:52:50 - progress_bar.py[line:274] - INFO: epoch 001:   3686 / 7081 loss=-0.001, score=1.142, ntokens=1302.3, nsentences=80, sample_size=1302.3, wps=134.9, ups=0.1, wpb=1302.3, bsz=80, num_updates=3680, lr=9.54182e-06, gnorm=0.469, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=64263
2022-06-28 20:54:26 - progress_bar.py[line:274] - INFO: epoch 001:   3696 / 7081 loss=0.002, score=1.101, ntokens=1275, nsentences=80, sample_size=1275, wps=132.6, ups=0.1, wpb=1275, bsz=80, num_updates=3690, lr=9.53887e-06, gnorm=0.539, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=64359
2022-06-28 20:56:01 - progress_bar.py[line:274] - INFO: epoch 001:   3706 / 7081 loss=0.001, score=1.362, ntokens=1290, nsentences=80, sample_size=1290, wps=134.8, ups=0.1, wpb=1290, bsz=80, num_updates=3700, lr=9.53593e-06, gnorm=0.419, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=64455
2022-06-28 20:57:37 - progress_bar.py[line:274] - INFO: epoch 001:   3716 / 7081 loss=-0.002, score=1.218, ntokens=1268.3, nsentences=80, sample_size=1268.3, wps=132.2, ups=0.1, wpb=1268.3, bsz=80, num_updates=3710, lr=9.53298e-06, gnorm=0.378, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=64551
2022-06-28 20:59:13 - progress_bar.py[line:274] - INFO: epoch 001:   3726 / 7081 loss=0, score=1.189, ntokens=1282, nsentences=80, sample_size=1282, wps=134.6, ups=0.11, wpb=1282, bsz=80, num_updates=3720, lr=9.53004e-06, gnorm=0.357, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=64646
2022-06-28 21:00:49 - progress_bar.py[line:274] - INFO: epoch 001:   3736 / 7081 loss=0, score=1.241, ntokens=1282.5, nsentences=80, sample_size=1282.5, wps=132.9, ups=0.1, wpb=1282.5, bsz=80, num_updates=3730, lr=9.52709e-06, gnorm=0.525, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=64743
2022-06-28 21:02:25 - progress_bar.py[line:274] - INFO: epoch 001:   3746 / 7081 loss=-0.002, score=1.11, ntokens=1283.8, nsentences=80, sample_size=1283.8, wps=133.6, ups=0.1, wpb=1283.8, bsz=80, num_updates=3740, lr=9.52415e-06, gnorm=0.359, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=64839
2022-06-28 21:04:01 - progress_bar.py[line:274] - INFO: epoch 001:   3756 / 7081 loss=0.001, score=1.192, ntokens=1280.7, nsentences=80, sample_size=1280.7, wps=133, ups=0.1, wpb=1280.7, bsz=80, num_updates=3750, lr=9.5212e-06, gnorm=0.382, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=64935
2022-06-28 21:05:38 - progress_bar.py[line:274] - INFO: epoch 001:   3766 / 7081 loss=0.001, score=1.272, ntokens=1275.7, nsentences=80, sample_size=1275.7, wps=132.6, ups=0.1, wpb=1275.7, bsz=80, num_updates=3760, lr=9.51826e-06, gnorm=0.415, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=65031
2022-06-28 21:07:13 - progress_bar.py[line:274] - INFO: epoch 001:   3776 / 7081 loss=0, score=1.263, ntokens=1272.8, nsentences=80, sample_size=1272.8, wps=133, ups=0.1, wpb=1272.8, bsz=80, num_updates=3770, lr=9.51532e-06, gnorm=0.613, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=65127
2022-06-28 21:08:47 - progress_bar.py[line:274] - INFO: epoch 001:   3786 / 7081 loss=0.001, score=1.203, ntokens=1267, nsentences=80, sample_size=1267, wps=135.1, ups=0.11, wpb=1267, bsz=80, num_updates=3780, lr=9.51237e-06, gnorm=0.328, clip=0, loss_scale=64, train_wall=94, gb_free=6.7, wall=65221
2022-06-28 21:10:23 - progress_bar.py[line:274] - INFO: epoch 001:   3796 / 7081 loss=0.001, score=1.227, ntokens=1284, nsentences=80, sample_size=1284, wps=134.5, ups=0.1, wpb=1284, bsz=80, num_updates=3790, lr=9.50943e-06, gnorm=0.216, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=65316
2022-06-28 21:11:59 - progress_bar.py[line:274] - INFO: epoch 001:   3806 / 7081 loss=-0.002, score=1.185, ntokens=1302.4, nsentences=80, sample_size=1302.4, wps=135.6, ups=0.1, wpb=1302.4, bsz=80, num_updates=3800, lr=9.50648e-06, gnorm=0.467, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=65412
2022-06-28 21:13:34 - progress_bar.py[line:274] - INFO: epoch 001:   3816 / 7081 loss=0.001, score=1.31, ntokens=1315.3, nsentences=80, sample_size=1315.3, wps=138.2, ups=0.11, wpb=1315.3, bsz=80, num_updates=3810, lr=9.50354e-06, gnorm=0.427, clip=10, loss_scale=64, train_wall=95, gb_free=6.7, wall=65507
2022-06-28 21:15:08 - progress_bar.py[line:274] - INFO: epoch 001:   3826 / 7081 loss=-0.001, score=1.226, ntokens=1309, nsentences=80, sample_size=1309, wps=138.7, ups=0.11, wpb=1309, bsz=80, num_updates=3820, lr=9.50059e-06, gnorm=0.36, clip=0, loss_scale=64, train_wall=94, gb_free=6.7, wall=65602
2022-06-28 21:16:46 - progress_bar.py[line:274] - INFO: epoch 001:   3836 / 7081 loss=-0.001, score=1.24, ntokens=1323.8, nsentences=80, sample_size=1323.8, wps=136, ups=0.1, wpb=1323.8, bsz=80, num_updates=3830, lr=9.49765e-06, gnorm=0.285, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=65699
2022-06-28 21:18:21 - progress_bar.py[line:274] - INFO: epoch 001:   3846 / 7081 loss=-0.001, score=1.174, ntokens=1337.7, nsentences=80, sample_size=1337.7, wps=140.1, ups=0.1, wpb=1337.7, bsz=80, num_updates=3840, lr=9.4947e-06, gnorm=0.25, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=65795
2022-06-28 21:19:57 - progress_bar.py[line:274] - INFO: epoch 001:   3856 / 7081 loss=-0.001, score=1.356, ntokens=1344.1, nsentences=80, sample_size=1344.1, wps=139.9, ups=0.1, wpb=1344.1, bsz=80, num_updates=3850, lr=9.49176e-06, gnorm=0.349, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=65891
2022-06-28 21:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   3866 / 7081 loss=0.001, score=1.258, ntokens=1338.6, nsentences=80, sample_size=1338.6, wps=138.1, ups=0.1, wpb=1338.6, bsz=80, num_updates=3860, lr=9.48881e-06, gnorm=0.403, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=65988
2022-06-28 21:23:10 - progress_bar.py[line:274] - INFO: epoch 001:   3876 / 7081 loss=-0, score=1.143, ntokens=1344.4, nsentences=80, sample_size=1344.4, wps=139.6, ups=0.1, wpb=1344.4, bsz=80, num_updates=3870, lr=9.48587e-06, gnorm=0.401, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=66084
2022-06-28 21:24:47 - progress_bar.py[line:274] - INFO: epoch 001:   3886 / 7081 loss=-0.001, score=1.202, ntokens=1336.2, nsentences=80, sample_size=1336.2, wps=138.9, ups=0.1, wpb=1336.2, bsz=80, num_updates=3880, lr=9.48292e-06, gnorm=0.679, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=66180
2022-06-28 21:26:23 - progress_bar.py[line:274] - INFO: epoch 001:   3896 / 7081 loss=0.001, score=1.268, ntokens=1343.1, nsentences=80, sample_size=1343.1, wps=139.7, ups=0.1, wpb=1343.1, bsz=80, num_updates=3890, lr=9.47998e-06, gnorm=0.314, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=66276
2022-06-28 21:28:00 - progress_bar.py[line:274] - INFO: epoch 001:   3906 / 7081 loss=-0, score=1.198, ntokens=1348.2, nsentences=80, sample_size=1348.2, wps=138.3, ups=0.1, wpb=1348.2, bsz=80, num_updates=3900, lr=9.47703e-06, gnorm=0.253, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=66374
2022-06-28 21:29:38 - progress_bar.py[line:274] - INFO: epoch 001:   3916 / 7081 loss=-0.001, score=1.236, ntokens=1340.7, nsentences=80, sample_size=1340.7, wps=137.6, ups=0.1, wpb=1340.7, bsz=80, num_updates=3910, lr=9.47409e-06, gnorm=0.286, clip=10, loss_scale=64, train_wall=97, gb_free=6.7, wall=66471
2022-06-28 21:31:14 - progress_bar.py[line:274] - INFO: epoch 001:   3926 / 7081 loss=0, score=1.224, ntokens=1338.4, nsentences=80, sample_size=1338.4, wps=138.7, ups=0.1, wpb=1338.4, bsz=80, num_updates=3920, lr=9.47115e-06, gnorm=0.58, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=66568
2022-06-28 21:32:50 - progress_bar.py[line:274] - INFO: epoch 001:   3936 / 7081 loss=0, score=1.257, ntokens=1345.6, nsentences=80, sample_size=1345.6, wps=140.4, ups=0.1, wpb=1345.6, bsz=80, num_updates=3930, lr=9.4682e-06, gnorm=0.34, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=66663
2022-06-28 21:34:27 - progress_bar.py[line:274] - INFO: epoch 001:   3946 / 7081 loss=-0.001, score=1.214, ntokens=1333.7, nsentences=80, sample_size=1333.7, wps=138.1, ups=0.1, wpb=1333.7, bsz=80, num_updates=3940, lr=9.46526e-06, gnorm=0.662, clip=20, loss_scale=64, train_wall=96, gb_free=6.7, wall=66760
2022-06-28 21:36:02 - progress_bar.py[line:274] - INFO: epoch 001:   3956 / 7081 loss=-0, score=1.234, ntokens=1323.6, nsentences=80, sample_size=1323.6, wps=138.8, ups=0.1, wpb=1323.6, bsz=80, num_updates=3950, lr=9.46231e-06, gnorm=0.281, clip=10, loss_scale=64, train_wall=95, gb_free=6.7, wall=66855
2022-06-28 21:37:38 - progress_bar.py[line:274] - INFO: epoch 001:   3966 / 7081 loss=-0.001, score=1.127, ntokens=1313.7, nsentences=80, sample_size=1313.7, wps=136, ups=0.1, wpb=1313.7, bsz=80, num_updates=3960, lr=9.45937e-06, gnorm=0.383, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=66952
2022-06-28 21:39:15 - progress_bar.py[line:274] - INFO: epoch 001:   3976 / 7081 loss=0, score=1.248, ntokens=1327.5, nsentences=80, sample_size=1327.5, wps=138.1, ups=0.1, wpb=1327.5, bsz=80, num_updates=3970, lr=9.45642e-06, gnorm=0.298, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=67048
2022-06-28 21:40:51 - progress_bar.py[line:274] - INFO: epoch 001:   3986 / 7081 loss=0.001, score=1.302, ntokens=1337.9, nsentences=80, sample_size=1337.9, wps=139, ups=0.1, wpb=1337.9, bsz=80, num_updates=3980, lr=9.45348e-06, gnorm=0.6, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=67144
2022-06-28 21:42:27 - progress_bar.py[line:274] - INFO: epoch 001:   3996 / 7081 loss=-0.001, score=1.263, ntokens=1344.8, nsentences=80, sample_size=1344.8, wps=139.9, ups=0.1, wpb=1344.8, bsz=80, num_updates=3990, lr=9.45053e-06, gnorm=0.307, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=67240
2022-06-28 21:44:03 - progress_bar.py[line:274] - INFO: epoch 001:   4006 / 7081 loss=0, score=1.209, ntokens=1348.7, nsentences=80, sample_size=1348.7, wps=140.7, ups=0.1, wpb=1348.7, bsz=80, num_updates=4000, lr=9.44759e-06, gnorm=0.331, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=67336
slice_id 1 seek offset 2500
2022-06-28 21:44:03 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-28 22:41:04 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0 | score 1.156 | ntokens 168.754 | nsentences 10 | sample_size 168.754 | cider 1.175 | wps 123.3 | wpb 168.8 | bsz 10 | num_updates 4000 | best_cider 1.175
2022-06-28 22:41:04 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2022-06-28 22:41:04 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_4000.pt
2022-06-28 22:41:19 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_4000.pt
2022-06-28 22:42:57 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 1.175) (writing took 112.9226149180904 seconds)
2022-06-28 22:44:35 - progress_bar.py[line:274] - INFO: epoch 001:   4016 / 7081 loss=0, score=1.262, ntokens=1348.6, nsentences=80, sample_size=1348.6, wps=3.7, ups=0, wpb=1348.6, bsz=80, num_updates=4010, lr=9.44464e-06, gnorm=0.251, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=70969
2022-06-28 22:46:11 - progress_bar.py[line:274] - INFO: epoch 001:   4026 / 7081 loss=0, score=1.233, ntokens=1348.1, nsentences=80, sample_size=1348.1, wps=139.9, ups=0.1, wpb=1348.1, bsz=80, num_updates=4020, lr=9.4417e-06, gnorm=0.255, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=71065
2022-06-28 22:47:49 - progress_bar.py[line:274] - INFO: epoch 001:   4036 / 7081 loss=0, score=1.108, ntokens=1353.2, nsentences=80, sample_size=1353.2, wps=138.9, ups=0.1, wpb=1353.2, bsz=80, num_updates=4030, lr=9.43875e-06, gnorm=0.397, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=71162
2022-06-28 22:49:25 - progress_bar.py[line:274] - INFO: epoch 001:   4046 / 7081 loss=-0, score=1.154, ntokens=1353.5, nsentences=80, sample_size=1353.5, wps=140.2, ups=0.1, wpb=1353.5, bsz=80, num_updates=4040, lr=9.43581e-06, gnorm=0.331, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=71259
2022-06-28 22:51:03 - progress_bar.py[line:274] - INFO: epoch 001:   4056 / 7081 loss=0, score=1.265, ntokens=1351.5, nsentences=80, sample_size=1351.5, wps=137.9, ups=0.1, wpb=1351.5, bsz=80, num_updates=4050, lr=9.43287e-06, gnorm=0.165, clip=0, loss_scale=128, train_wall=98, gb_free=6.7, wall=71357
2022-06-28 22:52:40 - progress_bar.py[line:274] - INFO: epoch 001:   4066 / 7081 loss=0, score=1.271, ntokens=1356, nsentences=80, sample_size=1356, wps=141, ups=0.1, wpb=1356, bsz=80, num_updates=4060, lr=9.42992e-06, gnorm=0.261, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=71453
2022-06-28 22:54:17 - progress_bar.py[line:274] - INFO: epoch 001:   4076 / 7081 loss=-0.001, score=1.26, ntokens=1358.3, nsentences=80, sample_size=1358.3, wps=139.7, ups=0.1, wpb=1358.3, bsz=80, num_updates=4070, lr=9.42698e-06, gnorm=0.436, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=71550
2022-06-28 22:55:53 - progress_bar.py[line:274] - INFO: epoch 001:   4086 / 7081 loss=-0, score=1.283, ntokens=1356.2, nsentences=80, sample_size=1356.2, wps=141.1, ups=0.1, wpb=1356.2, bsz=80, num_updates=4080, lr=9.42403e-06, gnorm=0.384, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=71646
2022-06-28 22:57:29 - progress_bar.py[line:274] - INFO: epoch 001:   4096 / 7081 loss=0, score=1.199, ntokens=1352.6, nsentences=80, sample_size=1352.6, wps=140.7, ups=0.1, wpb=1352.6, bsz=80, num_updates=4090, lr=9.42109e-06, gnorm=0.271, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=71743
2022-06-28 22:59:06 - progress_bar.py[line:274] - INFO: epoch 001:   4106 / 7081 loss=0, score=1.274, ntokens=1355.6, nsentences=80, sample_size=1355.6, wps=139.5, ups=0.1, wpb=1355.6, bsz=80, num_updates=4100, lr=9.41814e-06, gnorm=0.252, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=71840
2022-06-28 23:00:43 - progress_bar.py[line:274] - INFO: epoch 001:   4116 / 7081 loss=0.001, score=1.253, ntokens=1357, nsentences=80, sample_size=1357, wps=140.1, ups=0.1, wpb=1357, bsz=80, num_updates=4110, lr=9.4152e-06, gnorm=0.467, clip=20, loss_scale=128, train_wall=97, gb_free=6.7, wall=71937
2022-06-28 23:02:20 - progress_bar.py[line:274] - INFO: epoch 001:   4126 / 7081 loss=-0.001, score=1.184, ntokens=1357.7, nsentences=80, sample_size=1357.7, wps=140.6, ups=0.1, wpb=1357.7, bsz=80, num_updates=4120, lr=9.41225e-06, gnorm=0.289, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=72033
2022-06-28 23:03:56 - progress_bar.py[line:274] - INFO: epoch 001:   4136 / 7081 loss=0.001, score=1.189, ntokens=1358.3, nsentences=80, sample_size=1358.3, wps=141.5, ups=0.1, wpb=1358.3, bsz=80, num_updates=4130, lr=9.40931e-06, gnorm=0.345, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=72129
2022-06-28 23:05:31 - progress_bar.py[line:274] - INFO: epoch 001:   4146 / 7081 loss=0, score=1.338, ntokens=1356.9, nsentences=80, sample_size=1356.9, wps=142.3, ups=0.1, wpb=1356.9, bsz=80, num_updates=4140, lr=9.40636e-06, gnorm=0.246, clip=0, loss_scale=128, train_wall=95, gb_free=6.7, wall=72225
2022-06-28 23:07:06 - progress_bar.py[line:274] - INFO: epoch 001:   4156 / 7081 loss=0.001, score=1.162, ntokens=1354.9, nsentences=80, sample_size=1354.9, wps=142.9, ups=0.11, wpb=1354.9, bsz=80, num_updates=4150, lr=9.40342e-06, gnorm=0.308, clip=0, loss_scale=128, train_wall=95, gb_free=6.7, wall=72319
2022-06-28 23:08:42 - progress_bar.py[line:274] - INFO: epoch 001:   4166 / 7081 loss=-0, score=1.399, ntokens=1357.3, nsentences=80, sample_size=1357.3, wps=141.5, ups=0.1, wpb=1357.3, bsz=80, num_updates=4160, lr=9.40047e-06, gnorm=0.186, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=72415
2022-06-28 23:10:17 - progress_bar.py[line:274] - INFO: epoch 001:   4176 / 7081 loss=-0, score=1.143, ntokens=1355.9, nsentences=80, sample_size=1355.9, wps=141.8, ups=0.1, wpb=1355.9, bsz=80, num_updates=4170, lr=9.39753e-06, gnorm=0.276, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=72511
2022-06-28 23:11:06 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-28 23:12:06 - progress_bar.py[line:274] - INFO: epoch 001:   4187 / 7081 loss=-0, score=1.197, ntokens=1359.3, nsentences=80, sample_size=1359.3, wps=125, ups=0.09, wpb=1359.3, bsz=80, num_updates=4180, lr=9.39459e-06, gnorm=0.264, clip=0, loss_scale=64, train_wall=109, gb_free=6.7, wall=72620
2022-06-28 23:13:43 - progress_bar.py[line:274] - INFO: epoch 001:   4197 / 7081 loss=-0, score=1.352, ntokens=1357, nsentences=80, sample_size=1357, wps=139.8, ups=0.1, wpb=1357, bsz=80, num_updates=4190, lr=9.39164e-06, gnorm=0.449, clip=10, loss_scale=64, train_wall=97, gb_free=6.7, wall=72717
2022-06-28 23:15:20 - progress_bar.py[line:274] - INFO: epoch 001:   4207 / 7081 loss=-0, score=1.079, ntokens=1356.5, nsentences=80, sample_size=1356.5, wps=139.7, ups=0.1, wpb=1356.5, bsz=80, num_updates=4200, lr=9.3887e-06, gnorm=0.558, clip=20, loss_scale=64, train_wall=97, gb_free=6.7, wall=72814
2022-06-28 23:16:56 - progress_bar.py[line:274] - INFO: epoch 001:   4217 / 7081 loss=0, score=1.258, ntokens=1357.5, nsentences=80, sample_size=1357.5, wps=141.5, ups=0.1, wpb=1357.5, bsz=80, num_updates=4210, lr=9.38575e-06, gnorm=0.301, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=72910
2022-06-28 23:18:32 - progress_bar.py[line:274] - INFO: epoch 001:   4227 / 7081 loss=-0, score=1.202, ntokens=1358.4, nsentences=80, sample_size=1358.4, wps=142.1, ups=0.1, wpb=1358.4, bsz=80, num_updates=4220, lr=9.38281e-06, gnorm=0.152, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=73005
2022-06-28 23:20:08 - progress_bar.py[line:274] - INFO: epoch 001:   4237 / 7081 loss=-0, score=1.212, ntokens=1357.6, nsentences=80, sample_size=1357.6, wps=141.5, ups=0.1, wpb=1357.6, bsz=80, num_updates=4230, lr=9.37986e-06, gnorm=0.227, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=73101
2022-06-28 23:21:44 - progress_bar.py[line:274] - INFO: epoch 001:   4247 / 7081 loss=-0, score=1.35, ntokens=1354.7, nsentences=80, sample_size=1354.7, wps=140.6, ups=0.1, wpb=1354.7, bsz=80, num_updates=4240, lr=9.37692e-06, gnorm=0.382, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=73198
2022-06-28 23:23:20 - progress_bar.py[line:274] - INFO: epoch 001:   4257 / 7081 loss=0, score=1.256, ntokens=1348, nsentences=80, sample_size=1348, wps=140, ups=0.1, wpb=1348, bsz=80, num_updates=4250, lr=9.37397e-06, gnorm=0.275, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=73294
2022-06-28 23:24:57 - progress_bar.py[line:274] - INFO: epoch 001:   4267 / 7081 loss=0, score=1.289, ntokens=1357.4, nsentences=80, sample_size=1357.4, wps=140.4, ups=0.1, wpb=1357.4, bsz=80, num_updates=4260, lr=9.37103e-06, gnorm=0.269, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=73391
2022-06-28 23:26:33 - progress_bar.py[line:274] - INFO: epoch 001:   4277 / 7081 loss=0, score=1.339, ntokens=1356.1, nsentences=80, sample_size=1356.1, wps=140.8, ups=0.1, wpb=1356.1, bsz=80, num_updates=4270, lr=9.36808e-06, gnorm=0.373, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=73487
2022-06-28 23:28:09 - progress_bar.py[line:274] - INFO: epoch 001:   4287 / 7081 loss=-0, score=1.181, ntokens=1356.4, nsentences=80, sample_size=1356.4, wps=141.3, ups=0.1, wpb=1356.4, bsz=80, num_updates=4280, lr=9.36514e-06, gnorm=0.273, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=73583
2022-06-28 23:29:47 - progress_bar.py[line:274] - INFO: epoch 001:   4297 / 7081 loss=0, score=1.194, ntokens=1354.2, nsentences=80, sample_size=1354.2, wps=138.8, ups=0.1, wpb=1354.2, bsz=80, num_updates=4290, lr=9.36219e-06, gnorm=0.218, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=73681
2022-06-28 23:31:23 - progress_bar.py[line:274] - INFO: epoch 001:   4307 / 7081 loss=-0, score=1.243, ntokens=1356, nsentences=80, sample_size=1356, wps=141.1, ups=0.1, wpb=1356, bsz=80, num_updates=4300, lr=9.35925e-06, gnorm=0.362, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=73777
2022-06-28 23:32:59 - progress_bar.py[line:274] - INFO: epoch 001:   4317 / 7081 loss=-0.001, score=1.19, ntokens=1354.8, nsentences=80, sample_size=1354.8, wps=141.3, ups=0.1, wpb=1354.8, bsz=80, num_updates=4310, lr=9.35631e-06, gnorm=0.248, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=73873
2022-06-28 23:34:35 - progress_bar.py[line:274] - INFO: epoch 001:   4327 / 7081 loss=0.001, score=1.249, ntokens=1354.9, nsentences=80, sample_size=1354.9, wps=141.1, ups=0.1, wpb=1354.9, bsz=80, num_updates=4320, lr=9.35336e-06, gnorm=0.217, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=73969
2022-06-28 23:36:11 - progress_bar.py[line:274] - INFO: epoch 001:   4337 / 7081 loss=-0, score=1.205, ntokens=1356.7, nsentences=80, sample_size=1356.7, wps=141.1, ups=0.1, wpb=1356.7, bsz=80, num_updates=4330, lr=9.35042e-06, gnorm=0.212, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=74065
2022-06-28 23:37:48 - progress_bar.py[line:274] - INFO: epoch 001:   4347 / 7081 loss=0.001, score=1.254, ntokens=1356.9, nsentences=80, sample_size=1356.9, wps=140.7, ups=0.1, wpb=1356.9, bsz=80, num_updates=4340, lr=9.34747e-06, gnorm=0.347, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=74161
2022-06-28 23:39:23 - progress_bar.py[line:274] - INFO: epoch 001:   4357 / 7081 loss=0, score=1.223, ntokens=1356.8, nsentences=80, sample_size=1356.8, wps=142.5, ups=0.11, wpb=1356.8, bsz=80, num_updates=4350, lr=9.34453e-06, gnorm=0.203, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=74256
2022-06-28 23:40:59 - progress_bar.py[line:274] - INFO: epoch 001:   4367 / 7081 loss=-0, score=1.249, ntokens=1356.3, nsentences=80, sample_size=1356.3, wps=141.2, ups=0.1, wpb=1356.3, bsz=80, num_updates=4360, lr=9.34158e-06, gnorm=0.342, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=74352
2022-06-28 23:42:36 - progress_bar.py[line:274] - INFO: epoch 001:   4377 / 7081 loss=0.001, score=1.206, ntokens=1356.1, nsentences=80, sample_size=1356.1, wps=139.5, ups=0.1, wpb=1356.1, bsz=80, num_updates=4370, lr=9.33864e-06, gnorm=0.233, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=74450
2022-06-28 23:44:12 - progress_bar.py[line:274] - INFO: epoch 001:   4387 / 7081 loss=0, score=1.125, ntokens=1356.7, nsentences=80, sample_size=1356.7, wps=141.1, ups=0.1, wpb=1356.7, bsz=80, num_updates=4380, lr=9.33569e-06, gnorm=0.17, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=74546
2022-06-28 23:45:48 - progress_bar.py[line:274] - INFO: epoch 001:   4397 / 7081 loss=0, score=1.222, ntokens=1358.5, nsentences=80, sample_size=1358.5, wps=141.5, ups=0.1, wpb=1358.5, bsz=80, num_updates=4390, lr=9.33275e-06, gnorm=0.292, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=74642
2022-06-28 23:47:25 - progress_bar.py[line:274] - INFO: epoch 001:   4407 / 7081 loss=0, score=1.212, ntokens=1358.7, nsentences=80, sample_size=1358.7, wps=140.7, ups=0.1, wpb=1358.7, bsz=80, num_updates=4400, lr=9.3298e-06, gnorm=0.349, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=74738
2022-06-28 23:49:02 - progress_bar.py[line:274] - INFO: epoch 001:   4417 / 7081 loss=-0, score=1.271, ntokens=1357.8, nsentences=80, sample_size=1357.8, wps=139.8, ups=0.1, wpb=1357.8, bsz=80, num_updates=4410, lr=9.32686e-06, gnorm=0.273, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=74835
2022-06-28 23:50:39 - progress_bar.py[line:274] - INFO: epoch 001:   4427 / 7081 loss=-0, score=1.233, ntokens=1356.5, nsentences=80, sample_size=1356.5, wps=140.5, ups=0.1, wpb=1356.5, bsz=80, num_updates=4420, lr=9.32391e-06, gnorm=0.307, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=74932
2022-06-28 23:52:15 - progress_bar.py[line:274] - INFO: epoch 001:   4437 / 7081 loss=-0.001, score=1.23, ntokens=1359.7, nsentences=80, sample_size=1359.7, wps=141.2, ups=0.1, wpb=1359.7, bsz=80, num_updates=4430, lr=9.32097e-06, gnorm=0.276, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=75028
2022-06-28 23:53:51 - progress_bar.py[line:274] - INFO: epoch 001:   4447 / 7081 loss=0.001, score=1.141, ntokens=1358.3, nsentences=80, sample_size=1358.3, wps=140.5, ups=0.1, wpb=1358.3, bsz=80, num_updates=4440, lr=9.31803e-06, gnorm=0.515, clip=10, loss_scale=64, train_wall=97, gb_free=6.7, wall=75125
2022-06-28 23:55:28 - progress_bar.py[line:274] - INFO: epoch 001:   4457 / 7081 loss=0, score=1.241, ntokens=1358.8, nsentences=80, sample_size=1358.8, wps=140.3, ups=0.1, wpb=1358.8, bsz=80, num_updates=4450, lr=9.31508e-06, gnorm=0.253, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=75222
2022-06-28 23:57:05 - progress_bar.py[line:274] - INFO: epoch 001:   4467 / 7081 loss=0.001, score=1.135, ntokens=1359.7, nsentences=80, sample_size=1359.7, wps=141.1, ups=0.1, wpb=1359.7, bsz=80, num_updates=4460, lr=9.31214e-06, gnorm=0.194, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=75318
2022-06-28 23:58:41 - progress_bar.py[line:274] - INFO: epoch 001:   4477 / 7081 loss=0, score=1.264, ntokens=1353.7, nsentences=80, sample_size=1353.7, wps=140.1, ups=0.1, wpb=1353.7, bsz=80, num_updates=4470, lr=9.30919e-06, gnorm=0.205, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=75415
2022-06-29 00:00:18 - progress_bar.py[line:274] - INFO: epoch 001:   4487 / 7081 loss=0, score=1.295, ntokens=1358.6, nsentences=80, sample_size=1358.6, wps=141.2, ups=0.1, wpb=1358.6, bsz=80, num_updates=4480, lr=9.30625e-06, gnorm=0.489, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=75511
2022-06-29 00:01:54 - progress_bar.py[line:274] - INFO: epoch 001:   4497 / 7081 loss=-0.001, score=1.158, ntokens=1356.3, nsentences=80, sample_size=1356.3, wps=140.6, ups=0.1, wpb=1356.3, bsz=80, num_updates=4490, lr=9.3033e-06, gnorm=0.355, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=75608
2022-06-29 00:03:30 - progress_bar.py[line:274] - INFO: epoch 001:   4507 / 7081 loss=-0, score=1.14, ntokens=1358.5, nsentences=80, sample_size=1358.5, wps=141.7, ups=0.1, wpb=1358.5, bsz=80, num_updates=4500, lr=9.30036e-06, gnorm=0.185, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=75703
slice_id 1 seek offset 2500
2022-06-29 00:03:30 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-29 01:00:27 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0 | score 1.154 | ntokens 169.731 | nsentences 10 | sample_size 169.731 | cider 1.176 | wps 124.2 | wpb 169.7 | bsz 10 | num_updates 4500 | best_cider 1.176
2022-06-29 01:00:27 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4500 updates
2022-06-29 01:00:27 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_4500.pt
2022-06-29 01:00:40 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_4500.pt
2022-06-29 01:02:16 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_4500.pt (epoch 1 @ 4500 updates, score 1.176) (writing took 109.31741659715772 seconds)
2022-06-29 01:03:52 - progress_bar.py[line:274] - INFO: epoch 001:   4517 / 7081 loss=0.001, score=1.216, ntokens=1355, nsentences=80, sample_size=1355, wps=3.7, ups=0, wpb=1355, bsz=80, num_updates=4510, lr=9.29741e-06, gnorm=0.479, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=79326
2022-06-29 01:05:28 - progress_bar.py[line:274] - INFO: epoch 001:   4527 / 7081 loss=-0.001, score=1.265, ntokens=1357.2, nsentences=80, sample_size=1357.2, wps=140.9, ups=0.1, wpb=1357.2, bsz=80, num_updates=4520, lr=9.29447e-06, gnorm=0.221, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=79422
2022-06-29 01:07:06 - progress_bar.py[line:274] - INFO: epoch 001:   4537 / 7081 loss=-0, score=1.279, ntokens=1355.9, nsentences=80, sample_size=1355.9, wps=139.3, ups=0.1, wpb=1355.9, bsz=80, num_updates=4530, lr=9.29152e-06, gnorm=0.207, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=79519
2022-06-29 01:08:42 - progress_bar.py[line:274] - INFO: epoch 001:   4547 / 7081 loss=-0, score=1.136, ntokens=1357.1, nsentences=80, sample_size=1357.1, wps=140.4, ups=0.1, wpb=1357.1, bsz=80, num_updates=4540, lr=9.28858e-06, gnorm=0.155, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=79616
2022-06-29 01:10:19 - progress_bar.py[line:274] - INFO: epoch 001:   4557 / 7081 loss=-0.001, score=1.229, ntokens=1357.7, nsentences=80, sample_size=1357.7, wps=141.1, ups=0.1, wpb=1357.7, bsz=80, num_updates=4550, lr=9.28563e-06, gnorm=0.177, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=79712
2022-06-29 01:11:55 - progress_bar.py[line:274] - INFO: epoch 001:   4567 / 7081 loss=-0, score=1.185, ntokens=1357.9, nsentences=80, sample_size=1357.9, wps=140.3, ups=0.1, wpb=1357.9, bsz=80, num_updates=4560, lr=9.28269e-06, gnorm=0.27, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=79809
2022-06-29 01:13:32 - progress_bar.py[line:274] - INFO: epoch 001:   4577 / 7081 loss=-0.001, score=1.331, ntokens=1357.1, nsentences=80, sample_size=1357.1, wps=139.8, ups=0.1, wpb=1357.1, bsz=80, num_updates=4570, lr=9.27975e-06, gnorm=0.346, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=79906
2022-06-29 01:15:08 - progress_bar.py[line:274] - INFO: epoch 001:   4587 / 7081 loss=0, score=1.223, ntokens=1358.4, nsentences=80, sample_size=1358.4, wps=141.6, ups=0.1, wpb=1358.4, bsz=80, num_updates=4580, lr=9.2768e-06, gnorm=0.312, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=80002
2022-06-29 01:16:44 - progress_bar.py[line:274] - INFO: epoch 001:   4597 / 7081 loss=-0, score=1.137, ntokens=1356.9, nsentences=80, sample_size=1356.9, wps=141.3, ups=0.1, wpb=1356.9, bsz=80, num_updates=4590, lr=9.27386e-06, gnorm=0.26, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=80098
2022-06-29 01:18:20 - progress_bar.py[line:274] - INFO: epoch 001:   4607 / 7081 loss=-0, score=1.145, ntokens=1356.7, nsentences=80, sample_size=1356.7, wps=142.6, ups=0.11, wpb=1356.7, bsz=80, num_updates=4600, lr=9.27091e-06, gnorm=0.232, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=80193
2022-06-29 01:19:54 - progress_bar.py[line:274] - INFO: epoch 001:   4617 / 7081 loss=0, score=1.201, ntokens=1356.8, nsentences=80, sample_size=1356.8, wps=143.3, ups=0.11, wpb=1356.8, bsz=80, num_updates=4610, lr=9.26797e-06, gnorm=0.233, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=80288
2022-06-29 01:21:30 - progress_bar.py[line:274] - INFO: epoch 001:   4627 / 7081 loss=-0, score=1.262, ntokens=1356, nsentences=80, sample_size=1356, wps=141.6, ups=0.1, wpb=1356, bsz=80, num_updates=4620, lr=9.26502e-06, gnorm=0.262, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=80383
2022-06-29 01:23:06 - progress_bar.py[line:274] - INFO: epoch 001:   4637 / 7081 loss=-0, score=1.249, ntokens=1358.2, nsentences=80, sample_size=1358.2, wps=141.7, ups=0.1, wpb=1358.2, bsz=80, num_updates=4630, lr=9.26208e-06, gnorm=0.245, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=80479
2022-06-29 01:24:41 - progress_bar.py[line:274] - INFO: epoch 001:   4647 / 7081 loss=0, score=1.109, ntokens=1358.5, nsentences=80, sample_size=1358.5, wps=142.9, ups=0.11, wpb=1358.5, bsz=80, num_updates=4640, lr=9.25913e-06, gnorm=0.317, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=80574
2022-06-29 01:26:17 - progress_bar.py[line:274] - INFO: epoch 001:   4657 / 7081 loss=0, score=1.309, ntokens=1356, nsentences=80, sample_size=1356, wps=141.1, ups=0.1, wpb=1356, bsz=80, num_updates=4650, lr=9.25619e-06, gnorm=0.302, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=80670
2022-06-29 01:27:54 - progress_bar.py[line:274] - INFO: epoch 001:   4667 / 7081 loss=0, score=1.273, ntokens=1358.7, nsentences=80, sample_size=1358.7, wps=140.7, ups=0.1, wpb=1358.7, bsz=80, num_updates=4660, lr=9.25324e-06, gnorm=0.23, clip=0, loss_scale=64, train_wall=96, gb_free=6.7, wall=80767
2022-06-29 01:29:30 - progress_bar.py[line:274] - INFO: epoch 001:   4677 / 7081 loss=0, score=1.352, ntokens=1359.3, nsentences=80, sample_size=1359.3, wps=141, ups=0.1, wpb=1359.3, bsz=80, num_updates=4670, lr=9.2503e-06, gnorm=0.433, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=80863
2022-06-29 01:31:07 - progress_bar.py[line:274] - INFO: epoch 001:   4687 / 7081 loss=-0.001, score=1.252, ntokens=1359.8, nsentences=80, sample_size=1359.8, wps=140.7, ups=0.1, wpb=1359.8, bsz=80, num_updates=4680, lr=9.24735e-06, gnorm=0.253, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=80960
2022-06-29 01:32:43 - progress_bar.py[line:274] - INFO: epoch 001:   4697 / 7081 loss=-0, score=1.315, ntokens=1358.3, nsentences=80, sample_size=1358.3, wps=140.7, ups=0.1, wpb=1358.3, bsz=80, num_updates=4690, lr=9.24441e-06, gnorm=0.521, clip=30, loss_scale=128, train_wall=96, gb_free=6.7, wall=81057
2022-06-29 01:34:19 - progress_bar.py[line:274] - INFO: epoch 001:   4707 / 7081 loss=-0, score=1.316, ntokens=1359.1, nsentences=80, sample_size=1359.1, wps=141.3, ups=0.1, wpb=1359.1, bsz=80, num_updates=4700, lr=9.24147e-06, gnorm=0.291, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=81153
2022-06-29 01:35:56 - progress_bar.py[line:274] - INFO: epoch 001:   4717 / 7081 loss=0, score=1.262, ntokens=1357.7, nsentences=80, sample_size=1357.7, wps=140.3, ups=0.1, wpb=1357.7, bsz=80, num_updates=4710, lr=9.23852e-06, gnorm=0.292, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=81250
2022-06-29 01:37:33 - progress_bar.py[line:274] - INFO: epoch 001:   4727 / 7081 loss=0.001, score=1.193, ntokens=1358.5, nsentences=80, sample_size=1358.5, wps=140.5, ups=0.1, wpb=1358.5, bsz=80, num_updates=4720, lr=9.23558e-06, gnorm=0.254, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=81346
2022-06-29 01:39:08 - progress_bar.py[line:274] - INFO: epoch 001:   4737 / 7081 loss=-0.001, score=1.298, ntokens=1358.2, nsentences=80, sample_size=1358.2, wps=142.1, ups=0.1, wpb=1358.2, bsz=80, num_updates=4730, lr=9.23263e-06, gnorm=0.516, clip=10, loss_scale=128, train_wall=95, gb_free=6.7, wall=81442
2022-06-29 01:40:44 - progress_bar.py[line:274] - INFO: epoch 001:   4747 / 7081 loss=-0, score=1.24, ntokens=1359.2, nsentences=80, sample_size=1359.2, wps=142.2, ups=0.1, wpb=1359.2, bsz=80, num_updates=4740, lr=9.22969e-06, gnorm=0.319, clip=0, loss_scale=128, train_wall=95, gb_free=6.7, wall=81537
2022-06-29 01:42:21 - progress_bar.py[line:274] - INFO: epoch 001:   4757 / 7081 loss=0.001, score=1.243, ntokens=1358.4, nsentences=80, sample_size=1358.4, wps=139.7, ups=0.1, wpb=1358.4, bsz=80, num_updates=4750, lr=9.22674e-06, gnorm=0.788, clip=20, loss_scale=128, train_wall=97, gb_free=6.7, wall=81635
2022-06-29 01:43:58 - progress_bar.py[line:274] - INFO: epoch 001:   4767 / 7081 loss=0, score=1.268, ntokens=1359.4, nsentences=80, sample_size=1359.4, wps=140.7, ups=0.1, wpb=1359.4, bsz=80, num_updates=4760, lr=9.2238e-06, gnorm=0.455, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=81731
2022-06-29 01:45:35 - progress_bar.py[line:274] - INFO: epoch 001:   4777 / 7081 loss=-0, score=1.269, ntokens=1357.8, nsentences=80, sample_size=1357.8, wps=139, ups=0.1, wpb=1357.8, bsz=80, num_updates=4770, lr=9.22085e-06, gnorm=0.26, clip=0, loss_scale=128, train_wall=98, gb_free=6.7, wall=81829
2022-06-29 01:47:12 - progress_bar.py[line:274] - INFO: epoch 001:   4787 / 7081 loss=-0, score=1.223, ntokens=1357.3, nsentences=80, sample_size=1357.3, wps=140.5, ups=0.1, wpb=1357.3, bsz=80, num_updates=4780, lr=9.21791e-06, gnorm=0.568, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=81926
2022-06-29 01:48:49 - progress_bar.py[line:274] - INFO: epoch 001:   4797 / 7081 loss=0, score=1.269, ntokens=1355.4, nsentences=80, sample_size=1355.4, wps=140.2, ups=0.1, wpb=1355.4, bsz=80, num_updates=4790, lr=9.21496e-06, gnorm=0.321, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=82022
2022-06-29 01:50:26 - progress_bar.py[line:274] - INFO: epoch 001:   4807 / 7081 loss=0, score=1.141, ntokens=1358.2, nsentences=80, sample_size=1358.2, wps=140.2, ups=0.1, wpb=1358.2, bsz=80, num_updates=4800, lr=9.21202e-06, gnorm=0.348, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=82119
2022-06-29 01:52:02 - progress_bar.py[line:274] - INFO: epoch 001:   4817 / 7081 loss=-0.001, score=1.168, ntokens=1356.5, nsentences=80, sample_size=1356.5, wps=140.3, ups=0.1, wpb=1356.5, bsz=80, num_updates=4810, lr=9.20907e-06, gnorm=0.394, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=82216
2022-06-29 01:53:38 - progress_bar.py[line:274] - INFO: epoch 001:   4827 / 7081 loss=0, score=1.32, ntokens=1358.6, nsentences=80, sample_size=1358.6, wps=141.8, ups=0.1, wpb=1358.6, bsz=80, num_updates=4820, lr=9.20613e-06, gnorm=0.376, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=82312
2022-06-29 01:55:14 - progress_bar.py[line:274] - INFO: epoch 001:   4837 / 7081 loss=0, score=1.204, ntokens=1355.3, nsentences=80, sample_size=1355.3, wps=140.7, ups=0.1, wpb=1355.3, bsz=80, num_updates=4830, lr=9.20319e-06, gnorm=0.54, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=82408
2022-06-29 01:56:51 - progress_bar.py[line:274] - INFO: epoch 001:   4847 / 7081 loss=-0, score=1.404, ntokens=1353.9, nsentences=80, sample_size=1353.9, wps=139.7, ups=0.1, wpb=1353.9, bsz=80, num_updates=4840, lr=9.20024e-06, gnorm=0.276, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=82505
2022-06-29 01:58:29 - progress_bar.py[line:274] - INFO: epoch 001:   4857 / 7081 loss=-0.001, score=1.387, ntokens=1357, nsentences=80, sample_size=1357, wps=138.7, ups=0.1, wpb=1357, bsz=80, num_updates=4850, lr=9.1973e-06, gnorm=0.609, clip=20, loss_scale=128, train_wall=98, gb_free=6.7, wall=82603
2022-06-29 02:00:05 - progress_bar.py[line:274] - INFO: epoch 001:   4867 / 7081 loss=0.001, score=1.221, ntokens=1355.6, nsentences=80, sample_size=1355.6, wps=140.8, ups=0.1, wpb=1355.6, bsz=80, num_updates=4860, lr=9.19435e-06, gnorm=0.426, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=82699
2022-06-29 02:01:43 - progress_bar.py[line:274] - INFO: epoch 001:   4877 / 7081 loss=-0.001, score=1.249, ntokens=1352.4, nsentences=80, sample_size=1352.4, wps=139.3, ups=0.1, wpb=1352.4, bsz=80, num_updates=4870, lr=9.19141e-06, gnorm=0.51, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=82796
2022-06-29 02:03:19 - progress_bar.py[line:274] - INFO: epoch 001:   4887 / 7081 loss=-0.003, score=1.267, ntokens=1348, nsentences=80, sample_size=1348, wps=139.3, ups=0.1, wpb=1348, bsz=80, num_updates=4880, lr=9.18846e-06, gnorm=0.716, clip=20, loss_scale=128, train_wall=97, gb_free=6.7, wall=82893
2022-06-29 02:04:57 - progress_bar.py[line:274] - INFO: epoch 001:   4897 / 7081 loss=-0, score=1.202, ntokens=1338.5, nsentences=80, sample_size=1338.5, wps=137.3, ups=0.1, wpb=1338.5, bsz=80, num_updates=4890, lr=9.18552e-06, gnorm=0.438, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=82990
2022-06-29 02:06:34 - progress_bar.py[line:274] - INFO: epoch 001:   4907 / 7081 loss=-0, score=1.284, ntokens=1340.4, nsentences=80, sample_size=1340.4, wps=138, ups=0.1, wpb=1340.4, bsz=80, num_updates=4900, lr=9.18257e-06, gnorm=0.353, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=83087
2022-06-29 02:08:11 - progress_bar.py[line:274] - INFO: epoch 001:   4917 / 7081 loss=0, score=1.298, ntokens=1338.4, nsentences=80, sample_size=1338.4, wps=138, ups=0.1, wpb=1338.4, bsz=80, num_updates=4910, lr=9.17963e-06, gnorm=0.586, clip=30, loss_scale=128, train_wall=97, gb_free=6.7, wall=83184
2022-06-29 02:09:48 - progress_bar.py[line:274] - INFO: epoch 001:   4927 / 7081 loss=-0, score=1.257, ntokens=1341.4, nsentences=80, sample_size=1341.4, wps=137.7, ups=0.1, wpb=1341.4, bsz=80, num_updates=4920, lr=9.17668e-06, gnorm=0.572, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=83282
2022-06-29 02:11:24 - progress_bar.py[line:274] - INFO: epoch 001:   4937 / 7081 loss=-0.001, score=1.294, ntokens=1352.6, nsentences=80, sample_size=1352.6, wps=140.9, ups=0.1, wpb=1352.6, bsz=80, num_updates=4930, lr=9.17374e-06, gnorm=0.4, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=83378
2022-06-29 02:13:00 - progress_bar.py[line:274] - INFO: epoch 001:   4947 / 7081 loss=-0.001, score=1.208, ntokens=1351.4, nsentences=80, sample_size=1351.4, wps=141.3, ups=0.1, wpb=1351.4, bsz=80, num_updates=4940, lr=9.17079e-06, gnorm=0.463, clip=20, loss_scale=128, train_wall=96, gb_free=6.7, wall=83473
2022-06-29 02:14:38 - progress_bar.py[line:274] - INFO: epoch 001:   4957 / 7081 loss=-0.001, score=1.191, ntokens=1350.4, nsentences=80, sample_size=1350.4, wps=138.2, ups=0.1, wpb=1350.4, bsz=80, num_updates=4950, lr=9.16785e-06, gnorm=0.37, clip=10, loss_scale=128, train_wall=98, gb_free=6.7, wall=83571
2022-06-29 02:16:14 - progress_bar.py[line:274] - INFO: epoch 001:   4967 / 7081 loss=0.001, score=1.258, ntokens=1341.4, nsentences=80, sample_size=1341.4, wps=139.1, ups=0.1, wpb=1341.4, bsz=80, num_updates=4960, lr=9.1649e-06, gnorm=0.465, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=83668
2022-06-29 02:17:51 - progress_bar.py[line:274] - INFO: epoch 001:   4977 / 7081 loss=-0, score=1.207, ntokens=1339.9, nsentences=80, sample_size=1339.9, wps=138.9, ups=0.1, wpb=1339.9, bsz=80, num_updates=4970, lr=9.16196e-06, gnorm=0.468, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=83764
2022-06-29 02:19:25 - progress_bar.py[line:274] - INFO: epoch 001:   4987 / 7081 loss=-0.001, score=1.251, ntokens=1338, nsentences=80, sample_size=1338, wps=141.3, ups=0.11, wpb=1338, bsz=80, num_updates=4980, lr=9.15902e-06, gnorm=0.38, clip=0, loss_scale=128, train_wall=95, gb_free=6.7, wall=83859
2022-06-29 02:21:02 - progress_bar.py[line:274] - INFO: epoch 001:   4997 / 7081 loss=-0, score=1.36, ntokens=1337.1, nsentences=80, sample_size=1337.1, wps=138.7, ups=0.1, wpb=1337.1, bsz=80, num_updates=4990, lr=9.15607e-06, gnorm=0.426, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=83955
slice_id 1 seek offset 2500
2022-06-29 02:22:38 - progress_bar.py[line:274] - INFO: epoch 001:   5007 / 7081 loss=0, score=1.258, ntokens=1318, nsentences=80, sample_size=1318, wps=137, ups=0.1, wpb=1318, bsz=80, num_updates=5000, lr=9.15313e-06, gnorm=0.368, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=84051
2022-06-29 02:22:38 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 0 seek offset 0
2022-06-29 03:19:34 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0 | score 1.159 | ntokens 163.011 | nsentences 10 | sample_size 163.011 | cider 1.182 | wps 119.3 | wpb 163 | bsz 10 | num_updates 5000 | best_cider 1.182
2022-06-29 03:19:34 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5000 updates
2022-06-29 03:19:34 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_5000.pt
2022-06-29 03:19:44 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_5000.pt
2022-06-29 03:21:31 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_5000.pt (epoch 1 @ 5000 updates, score 1.182) (writing took 116.94571144040674 seconds)
2022-06-29 03:23:10 - progress_bar.py[line:274] - INFO: epoch 001:   5017 / 7081 loss=-0, score=1.168, ntokens=1303.1, nsentences=80, sample_size=1303.1, wps=3.6, ups=0, wpb=1303.1, bsz=80, num_updates=5010, lr=9.15018e-06, gnorm=0.535, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=87683
2022-06-29 03:24:46 - progress_bar.py[line:274] - INFO: epoch 001:   5027 / 7081 loss=0, score=1.307, ntokens=1294.8, nsentences=80, sample_size=1294.8, wps=134.4, ups=0.1, wpb=1294.8, bsz=80, num_updates=5020, lr=9.14724e-06, gnorm=0.233, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=87780
2022-06-29 03:26:22 - progress_bar.py[line:274] - INFO: epoch 001:   5037 / 7081 loss=-0.001, score=1.312, ntokens=1295.3, nsentences=80, sample_size=1295.3, wps=134.7, ups=0.1, wpb=1295.3, bsz=80, num_updates=5030, lr=9.14429e-06, gnorm=0.393, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=87876
2022-06-29 03:27:59 - progress_bar.py[line:274] - INFO: epoch 001:   5047 / 7081 loss=0.001, score=1.176, ntokens=1298.6, nsentences=80, sample_size=1298.6, wps=133.8, ups=0.1, wpb=1298.6, bsz=80, num_updates=5040, lr=9.14135e-06, gnorm=0.502, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=87973
2022-06-29 03:29:36 - progress_bar.py[line:274] - INFO: epoch 001:   5057 / 7081 loss=0, score=1.233, ntokens=1301.5, nsentences=80, sample_size=1301.5, wps=135.1, ups=0.1, wpb=1301.5, bsz=80, num_updates=5050, lr=9.1384e-06, gnorm=0.444, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=88069
2022-06-29 03:31:13 - progress_bar.py[line:274] - INFO: epoch 001:   5067 / 7081 loss=0.001, score=1.217, ntokens=1303.5, nsentences=80, sample_size=1303.5, wps=134.4, ups=0.1, wpb=1303.5, bsz=80, num_updates=5060, lr=9.13546e-06, gnorm=0.47, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=88166
2022-06-29 03:32:49 - progress_bar.py[line:274] - INFO: epoch 001:   5077 / 7081 loss=-0.001, score=1.329, ntokens=1286.1, nsentences=80, sample_size=1286.1, wps=133.6, ups=0.1, wpb=1286.1, bsz=80, num_updates=5070, lr=9.13251e-06, gnorm=0.556, clip=20, loss_scale=128, train_wall=96, gb_free=6.7, wall=88263
2022-06-29 03:34:26 - progress_bar.py[line:274] - INFO: epoch 001:   5087 / 7081 loss=0.001, score=1.161, ntokens=1298.2, nsentences=80, sample_size=1298.2, wps=134.5, ups=0.1, wpb=1298.2, bsz=80, num_updates=5080, lr=9.12957e-06, gnorm=0.409, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=88359
2022-06-29 03:36:03 - progress_bar.py[line:274] - INFO: epoch 001:   5097 / 7081 loss=0, score=1.149, ntokens=1290.2, nsentences=80, sample_size=1290.2, wps=132.9, ups=0.1, wpb=1290.2, bsz=80, num_updates=5090, lr=9.12662e-06, gnorm=0.574, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=88456
2022-06-29 03:37:38 - progress_bar.py[line:274] - INFO: epoch 001:   5107 / 7081 loss=-0.002, score=1.238, ntokens=1298.9, nsentences=80, sample_size=1298.9, wps=135.8, ups=0.1, wpb=1298.9, bsz=80, num_updates=5100, lr=9.12368e-06, gnorm=0.319, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=88552
2022-06-29 03:39:15 - progress_bar.py[line:274] - INFO: epoch 001:   5117 / 7081 loss=0, score=1.21, ntokens=1284.1, nsentences=80, sample_size=1284.1, wps=132.8, ups=0.1, wpb=1284.1, bsz=80, num_updates=5110, lr=9.12074e-06, gnorm=0.654, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=88649
2022-06-29 03:40:51 - progress_bar.py[line:274] - INFO: epoch 001:   5127 / 7081 loss=-0.001, score=1.326, ntokens=1292, nsentences=80, sample_size=1292, wps=134.7, ups=0.1, wpb=1292, bsz=80, num_updates=5120, lr=9.11779e-06, gnorm=0.545, clip=20, loss_scale=128, train_wall=96, gb_free=6.7, wall=88744
2022-06-29 03:42:27 - progress_bar.py[line:274] - INFO: epoch 001:   5137 / 7081 loss=0, score=1.304, ntokens=1288.4, nsentences=80, sample_size=1288.4, wps=134.4, ups=0.1, wpb=1288.4, bsz=80, num_updates=5130, lr=9.11485e-06, gnorm=0.256, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=88840
2022-06-29 03:44:03 - progress_bar.py[line:274] - INFO: epoch 001:   5147 / 7081 loss=-0, score=1.241, ntokens=1285.8, nsentences=80, sample_size=1285.8, wps=133.8, ups=0.1, wpb=1285.8, bsz=80, num_updates=5140, lr=9.1119e-06, gnorm=0.165, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=88936
2022-06-29 03:45:40 - progress_bar.py[line:274] - INFO: epoch 001:   5157 / 7081 loss=-0, score=1.085, ntokens=1281.7, nsentences=80, sample_size=1281.7, wps=131.6, ups=0.1, wpb=1281.7, bsz=80, num_updates=5150, lr=9.10896e-06, gnorm=0.367, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=89034
2022-06-29 03:47:17 - progress_bar.py[line:274] - INFO: epoch 001:   5167 / 7081 loss=-0, score=1.234, ntokens=1278.3, nsentences=80, sample_size=1278.3, wps=132.1, ups=0.1, wpb=1278.3, bsz=80, num_updates=5160, lr=9.10601e-06, gnorm=0.313, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=89131
2022-06-29 03:48:54 - progress_bar.py[line:274] - INFO: epoch 001:   5177 / 7081 loss=0, score=1.165, ntokens=1292.9, nsentences=80, sample_size=1292.9, wps=133.2, ups=0.1, wpb=1292.9, bsz=80, num_updates=5170, lr=9.10307e-06, gnorm=0.228, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=89228
2022-06-29 03:50:31 - progress_bar.py[line:274] - INFO: epoch 001:   5187 / 7081 loss=0, score=1.296, ntokens=1300.1, nsentences=80, sample_size=1300.1, wps=133.9, ups=0.1, wpb=1300.1, bsz=80, num_updates=5180, lr=9.10012e-06, gnorm=0.415, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=89325
2022-06-29 03:52:08 - progress_bar.py[line:274] - INFO: epoch 001:   5197 / 7081 loss=-0, score=1.239, ntokens=1299.9, nsentences=80, sample_size=1299.9, wps=134.3, ups=0.1, wpb=1299.9, bsz=80, num_updates=5190, lr=9.09718e-06, gnorm=0.521, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=89422
2022-06-29 03:53:44 - progress_bar.py[line:274] - INFO: epoch 001:   5207 / 7081 loss=0.001, score=1.248, ntokens=1305.7, nsentences=80, sample_size=1305.7, wps=135.7, ups=0.1, wpb=1305.7, bsz=80, num_updates=5200, lr=9.09423e-06, gnorm=0.397, clip=0, loss_scale=256, train_wall=96, gb_free=6.7, wall=89518
2022-06-29 03:55:20 - progress_bar.py[line:274] - INFO: epoch 001:   5217 / 7081 loss=0.001, score=1.232, ntokens=1305.4, nsentences=80, sample_size=1305.4, wps=136.3, ups=0.1, wpb=1305.4, bsz=80, num_updates=5210, lr=9.09129e-06, gnorm=0.448, clip=10, loss_scale=256, train_wall=96, gb_free=6.7, wall=89614
2022-06-29 03:56:58 - progress_bar.py[line:274] - INFO: epoch 001:   5227 / 7081 loss=-0, score=1.335, ntokens=1323, nsentences=80, sample_size=1323, wps=135.7, ups=0.1, wpb=1323, bsz=80, num_updates=5220, lr=9.08834e-06, gnorm=0.42, clip=10, loss_scale=256, train_wall=97, gb_free=6.7, wall=89711
2022-06-29 03:58:34 - progress_bar.py[line:274] - INFO: epoch 001:   5237 / 7081 loss=0, score=1.301, ntokens=1335.9, nsentences=80, sample_size=1335.9, wps=138, ups=0.1, wpb=1335.9, bsz=80, num_updates=5230, lr=9.0854e-06, gnorm=0.272, clip=0, loss_scale=256, train_wall=97, gb_free=6.7, wall=89808
2022-06-29 04:00:11 - progress_bar.py[line:274] - INFO: epoch 001:   5247 / 7081 loss=0, score=1.281, ntokens=1335.3, nsentences=80, sample_size=1335.3, wps=137.7, ups=0.1, wpb=1335.3, bsz=80, num_updates=5240, lr=9.08246e-06, gnorm=0.262, clip=0, loss_scale=256, train_wall=97, gb_free=6.7, wall=89905
2022-06-29 04:01:48 - progress_bar.py[line:274] - INFO: epoch 001:   5257 / 7081 loss=0, score=1.256, ntokens=1339.3, nsentences=80, sample_size=1339.3, wps=138.7, ups=0.1, wpb=1339.3, bsz=80, num_updates=5250, lr=9.07951e-06, gnorm=0.425, clip=0, loss_scale=256, train_wall=96, gb_free=6.7, wall=90001
2022-06-29 04:03:25 - progress_bar.py[line:274] - INFO: epoch 001:   5267 / 7081 loss=0.001, score=1.245, ntokens=1334.4, nsentences=80, sample_size=1334.4, wps=137.2, ups=0.1, wpb=1334.4, bsz=80, num_updates=5260, lr=9.07657e-06, gnorm=0.409, clip=10, loss_scale=256, train_wall=97, gb_free=6.7, wall=90099
2022-06-29 04:05:03 - progress_bar.py[line:274] - INFO: epoch 001:   5277 / 7081 loss=0, score=1.254, ntokens=1344.8, nsentences=80, sample_size=1344.8, wps=137.9, ups=0.1, wpb=1344.8, bsz=80, num_updates=5270, lr=9.07362e-06, gnorm=0.456, clip=20, loss_scale=256, train_wall=97, gb_free=6.7, wall=90196
2022-06-29 04:06:21 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-06-29 04:06:48 - progress_bar.py[line:274] - INFO: epoch 001:   5288 / 7081 loss=-0.002, score=1.238, ntokens=1333.9, nsentences=80, sample_size=1333.9, wps=126.6, ups=0.09, wpb=1333.9, bsz=80, num_updates=5280, lr=9.07068e-06, gnorm=0.735, clip=30, loss_scale=128, train_wall=105, gb_free=6.7, wall=90302
2022-06-29 04:08:24 - progress_bar.py[line:274] - INFO: epoch 001:   5298 / 7081 loss=0.001, score=1.188, ntokens=1346.1, nsentences=80, sample_size=1346.1, wps=139.6, ups=0.1, wpb=1346.1, bsz=80, num_updates=5290, lr=9.06773e-06, gnorm=0.337, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=90398
2022-06-29 04:10:02 - progress_bar.py[line:274] - INFO: epoch 001:   5308 / 7081 loss=0, score=1.262, ntokens=1344.1, nsentences=80, sample_size=1344.1, wps=138.3, ups=0.1, wpb=1344.1, bsz=80, num_updates=5300, lr=9.06479e-06, gnorm=0.393, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=90495
2022-06-29 04:11:38 - progress_bar.py[line:274] - INFO: epoch 001:   5318 / 7081 loss=0, score=1.242, ntokens=1341.7, nsentences=80, sample_size=1341.7, wps=138.6, ups=0.1, wpb=1341.7, bsz=80, num_updates=5310, lr=9.06184e-06, gnorm=0.328, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=90592
2022-06-29 04:13:15 - progress_bar.py[line:274] - INFO: epoch 001:   5328 / 7081 loss=-0, score=1.25, ntokens=1347.8, nsentences=80, sample_size=1347.8, wps=140.1, ups=0.1, wpb=1347.8, bsz=80, num_updates=5320, lr=9.0589e-06, gnorm=0.247, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=90688
2022-06-29 04:14:51 - progress_bar.py[line:274] - INFO: epoch 001:   5338 / 7081 loss=-0, score=1.194, ntokens=1344.5, nsentences=80, sample_size=1344.5, wps=139.6, ups=0.1, wpb=1344.5, bsz=80, num_updates=5330, lr=9.05595e-06, gnorm=0.382, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=90784
2022-06-29 04:16:28 - progress_bar.py[line:274] - INFO: epoch 001:   5348 / 7081 loss=-0, score=1.232, ntokens=1345.2, nsentences=80, sample_size=1345.2, wps=139, ups=0.1, wpb=1345.2, bsz=80, num_updates=5340, lr=9.05301e-06, gnorm=0.46, clip=20, loss_scale=128, train_wall=97, gb_free=6.7, wall=90881
2022-06-29 04:18:04 - progress_bar.py[line:274] - INFO: epoch 001:   5358 / 7081 loss=0.001, score=1.163, ntokens=1347.1, nsentences=80, sample_size=1347.1, wps=139.8, ups=0.1, wpb=1347.1, bsz=80, num_updates=5350, lr=9.05006e-06, gnorm=0.498, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=90978
2022-06-29 04:19:41 - progress_bar.py[line:274] - INFO: epoch 001:   5368 / 7081 loss=-0.002, score=1.173, ntokens=1334.5, nsentences=80, sample_size=1334.5, wps=137.5, ups=0.1, wpb=1334.5, bsz=80, num_updates=5360, lr=9.04712e-06, gnorm=0.347, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=91075
2022-06-29 04:21:18 - progress_bar.py[line:274] - INFO: epoch 001:   5378 / 7081 loss=-0, score=1.165, ntokens=1332.7, nsentences=80, sample_size=1332.7, wps=137.6, ups=0.1, wpb=1332.7, bsz=80, num_updates=5370, lr=9.04418e-06, gnorm=0.222, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=91172
2022-06-29 04:22:55 - progress_bar.py[line:274] - INFO: epoch 001:   5388 / 7081 loss=0, score=1.246, ntokens=1328.2, nsentences=80, sample_size=1328.2, wps=136.5, ups=0.1, wpb=1328.2, bsz=80, num_updates=5380, lr=9.04123e-06, gnorm=0.266, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=91269
2022-06-29 04:24:32 - progress_bar.py[line:274] - INFO: epoch 001:   5398 / 7081 loss=0.001, score=1.268, ntokens=1322.8, nsentences=80, sample_size=1322.8, wps=136.4, ups=0.1, wpb=1322.8, bsz=80, num_updates=5390, lr=9.03829e-06, gnorm=0.32, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=91366
2022-06-29 04:26:09 - progress_bar.py[line:274] - INFO: epoch 001:   5408 / 7081 loss=-0.001, score=1.233, ntokens=1330.7, nsentences=80, sample_size=1330.7, wps=137.2, ups=0.1, wpb=1330.7, bsz=80, num_updates=5400, lr=9.03534e-06, gnorm=0.398, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=91463
2022-06-29 04:27:46 - progress_bar.py[line:274] - INFO: epoch 001:   5418 / 7081 loss=0, score=1.272, ntokens=1338.3, nsentences=80, sample_size=1338.3, wps=138.4, ups=0.1, wpb=1338.3, bsz=80, num_updates=5410, lr=9.0324e-06, gnorm=0.564, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=91560
2022-06-29 04:29:23 - progress_bar.py[line:274] - INFO: epoch 001:   5428 / 7081 loss=-0.001, score=1.196, ntokens=1319.6, nsentences=80, sample_size=1319.6, wps=136.2, ups=0.1, wpb=1319.6, bsz=80, num_updates=5420, lr=9.02945e-06, gnorm=0.57, clip=30, loss_scale=128, train_wall=97, gb_free=6.7, wall=91656
2022-06-29 04:31:00 - progress_bar.py[line:274] - INFO: epoch 001:   5438 / 7081 loss=0, score=1.272, ntokens=1322.2, nsentences=80, sample_size=1322.2, wps=136.7, ups=0.1, wpb=1322.2, bsz=80, num_updates=5430, lr=9.02651e-06, gnorm=0.376, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=91753
2022-06-29 04:32:36 - progress_bar.py[line:274] - INFO: epoch 001:   5448 / 7081 loss=-0, score=1.375, ntokens=1310.6, nsentences=80, sample_size=1310.6, wps=136.1, ups=0.1, wpb=1310.6, bsz=80, num_updates=5440, lr=9.02356e-06, gnorm=0.356, clip=0, loss_scale=128, train_wall=96, gb_free=6.7, wall=91849
2022-06-29 04:34:13 - progress_bar.py[line:274] - INFO: epoch 001:   5458 / 7081 loss=0, score=1.213, ntokens=1308.2, nsentences=80, sample_size=1308.2, wps=135.1, ups=0.1, wpb=1308.2, bsz=80, num_updates=5450, lr=9.02062e-06, gnorm=0.245, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=91946
2022-06-29 04:35:49 - progress_bar.py[line:274] - INFO: epoch 001:   5468 / 7081 loss=-0.002, score=1.125, ntokens=1301.8, nsentences=80, sample_size=1301.8, wps=134.8, ups=0.1, wpb=1301.8, bsz=80, num_updates=5460, lr=9.01767e-06, gnorm=0.566, clip=10, loss_scale=128, train_wall=96, gb_free=6.7, wall=92043
2022-06-29 04:37:26 - progress_bar.py[line:274] - INFO: epoch 001:   5478 / 7081 loss=-0, score=1.33, ntokens=1300.4, nsentences=80, sample_size=1300.4, wps=134.4, ups=0.1, wpb=1300.4, bsz=80, num_updates=5470, lr=9.01473e-06, gnorm=0.218, clip=0, loss_scale=128, train_wall=97, gb_free=6.7, wall=92140
2022-06-29 04:39:03 - progress_bar.py[line:274] - INFO: epoch 001:   5488 / 7081 loss=0.001, score=1.227, ntokens=1299.5, nsentences=80, sample_size=1299.5, wps=134.3, ups=0.1, wpb=1299.5, bsz=80, num_updates=5480, lr=9.01178e-06, gnorm=0.402, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=92236
2022-06-29 04:40:40 - progress_bar.py[line:274] - INFO: epoch 001:   5498 / 7081 loss=0, score=1.205, ntokens=1300.8, nsentences=80, sample_size=1300.8, wps=134.3, ups=0.1, wpb=1300.8, bsz=80, num_updates=5490, lr=9.00884e-06, gnorm=0.319, clip=10, loss_scale=128, train_wall=97, gb_free=6.7, wall=92333
2022-06-29 04:40:57 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-29 04:42:25 - progress_bar.py[line:274] - INFO: epoch 001:   5509 / 7081 loss=0, score=1.216, ntokens=1305.9, nsentences=80, sample_size=1305.9, wps=123.5, ups=0.09, wpb=1305.9, bsz=80, num_updates=5500, lr=9.0059e-06, gnorm=0.167, clip=0, loss_scale=64, train_wall=106, gb_free=6.7, wall=92439
slice_id 1 seek offset 2500
2022-06-29 04:42:25 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-29 05:39:30 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0 | score 1.158 | ntokens 163.894 | nsentences 10 | sample_size 163.894 | cider 1.179 | wps 119.6 | wpb 163.9 | bsz 10 | num_updates 5500 | best_cider 1.182
2022-06-29 05:39:30 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5500 updates
2022-06-29 05:39:30 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_5500.pt
2022-06-29 05:39:39 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_5500.pt
2022-06-29 05:40:41 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_5500.pt (epoch 1 @ 5500 updates, score 1.179) (writing took 71.16323130019009 seconds)
2022-06-29 05:42:17 - progress_bar.py[line:274] - INFO: epoch 001:   5519 / 7081 loss=-0.003, score=1.287, ntokens=1302.6, nsentences=80, sample_size=1302.6, wps=3.6, ups=0, wpb=1302.6, bsz=80, num_updates=5510, lr=9.00295e-06, gnorm=0.638, clip=10, loss_scale=64, train_wall=96, gb_free=6.7, wall=96031
2022-06-29 05:43:54 - progress_bar.py[line:274] - INFO: epoch 001:   5529 / 7081 loss=-0.001, score=1.261, ntokens=1285, nsentences=80, sample_size=1285, wps=132.7, ups=0.1, wpb=1285, bsz=80, num_updates=5520, lr=9.00001e-06, gnorm=0.178, clip=0, loss_scale=64, train_wall=97, gb_free=6.7, wall=96128
2022-06-29 05:45:30 - progress_bar.py[line:274] - INFO: epoch 001:   5539 / 7081 loss=0.002, score=1.261, ntokens=1263.6, nsentences=80, sample_size=1263.6, wps=132.2, ups=0.1, wpb=1263.6, bsz=80, num_updates=5530, lr=8.99706e-06, gnorm=0.349, clip=0, loss_scale=64, train_wall=95, gb_free=6.7, wall=96223
2022-06-29 05:47:01 - progress_bar.py[line:274] - INFO: epoch 001:   5549 / 7081 loss=0.001, score=1.184, ntokens=1261.4, nsentences=80, sample_size=1261.4, wps=138.5, ups=0.11, wpb=1261.4, bsz=80, num_updates=5540, lr=8.99412e-06, gnorm=0.519, clip=10, loss_scale=64, train_wall=91, gb_free=6.7, wall=96314
2022-06-29 05:48:25 - progress_bar.py[line:274] - INFO: epoch 001:   5559 / 7081 loss=0.002, score=1.273, ntokens=1242.2, nsentences=80, sample_size=1242.2, wps=147.9, ups=0.12, wpb=1242.2, bsz=80, num_updates=5550, lr=8.99117e-06, gnorm=0.479, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=96398
2022-06-29 05:49:49 - progress_bar.py[line:274] - INFO: epoch 001:   5569 / 7081 loss=0.001, score=1.248, ntokens=1241, nsentences=80, sample_size=1241, wps=147.8, ups=0.12, wpb=1241, bsz=80, num_updates=5560, lr=8.98823e-06, gnorm=0.303, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=96482
2022-06-29 05:51:13 - progress_bar.py[line:274] - INFO: epoch 001:   5579 / 7081 loss=0, score=1.19, ntokens=1244.4, nsentences=80, sample_size=1244.4, wps=148.2, ups=0.12, wpb=1244.4, bsz=80, num_updates=5570, lr=8.98528e-06, gnorm=0.416, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=96566
2022-06-29 05:52:37 - progress_bar.py[line:274] - INFO: epoch 001:   5589 / 7081 loss=-0.001, score=1.29, ntokens=1230.3, nsentences=80, sample_size=1230.3, wps=146.6, ups=0.12, wpb=1230.3, bsz=80, num_updates=5580, lr=8.98234e-06, gnorm=0.441, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=96650
2022-06-29 05:54:01 - progress_bar.py[line:274] - INFO: epoch 001:   5599 / 7081 loss=-0.003, score=1.368, ntokens=1270, nsentences=80, sample_size=1270, wps=151.1, ups=0.12, wpb=1270, bsz=80, num_updates=5590, lr=8.97939e-06, gnorm=0.414, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=96734
2022-06-29 05:55:24 - progress_bar.py[line:274] - INFO: epoch 001:   5609 / 7081 loss=-0.002, score=1.245, ntokens=1256.7, nsentences=80, sample_size=1256.7, wps=150.6, ups=0.12, wpb=1256.7, bsz=80, num_updates=5600, lr=8.97645e-06, gnorm=0.887, clip=20, loss_scale=64, train_wall=83, gb_free=6.7, wall=96818
2022-06-29 05:56:48 - progress_bar.py[line:274] - INFO: epoch 001:   5619 / 7081 loss=-0.003, score=1.15, ntokens=1286, nsentences=80, sample_size=1286, wps=153.5, ups=0.12, wpb=1286, bsz=80, num_updates=5610, lr=8.9735e-06, gnorm=0.575, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=96901
2022-06-29 05:58:11 - progress_bar.py[line:274] - INFO: epoch 001:   5629 / 7081 loss=-0.001, score=1.258, ntokens=1246.4, nsentences=80, sample_size=1246.4, wps=149.3, ups=0.12, wpb=1246.4, bsz=80, num_updates=5620, lr=8.97056e-06, gnorm=0.301, clip=0, loss_scale=64, train_wall=83, gb_free=6.7, wall=96985
2022-06-29 05:59:34 - progress_bar.py[line:274] - INFO: epoch 001:   5639 / 7081 loss=0, score=1.339, ntokens=1226.3, nsentences=80, sample_size=1226.3, wps=147.6, ups=0.12, wpb=1226.3, bsz=80, num_updates=5630, lr=8.96762e-06, gnorm=0.386, clip=0, loss_scale=64, train_wall=83, gb_free=6.7, wall=97068
2022-06-29 06:00:57 - progress_bar.py[line:274] - INFO: epoch 001:   5649 / 7081 loss=-0, score=1.188, ntokens=1218.2, nsentences=80, sample_size=1218.2, wps=146.6, ups=0.12, wpb=1218.2, bsz=80, num_updates=5640, lr=8.96467e-06, gnorm=0.494, clip=20, loss_scale=64, train_wall=83, gb_free=6.7, wall=97151
2022-06-29 06:02:21 - progress_bar.py[line:274] - INFO: epoch 001:   5659 / 7081 loss=0.001, score=1.283, ntokens=1228.3, nsentences=80, sample_size=1228.3, wps=147.7, ups=0.12, wpb=1228.3, bsz=80, num_updates=5650, lr=8.96173e-06, gnorm=0.311, clip=0, loss_scale=64, train_wall=83, gb_free=6.7, wall=97234
2022-06-29 06:03:43 - progress_bar.py[line:274] - INFO: epoch 001:   5669 / 7081 loss=0.003, score=1.132, ntokens=1230.6, nsentences=80, sample_size=1230.6, wps=149.1, ups=0.12, wpb=1230.6, bsz=80, num_updates=5660, lr=8.95878e-06, gnorm=0.439, clip=10, loss_scale=64, train_wall=82, gb_free=6.7, wall=97317
2022-06-29 06:05:07 - progress_bar.py[line:274] - INFO: epoch 001:   5679 / 7081 loss=0.001, score=1.287, ntokens=1247.1, nsentences=80, sample_size=1247.1, wps=149.3, ups=0.12, wpb=1247.1, bsz=80, num_updates=5670, lr=8.95584e-06, gnorm=0.467, clip=10, loss_scale=64, train_wall=83, gb_free=6.7, wall=97400
2022-06-29 06:06:30 - progress_bar.py[line:274] - INFO: epoch 001:   5689 / 7081 loss=0.003, score=1.268, ntokens=1241.5, nsentences=80, sample_size=1241.5, wps=149.2, ups=0.12, wpb=1241.5, bsz=80, num_updates=5680, lr=8.95289e-06, gnorm=0.514, clip=10, loss_scale=64, train_wall=83, gb_free=6.7, wall=97484
2022-06-29 06:07:54 - progress_bar.py[line:274] - INFO: epoch 001:   5699 / 7081 loss=-0.001, score=1.142, ntokens=1264.2, nsentences=80, sample_size=1264.2, wps=150.7, ups=0.12, wpb=1264.2, bsz=80, num_updates=5690, lr=8.94995e-06, gnorm=0.572, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=97567
2022-06-29 06:09:17 - progress_bar.py[line:274] - INFO: epoch 001:   5709 / 7081 loss=-0.001, score=1.239, ntokens=1249.3, nsentences=80, sample_size=1249.3, wps=149.6, ups=0.12, wpb=1249.3, bsz=80, num_updates=5700, lr=8.947e-06, gnorm=0.276, clip=0, loss_scale=64, train_wall=83, gb_free=6.7, wall=97651
2022-06-29 06:10:41 - progress_bar.py[line:274] - INFO: epoch 001:   5719 / 7081 loss=0, score=1.181, ntokens=1251.5, nsentences=80, sample_size=1251.5, wps=150.4, ups=0.12, wpb=1251.5, bsz=80, num_updates=5710, lr=8.94406e-06, gnorm=0.393, clip=0, loss_scale=64, train_wall=83, gb_free=6.7, wall=97734
2022-06-29 06:12:05 - progress_bar.py[line:274] - INFO: epoch 001:   5729 / 7081 loss=-0.001, score=1.26, ntokens=1251.4, nsentences=80, sample_size=1251.4, wps=149, ups=0.12, wpb=1251.4, bsz=80, num_updates=5720, lr=8.94111e-06, gnorm=0.432, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=97818
2022-06-29 06:13:28 - progress_bar.py[line:274] - INFO: epoch 001:   5739 / 7081 loss=-0, score=1.177, ntokens=1275.7, nsentences=80, sample_size=1275.7, wps=152.7, ups=0.12, wpb=1275.7, bsz=80, num_updates=5730, lr=8.93817e-06, gnorm=0.361, clip=10, loss_scale=64, train_wall=83, gb_free=6.7, wall=97902
2022-06-29 06:14:52 - progress_bar.py[line:274] - INFO: epoch 001:   5749 / 7081 loss=-0.001, score=1.211, ntokens=1293.5, nsentences=80, sample_size=1293.5, wps=154.1, ups=0.12, wpb=1293.5, bsz=80, num_updates=5740, lr=8.93522e-06, gnorm=0.33, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=97986
2022-06-29 06:16:16 - progress_bar.py[line:274] - INFO: epoch 001:   5759 / 7081 loss=-0.001, score=1.233, ntokens=1278, nsentences=80, sample_size=1278, wps=152, ups=0.12, wpb=1278, bsz=80, num_updates=5750, lr=8.93228e-06, gnorm=0.397, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=98070
2022-06-29 06:17:40 - progress_bar.py[line:274] - INFO: epoch 001:   5769 / 7081 loss=0.002, score=1.318, ntokens=1288.8, nsentences=80, sample_size=1288.8, wps=152.8, ups=0.12, wpb=1288.8, bsz=80, num_updates=5760, lr=8.92934e-06, gnorm=0.199, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=98154
2022-06-29 06:19:04 - progress_bar.py[line:274] - INFO: epoch 001:   5779 / 7081 loss=0.002, score=1.23, ntokens=1294.4, nsentences=80, sample_size=1294.4, wps=154.7, ups=0.12, wpb=1294.4, bsz=80, num_updates=5770, lr=8.92639e-06, gnorm=0.474, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=98238
2022-06-29 06:20:28 - progress_bar.py[line:274] - INFO: epoch 001:   5789 / 7081 loss=0.001, score=1.218, ntokens=1319.4, nsentences=80, sample_size=1319.4, wps=156.6, ups=0.12, wpb=1319.4, bsz=80, num_updates=5780, lr=8.92345e-06, gnorm=0.17, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=98322
2022-06-29 06:21:52 - progress_bar.py[line:274] - INFO: epoch 001:   5799 / 7081 loss=-0, score=1.225, ntokens=1307.8, nsentences=80, sample_size=1307.8, wps=156.1, ups=0.12, wpb=1307.8, bsz=80, num_updates=5790, lr=8.9205e-06, gnorm=0.385, clip=20, loss_scale=64, train_wall=84, gb_free=6.7, wall=98406
2022-06-29 06:23:16 - progress_bar.py[line:274] - INFO: epoch 001:   5809 / 7081 loss=0.001, score=1.351, ntokens=1293.5, nsentences=80, sample_size=1293.5, wps=153.9, ups=0.12, wpb=1293.5, bsz=80, num_updates=5800, lr=8.91756e-06, gnorm=0.423, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=98490
2022-06-29 06:24:40 - progress_bar.py[line:274] - INFO: epoch 001:   5819 / 7081 loss=-0, score=1.318, ntokens=1289.4, nsentences=80, sample_size=1289.4, wps=153.5, ups=0.12, wpb=1289.4, bsz=80, num_updates=5810, lr=8.91461e-06, gnorm=0.347, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=98574
2022-06-29 06:26:04 - progress_bar.py[line:274] - INFO: epoch 001:   5829 / 7081 loss=-0.001, score=1.147, ntokens=1278.5, nsentences=80, sample_size=1278.5, wps=152.3, ups=0.12, wpb=1278.5, bsz=80, num_updates=5820, lr=8.91167e-06, gnorm=0.66, clip=30, loss_scale=64, train_wall=84, gb_free=6.7, wall=98658
2022-06-29 06:27:28 - progress_bar.py[line:274] - INFO: epoch 001:   5839 / 7081 loss=-0.002, score=1.196, ntokens=1286.6, nsentences=80, sample_size=1286.6, wps=153, ups=0.12, wpb=1286.6, bsz=80, num_updates=5830, lr=8.90872e-06, gnorm=0.384, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=98742
2022-06-29 06:28:52 - progress_bar.py[line:274] - INFO: epoch 001:   5849 / 7081 loss=0.001, score=1.198, ntokens=1292.5, nsentences=80, sample_size=1292.5, wps=154, ups=0.12, wpb=1292.5, bsz=80, num_updates=5840, lr=8.90578e-06, gnorm=0.423, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=98826
2022-06-29 06:30:16 - progress_bar.py[line:274] - INFO: epoch 001:   5859 / 7081 loss=0.001, score=1.295, ntokens=1312.3, nsentences=80, sample_size=1312.3, wps=156.5, ups=0.12, wpb=1312.3, bsz=80, num_updates=5850, lr=8.90283e-06, gnorm=0.24, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=98910
2022-06-29 06:31:40 - progress_bar.py[line:274] - INFO: epoch 001:   5869 / 7081 loss=-0.001, score=1.204, ntokens=1299, nsentences=80, sample_size=1299, wps=154.7, ups=0.12, wpb=1299, bsz=80, num_updates=5860, lr=8.89989e-06, gnorm=0.31, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=98994
2022-06-29 06:33:04 - progress_bar.py[line:274] - INFO: epoch 001:   5879 / 7081 loss=-0.001, score=1.233, ntokens=1320, nsentences=80, sample_size=1320, wps=157.2, ups=0.12, wpb=1320, bsz=80, num_updates=5870, lr=8.89694e-06, gnorm=0.366, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=99078
2022-06-29 06:34:28 - progress_bar.py[line:274] - INFO: epoch 001:   5889 / 7081 loss=0.001, score=1.161, ntokens=1307.8, nsentences=80, sample_size=1307.8, wps=155.4, ups=0.12, wpb=1307.8, bsz=80, num_updates=5880, lr=8.894e-06, gnorm=0.498, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=99162
2022-06-29 06:35:52 - progress_bar.py[line:274] - INFO: epoch 001:   5899 / 7081 loss=-0.002, score=1.227, ntokens=1306.4, nsentences=80, sample_size=1306.4, wps=155.6, ups=0.12, wpb=1306.4, bsz=80, num_updates=5890, lr=8.89105e-06, gnorm=0.555, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=99246
2022-06-29 06:37:16 - progress_bar.py[line:274] - INFO: epoch 001:   5909 / 7081 loss=0, score=1.251, ntokens=1316.4, nsentences=80, sample_size=1316.4, wps=156.7, ups=0.12, wpb=1316.4, bsz=80, num_updates=5900, lr=8.88811e-06, gnorm=0.589, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=99330
2022-06-29 06:38:40 - progress_bar.py[line:274] - INFO: epoch 001:   5919 / 7081 loss=-0.001, score=1.325, ntokens=1309.8, nsentences=80, sample_size=1309.8, wps=156.3, ups=0.12, wpb=1309.8, bsz=80, num_updates=5910, lr=8.88517e-06, gnorm=0.511, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=99413
2022-06-29 06:40:04 - progress_bar.py[line:274] - INFO: epoch 001:   5929 / 7081 loss=-0.001, score=1.266, ntokens=1331.9, nsentences=80, sample_size=1331.9, wps=158.7, ups=0.12, wpb=1331.9, bsz=80, num_updates=5920, lr=8.88222e-06, gnorm=0.35, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=99497
2022-06-29 06:41:28 - progress_bar.py[line:274] - INFO: epoch 001:   5939 / 7081 loss=-0, score=1.239, ntokens=1314.1, nsentences=80, sample_size=1314.1, wps=156.7, ups=0.12, wpb=1314.1, bsz=80, num_updates=5930, lr=8.87928e-06, gnorm=0.488, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=99581
2022-06-29 06:42:52 - progress_bar.py[line:274] - INFO: epoch 001:   5949 / 7081 loss=0.001, score=1.23, ntokens=1309.5, nsentences=80, sample_size=1309.5, wps=155.9, ups=0.12, wpb=1309.5, bsz=80, num_updates=5940, lr=8.87633e-06, gnorm=0.352, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=99665
2022-06-29 06:44:16 - progress_bar.py[line:274] - INFO: epoch 001:   5959 / 7081 loss=-0.002, score=1.286, ntokens=1305.9, nsentences=80, sample_size=1305.9, wps=155.1, ups=0.12, wpb=1305.9, bsz=80, num_updates=5950, lr=8.87339e-06, gnorm=0.369, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=99749
2022-06-29 06:45:40 - progress_bar.py[line:274] - INFO: epoch 001:   5969 / 7081 loss=-0.001, score=1.278, ntokens=1303.1, nsentences=80, sample_size=1303.1, wps=155.5, ups=0.12, wpb=1303.1, bsz=80, num_updates=5960, lr=8.87044e-06, gnorm=0.264, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=99833
2022-06-29 06:47:03 - progress_bar.py[line:274] - INFO: epoch 001:   5979 / 7081 loss=-0.003, score=1.216, ntokens=1287.1, nsentences=80, sample_size=1287.1, wps=153.9, ups=0.12, wpb=1287.1, bsz=80, num_updates=5970, lr=8.8675e-06, gnorm=0.408, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=99917
2022-06-29 06:48:27 - progress_bar.py[line:274] - INFO: epoch 001:   5989 / 7081 loss=-0.001, score=1.138, ntokens=1271.9, nsentences=80, sample_size=1271.9, wps=151.8, ups=0.12, wpb=1271.9, bsz=80, num_updates=5980, lr=8.86455e-06, gnorm=0.647, clip=20, loss_scale=64, train_wall=84, gb_free=6.7, wall=100001
2022-06-29 06:49:51 - progress_bar.py[line:274] - INFO: epoch 001:   5999 / 7081 loss=-0.001, score=1.179, ntokens=1250.4, nsentences=80, sample_size=1250.4, wps=149.8, ups=0.12, wpb=1250.4, bsz=80, num_updates=5990, lr=8.86161e-06, gnorm=0.477, clip=20, loss_scale=64, train_wall=83, gb_free=6.7, wall=100084
2022-06-29 06:51:14 - progress_bar.py[line:274] - INFO: epoch 001:   6009 / 7081 loss=0.001, score=1.158, ntokens=1250.8, nsentences=80, sample_size=1250.8, wps=150.6, ups=0.12, wpb=1250.8, bsz=80, num_updates=6000, lr=8.85866e-06, gnorm=0.36, clip=10, loss_scale=64, train_wall=83, gb_free=6.7, wall=100167
slice_id 1 seek offset 2500
2022-06-29 06:51:14 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-29 07:37:38 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0 | score 1.156 | ntokens 155.37 | nsentences 10 | sample_size 155.37 | cider 1.179 | wps 139.5 | wpb 155.4 | bsz 10 | num_updates 6000 | best_cider 1.182
2022-06-29 07:37:38 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2022-06-29 07:37:38 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_6000.pt
2022-06-29 07:37:48 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_6000.pt
2022-06-29 07:38:46 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 1.179) (writing took 67.55711735319346 seconds)
2022-06-29 07:40:09 - progress_bar.py[line:274] - INFO: epoch 001:   6019 / 7081 loss=0.001, score=1.199, ntokens=1244.6, nsentences=80, sample_size=1244.6, wps=4.2, ups=0, wpb=1244.6, bsz=80, num_updates=6010, lr=8.85572e-06, gnorm=0.4, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=103103
2022-06-29 07:41:33 - progress_bar.py[line:274] - INFO: epoch 001:   6029 / 7081 loss=-0, score=1.206, ntokens=1260.3, nsentences=80, sample_size=1260.3, wps=150.5, ups=0.12, wpb=1260.3, bsz=80, num_updates=6020, lr=8.85277e-06, gnorm=0.26, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=103186
2022-06-29 07:42:56 - progress_bar.py[line:274] - INFO: epoch 001:   6039 / 7081 loss=0, score=1.117, ntokens=1245.3, nsentences=80, sample_size=1245.3, wps=149, ups=0.12, wpb=1245.3, bsz=80, num_updates=6030, lr=8.84983e-06, gnorm=0.573, clip=20, loss_scale=128, train_wall=83, gb_free=6.7, wall=103270
2022-06-29 07:44:20 - progress_bar.py[line:274] - INFO: epoch 001:   6049 / 7081 loss=-0, score=1.363, ntokens=1254.4, nsentences=80, sample_size=1254.4, wps=149.5, ups=0.12, wpb=1254.4, bsz=80, num_updates=6040, lr=8.84689e-06, gnorm=0.474, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=103354
2022-06-29 07:45:44 - progress_bar.py[line:274] - INFO: epoch 001:   6059 / 7081 loss=0.002, score=1.329, ntokens=1262.6, nsentences=80, sample_size=1262.6, wps=151, ups=0.12, wpb=1262.6, bsz=80, num_updates=6050, lr=8.84394e-06, gnorm=0.684, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=103437
2022-06-29 07:47:08 - progress_bar.py[line:274] - INFO: epoch 001:   6069 / 7081 loss=-0, score=1.232, ntokens=1246.3, nsentences=80, sample_size=1246.3, wps=148.7, ups=0.12, wpb=1246.3, bsz=80, num_updates=6060, lr=8.841e-06, gnorm=0.293, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=103521
2022-06-29 07:48:31 - progress_bar.py[line:274] - INFO: epoch 001:   6079 / 7081 loss=-0, score=1.311, ntokens=1238.8, nsentences=80, sample_size=1238.8, wps=148.5, ups=0.12, wpb=1238.8, bsz=80, num_updates=6070, lr=8.83805e-06, gnorm=0.248, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=103605
2022-06-29 07:49:54 - progress_bar.py[line:274] - INFO: epoch 001:   6089 / 7081 loss=-0.001, score=1.188, ntokens=1224.5, nsentences=80, sample_size=1224.5, wps=147.8, ups=0.12, wpb=1224.5, bsz=80, num_updates=6080, lr=8.83511e-06, gnorm=0.387, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=103688
2022-06-29 07:51:17 - progress_bar.py[line:274] - INFO: epoch 001:   6099 / 7081 loss=-0, score=1.309, ntokens=1227.8, nsentences=80, sample_size=1227.8, wps=147.7, ups=0.12, wpb=1227.8, bsz=80, num_updates=6090, lr=8.83216e-06, gnorm=0.602, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=103771
2022-06-29 07:52:40 - progress_bar.py[line:274] - INFO: epoch 001:   6109 / 7081 loss=0, score=1.204, ntokens=1219.6, nsentences=80, sample_size=1219.6, wps=146.3, ups=0.12, wpb=1219.6, bsz=80, num_updates=6100, lr=8.82922e-06, gnorm=0.593, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=103854
2022-06-29 07:54:04 - progress_bar.py[line:274] - INFO: epoch 001:   6119 / 7081 loss=-0.001, score=1.272, ntokens=1218.4, nsentences=80, sample_size=1218.4, wps=146.2, ups=0.12, wpb=1218.4, bsz=80, num_updates=6110, lr=8.82627e-06, gnorm=0.518, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=103937
2022-06-29 07:55:27 - progress_bar.py[line:274] - INFO: epoch 001:   6129 / 7081 loss=-0.001, score=1.259, ntokens=1237.2, nsentences=80, sample_size=1237.2, wps=148.2, ups=0.12, wpb=1237.2, bsz=80, num_updates=6120, lr=8.82333e-06, gnorm=0.592, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=104021
2022-06-29 07:56:51 - progress_bar.py[line:274] - INFO: epoch 001:   6139 / 7081 loss=0.003, score=1.284, ntokens=1236.4, nsentences=80, sample_size=1236.4, wps=148.4, ups=0.12, wpb=1236.4, bsz=80, num_updates=6130, lr=8.82038e-06, gnorm=0.582, clip=20, loss_scale=128, train_wall=83, gb_free=6.7, wall=104104
2022-06-29 07:58:14 - progress_bar.py[line:274] - INFO: epoch 001:   6149 / 7081 loss=0, score=1.117, ntokens=1239.8, nsentences=80, sample_size=1239.8, wps=148.2, ups=0.12, wpb=1239.8, bsz=80, num_updates=6140, lr=8.81744e-06, gnorm=0.563, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=104188
2022-06-29 07:59:38 - progress_bar.py[line:274] - INFO: epoch 001:   6159 / 7081 loss=0.003, score=1.21, ntokens=1225, nsentences=80, sample_size=1225, wps=146.2, ups=0.12, wpb=1225, bsz=80, num_updates=6150, lr=8.81449e-06, gnorm=0.523, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=104272
2022-06-29 08:01:03 - progress_bar.py[line:274] - INFO: epoch 001:   6169 / 7081 loss=0.002, score=1.243, ntokens=1230.9, nsentences=80, sample_size=1230.9, wps=145.7, ups=0.12, wpb=1230.9, bsz=80, num_updates=6160, lr=8.81155e-06, gnorm=0.723, clip=30, loss_scale=128, train_wall=84, gb_free=6.7, wall=104356
2022-06-29 08:02:26 - progress_bar.py[line:274] - INFO: epoch 001:   6179 / 7081 loss=0.001, score=1.266, ntokens=1220.9, nsentences=80, sample_size=1220.9, wps=145.7, ups=0.12, wpb=1220.9, bsz=80, num_updates=6170, lr=8.80861e-06, gnorm=0.546, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=104440
2022-06-29 08:03:51 - progress_bar.py[line:274] - INFO: epoch 001:   6189 / 7081 loss=0.002, score=1.243, ntokens=1227.3, nsentences=80, sample_size=1227.3, wps=145.7, ups=0.12, wpb=1227.3, bsz=80, num_updates=6180, lr=8.80566e-06, gnorm=0.517, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=104524
2022-06-29 08:05:15 - progress_bar.py[line:274] - INFO: epoch 001:   6199 / 7081 loss=-0.001, score=1.242, ntokens=1270.2, nsentences=80, sample_size=1270.2, wps=150.9, ups=0.12, wpb=1270.2, bsz=80, num_updates=6190, lr=8.80272e-06, gnorm=0.344, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=104608
2022-06-29 08:06:39 - progress_bar.py[line:274] - INFO: epoch 001:   6209 / 7081 loss=0.002, score=1.273, ntokens=1280.4, nsentences=80, sample_size=1280.4, wps=152.3, ups=0.12, wpb=1280.4, bsz=80, num_updates=6200, lr=8.79977e-06, gnorm=0.477, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=104692
2022-06-29 08:08:03 - progress_bar.py[line:274] - INFO: epoch 001:   6219 / 7081 loss=-0, score=1.221, ntokens=1298.9, nsentences=80, sample_size=1298.9, wps=154.4, ups=0.12, wpb=1298.9, bsz=80, num_updates=6210, lr=8.79683e-06, gnorm=0.358, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=104776
2022-06-29 08:09:27 - progress_bar.py[line:274] - INFO: epoch 001:   6229 / 7081 loss=0.001, score=1.082, ntokens=1303.2, nsentences=80, sample_size=1303.2, wps=154.6, ups=0.12, wpb=1303.2, bsz=80, num_updates=6220, lr=8.79388e-06, gnorm=0.512, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=104861
2022-06-29 08:10:52 - progress_bar.py[line:274] - INFO: epoch 001:   6239 / 7081 loss=-0.001, score=1.222, ntokens=1302.8, nsentences=80, sample_size=1302.8, wps=153.9, ups=0.12, wpb=1302.8, bsz=80, num_updates=6230, lr=8.79094e-06, gnorm=0.591, clip=10, loss_scale=128, train_wall=85, gb_free=6.7, wall=104945
2022-06-29 08:12:16 - progress_bar.py[line:274] - INFO: epoch 001:   6249 / 7081 loss=0.001, score=1.244, ntokens=1305.5, nsentences=80, sample_size=1305.5, wps=155.1, ups=0.12, wpb=1305.5, bsz=80, num_updates=6240, lr=8.78799e-06, gnorm=0.634, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=105030
2022-06-29 08:13:40 - progress_bar.py[line:274] - INFO: epoch 001:   6259 / 7081 loss=0.001, score=1.38, ntokens=1305.8, nsentences=80, sample_size=1305.8, wps=155.5, ups=0.12, wpb=1305.8, bsz=80, num_updates=6250, lr=8.78505e-06, gnorm=0.503, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=105114
2022-06-29 08:15:05 - progress_bar.py[line:274] - INFO: epoch 001:   6269 / 7081 loss=0.001, score=1.259, ntokens=1312.5, nsentences=80, sample_size=1312.5, wps=155.3, ups=0.12, wpb=1312.5, bsz=80, num_updates=6260, lr=8.7821e-06, gnorm=0.52, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=105198
2022-06-29 08:16:29 - progress_bar.py[line:274] - INFO: epoch 001:   6279 / 7081 loss=0, score=1.272, ntokens=1298.1, nsentences=80, sample_size=1298.1, wps=153.2, ups=0.12, wpb=1298.1, bsz=80, num_updates=6270, lr=8.77916e-06, gnorm=0.524, clip=0, loss_scale=128, train_wall=85, gb_free=6.7, wall=105283
2022-06-29 08:17:54 - progress_bar.py[line:274] - INFO: epoch 001:   6289 / 7081 loss=-0, score=1.287, ntokens=1314.2, nsentences=80, sample_size=1314.2, wps=155.6, ups=0.12, wpb=1314.2, bsz=80, num_updates=6280, lr=8.77621e-06, gnorm=0.654, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=105367
2022-06-29 08:19:18 - progress_bar.py[line:274] - INFO: epoch 001:   6299 / 7081 loss=-0.002, score=1.304, ntokens=1303.8, nsentences=80, sample_size=1303.8, wps=154.5, ups=0.12, wpb=1303.8, bsz=80, num_updates=6290, lr=8.77327e-06, gnorm=0.372, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=105452
2022-06-29 08:20:43 - progress_bar.py[line:274] - INFO: epoch 001:   6309 / 7081 loss=0, score=1.273, ntokens=1278.1, nsentences=80, sample_size=1278.1, wps=151.4, ups=0.12, wpb=1278.1, bsz=80, num_updates=6300, lr=8.77033e-06, gnorm=0.416, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=105536
2022-06-29 08:22:07 - progress_bar.py[line:274] - INFO: epoch 001:   6319 / 7081 loss=0.002, score=1.195, ntokens=1276.7, nsentences=80, sample_size=1276.7, wps=151.7, ups=0.12, wpb=1276.7, bsz=80, num_updates=6310, lr=8.76738e-06, gnorm=0.8, clip=30, loss_scale=128, train_wall=84, gb_free=6.7, wall=105620
2022-06-29 08:23:31 - progress_bar.py[line:274] - INFO: epoch 001:   6329 / 7081 loss=0.001, score=1.211, ntokens=1291.7, nsentences=80, sample_size=1291.7, wps=152.5, ups=0.12, wpb=1291.7, bsz=80, num_updates=6320, lr=8.76444e-06, gnorm=0.397, clip=0, loss_scale=128, train_wall=85, gb_free=6.7, wall=105705
2022-06-29 08:24:56 - progress_bar.py[line:274] - INFO: epoch 001:   6339 / 7081 loss=-0.001, score=1.264, ntokens=1305.9, nsentences=80, sample_size=1305.9, wps=154.3, ups=0.12, wpb=1305.9, bsz=80, num_updates=6330, lr=8.76149e-06, gnorm=0.511, clip=10, loss_scale=128, train_wall=85, gb_free=6.7, wall=105790
2022-06-29 08:26:21 - progress_bar.py[line:274] - INFO: epoch 001:   6349 / 7081 loss=-0.001, score=1.364, ntokens=1297.4, nsentences=80, sample_size=1297.4, wps=153.4, ups=0.12, wpb=1297.4, bsz=80, num_updates=6340, lr=8.75855e-06, gnorm=0.493, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=105874
2022-06-29 08:27:45 - progress_bar.py[line:274] - INFO: epoch 001:   6359 / 7081 loss=-0, score=1.179, ntokens=1300.9, nsentences=80, sample_size=1300.9, wps=153.6, ups=0.12, wpb=1300.9, bsz=80, num_updates=6350, lr=8.7556e-06, gnorm=0.652, clip=10, loss_scale=128, train_wall=85, gb_free=6.7, wall=105959
2022-06-29 08:29:10 - progress_bar.py[line:274] - INFO: epoch 001:   6369 / 7081 loss=-0.001, score=1.263, ntokens=1295.3, nsentences=80, sample_size=1295.3, wps=153.4, ups=0.12, wpb=1295.3, bsz=80, num_updates=6360, lr=8.75266e-06, gnorm=0.274, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=106043
2022-06-29 08:30:34 - progress_bar.py[line:274] - INFO: epoch 001:   6379 / 7081 loss=-0.001, score=1.36, ntokens=1295.7, nsentences=80, sample_size=1295.7, wps=153.3, ups=0.12, wpb=1295.7, bsz=80, num_updates=6370, lr=8.74971e-06, gnorm=0.627, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=106128
2022-06-29 08:31:58 - progress_bar.py[line:274] - INFO: epoch 001:   6389 / 7081 loss=0.001, score=1.27, ntokens=1286.2, nsentences=80, sample_size=1286.2, wps=153.1, ups=0.12, wpb=1286.2, bsz=80, num_updates=6380, lr=8.74677e-06, gnorm=0.619, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=106212
2022-06-29 08:33:22 - progress_bar.py[line:274] - INFO: epoch 001:   6399 / 7081 loss=-0, score=1.184, ntokens=1287.2, nsentences=80, sample_size=1287.2, wps=153.6, ups=0.12, wpb=1287.2, bsz=80, num_updates=6390, lr=8.74382e-06, gnorm=0.791, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=106296
2022-06-29 08:34:04 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-29 08:34:54 - progress_bar.py[line:274] - INFO: epoch 001:   6410 / 7081 loss=0.001, score=1.181, ntokens=1286.8, nsentences=80, sample_size=1286.8, wps=140.3, ups=0.11, wpb=1286.8, bsz=80, num_updates=6400, lr=8.74088e-06, gnorm=0.285, clip=0, loss_scale=64, train_wall=92, gb_free=6.7, wall=106387
2022-06-29 08:36:18 - progress_bar.py[line:274] - INFO: epoch 001:   6420 / 7081 loss=-0.001, score=1.271, ntokens=1294.2, nsentences=80, sample_size=1294.2, wps=154.5, ups=0.12, wpb=1294.2, bsz=80, num_updates=6410, lr=8.73793e-06, gnorm=0.311, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=106471
2022-06-29 08:37:42 - progress_bar.py[line:274] - INFO: epoch 001:   6430 / 7081 loss=-0.001, score=1.127, ntokens=1300.9, nsentences=80, sample_size=1300.9, wps=153.7, ups=0.12, wpb=1300.9, bsz=80, num_updates=6420, lr=8.73499e-06, gnorm=0.488, clip=20, loss_scale=64, train_wall=85, gb_free=6.7, wall=106556
2022-06-29 08:39:06 - progress_bar.py[line:274] - INFO: epoch 001:   6440 / 7081 loss=-0.001, score=1.27, ntokens=1290.4, nsentences=80, sample_size=1290.4, wps=153.6, ups=0.12, wpb=1290.4, bsz=80, num_updates=6430, lr=8.73205e-06, gnorm=0.485, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=106640
2022-06-29 08:40:30 - progress_bar.py[line:274] - INFO: epoch 001:   6450 / 7081 loss=0.001, score=1.21, ntokens=1280.2, nsentences=80, sample_size=1280.2, wps=152.4, ups=0.12, wpb=1280.2, bsz=80, num_updates=6440, lr=8.7291e-06, gnorm=0.472, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=106724
2022-06-29 08:41:54 - progress_bar.py[line:274] - INFO: epoch 001:   6460 / 7081 loss=-0, score=1.259, ntokens=1286.2, nsentences=80, sample_size=1286.2, wps=152.7, ups=0.12, wpb=1286.2, bsz=80, num_updates=6450, lr=8.72616e-06, gnorm=0.587, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=106808
2022-06-29 08:43:19 - progress_bar.py[line:274] - INFO: epoch 001:   6470 / 7081 loss=-0, score=1.149, ntokens=1274.5, nsentences=80, sample_size=1274.5, wps=151.3, ups=0.12, wpb=1274.5, bsz=80, num_updates=6460, lr=8.72321e-06, gnorm=0.368, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=106892
2022-06-29 08:44:43 - progress_bar.py[line:274] - INFO: epoch 001:   6480 / 7081 loss=0, score=1.234, ntokens=1261.8, nsentences=80, sample_size=1261.8, wps=149.8, ups=0.12, wpb=1261.8, bsz=80, num_updates=6470, lr=8.72027e-06, gnorm=0.376, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=106976
2022-06-29 08:46:07 - progress_bar.py[line:274] - INFO: epoch 001:   6490 / 7081 loss=0.001, score=1.31, ntokens=1266.8, nsentences=80, sample_size=1266.8, wps=150.1, ups=0.12, wpb=1266.8, bsz=80, num_updates=6480, lr=8.71732e-06, gnorm=0.586, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=107061
2022-06-29 08:47:32 - progress_bar.py[line:274] - INFO: epoch 001:   6500 / 7081 loss=-0.001, score=1.164, ntokens=1282.3, nsentences=80, sample_size=1282.3, wps=152.1, ups=0.12, wpb=1282.3, bsz=80, num_updates=6490, lr=8.71438e-06, gnorm=0.673, clip=20, loss_scale=64, train_wall=84, gb_free=6.7, wall=107145
2022-06-29 08:48:56 - progress_bar.py[line:274] - INFO: epoch 001:   6510 / 7081 loss=-0.002, score=1.201, ntokens=1271.8, nsentences=80, sample_size=1271.8, wps=151.5, ups=0.12, wpb=1271.8, bsz=80, num_updates=6500, lr=8.71143e-06, gnorm=0.868, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=107229
2022-06-29 08:48:56 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-06-29 09:35:29 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0 | score 1.158 | ntokens 157.162 | nsentences 10 | sample_size 157.162 | cider 1.183 | wps 140.7 | wpb 157.2 | bsz 10 | num_updates 6500 | best_cider 1.183
2022-06-29 09:35:29 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6500 updates
2022-06-29 09:35:29 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_6500.pt
2022-06-29 09:35:38 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_6500.pt
2022-06-29 09:37:06 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_6500.pt (epoch 1 @ 6500 updates, score 1.183) (writing took 96.5133664906025 seconds)
2022-06-29 09:38:30 - progress_bar.py[line:274] - INFO: epoch 001:   6520 / 7081 loss=0.002, score=1.251, ntokens=1254.9, nsentences=80, sample_size=1254.9, wps=4.2, ups=0, wpb=1254.9, bsz=80, num_updates=6510, lr=8.70849e-06, gnorm=0.386, clip=10, loss_scale=64, train_wall=83, gb_free=6.7, wall=110203
2022-06-29 09:39:54 - progress_bar.py[line:274] - INFO: epoch 001:   6530 / 7081 loss=-0.002, score=1.319, ntokens=1250.7, nsentences=80, sample_size=1250.7, wps=149.2, ups=0.12, wpb=1250.7, bsz=80, num_updates=6520, lr=8.70554e-06, gnorm=0.473, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=110287
2022-06-29 09:41:18 - progress_bar.py[line:274] - INFO: epoch 001:   6540 / 7081 loss=0.002, score=1.152, ntokens=1256.1, nsentences=80, sample_size=1256.1, wps=149.1, ups=0.12, wpb=1256.1, bsz=80, num_updates=6530, lr=8.7026e-06, gnorm=0.39, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=110371
2022-06-29 09:42:42 - progress_bar.py[line:274] - INFO: epoch 001:   6550 / 7081 loss=-0.002, score=1.261, ntokens=1257.8, nsentences=80, sample_size=1257.8, wps=149.7, ups=0.12, wpb=1257.8, bsz=80, num_updates=6540, lr=8.69965e-06, gnorm=0.537, clip=20, loss_scale=64, train_wall=84, gb_free=6.7, wall=110455
2022-06-29 09:44:06 - progress_bar.py[line:274] - INFO: epoch 001:   6560 / 7081 loss=-0.002, score=1.29, ntokens=1252.9, nsentences=80, sample_size=1252.9, wps=149.5, ups=0.12, wpb=1252.9, bsz=80, num_updates=6550, lr=8.69671e-06, gnorm=0.897, clip=20, loss_scale=64, train_wall=84, gb_free=6.7, wall=110539
2022-06-29 09:45:30 - progress_bar.py[line:274] - INFO: epoch 001:   6570 / 7081 loss=0.002, score=1.289, ntokens=1269.1, nsentences=80, sample_size=1269.1, wps=150.8, ups=0.12, wpb=1269.1, bsz=80, num_updates=6560, lr=8.69377e-06, gnorm=0.352, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=110623
2022-06-29 09:46:54 - progress_bar.py[line:274] - INFO: epoch 001:   6580 / 7081 loss=0.001, score=1.253, ntokens=1267.6, nsentences=80, sample_size=1267.6, wps=151, ups=0.12, wpb=1267.6, bsz=80, num_updates=6570, lr=8.69082e-06, gnorm=0.501, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=110707
2022-06-29 09:48:18 - progress_bar.py[line:274] - INFO: epoch 001:   6590 / 7081 loss=-0.002, score=1.347, ntokens=1268.1, nsentences=80, sample_size=1268.1, wps=151.2, ups=0.12, wpb=1268.1, bsz=80, num_updates=6580, lr=8.68788e-06, gnorm=0.395, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=110791
2022-06-29 09:49:42 - progress_bar.py[line:274] - INFO: epoch 001:   6600 / 7081 loss=-0.001, score=1.289, ntokens=1269.4, nsentences=80, sample_size=1269.4, wps=150.9, ups=0.12, wpb=1269.4, bsz=80, num_updates=6590, lr=8.68493e-06, gnorm=0.344, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=110875
2022-06-29 09:51:06 - progress_bar.py[line:274] - INFO: epoch 001:   6610 / 7081 loss=0, score=1.239, ntokens=1278.3, nsentences=80, sample_size=1278.3, wps=152, ups=0.12, wpb=1278.3, bsz=80, num_updates=6600, lr=8.68199e-06, gnorm=0.366, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=110959
2022-06-29 09:52:30 - progress_bar.py[line:274] - INFO: epoch 001:   6620 / 7081 loss=0, score=1.239, ntokens=1274.3, nsentences=80, sample_size=1274.3, wps=151, ups=0.12, wpb=1274.3, bsz=80, num_updates=6610, lr=8.67904e-06, gnorm=0.609, clip=30, loss_scale=64, train_wall=84, gb_free=6.7, wall=111044
2022-06-29 09:53:54 - progress_bar.py[line:274] - INFO: epoch 001:   6630 / 7081 loss=0.001, score=1.335, ntokens=1270.6, nsentences=80, sample_size=1270.6, wps=151.9, ups=0.12, wpb=1270.6, bsz=80, num_updates=6620, lr=8.6761e-06, gnorm=0.423, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=111127
2022-06-29 09:55:18 - progress_bar.py[line:274] - INFO: epoch 001:   6640 / 7081 loss=0.002, score=1.231, ntokens=1307.1, nsentences=80, sample_size=1307.1, wps=154.6, ups=0.12, wpb=1307.1, bsz=80, num_updates=6630, lr=8.67315e-06, gnorm=0.532, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=111212
2022-06-29 09:56:43 - progress_bar.py[line:274] - INFO: epoch 001:   6650 / 7081 loss=0.001, score=1.211, ntokens=1314.1, nsentences=80, sample_size=1314.1, wps=155.7, ups=0.12, wpb=1314.1, bsz=80, num_updates=6640, lr=8.67021e-06, gnorm=0.462, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=111296
2022-06-29 09:58:07 - progress_bar.py[line:274] - INFO: epoch 001:   6660 / 7081 loss=0, score=1.334, ntokens=1307, nsentences=80, sample_size=1307, wps=154.7, ups=0.12, wpb=1307, bsz=80, num_updates=6650, lr=8.66726e-06, gnorm=0.633, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=111381
2022-06-29 09:59:31 - progress_bar.py[line:274] - INFO: epoch 001:   6670 / 7081 loss=-0.001, score=1.266, ntokens=1266, nsentences=80, sample_size=1266, wps=151.6, ups=0.12, wpb=1266, bsz=80, num_updates=6660, lr=8.66432e-06, gnorm=0.847, clip=20, loss_scale=64, train_wall=83, gb_free=6.7, wall=111464
2022-06-29 10:00:55 - progress_bar.py[line:274] - INFO: epoch 001:   6680 / 7081 loss=0.001, score=1.296, ntokens=1273.5, nsentences=80, sample_size=1273.5, wps=152, ups=0.12, wpb=1273.5, bsz=80, num_updates=6670, lr=8.66137e-06, gnorm=0.782, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=111548
2022-06-29 10:02:19 - progress_bar.py[line:274] - INFO: epoch 001:   6690 / 7081 loss=0.001, score=1.213, ntokens=1277.3, nsentences=80, sample_size=1277.3, wps=151.7, ups=0.12, wpb=1277.3, bsz=80, num_updates=6680, lr=8.65843e-06, gnorm=0.473, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=111632
2022-06-29 10:03:43 - progress_bar.py[line:274] - INFO: epoch 001:   6700 / 7081 loss=-0.001, score=1.253, ntokens=1292.5, nsentences=80, sample_size=1292.5, wps=154.1, ups=0.12, wpb=1292.5, bsz=80, num_updates=6690, lr=8.65549e-06, gnorm=0.495, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=111716
2022-06-29 10:05:07 - progress_bar.py[line:274] - INFO: epoch 001:   6710 / 7081 loss=-0.001, score=1.144, ntokens=1287.7, nsentences=80, sample_size=1287.7, wps=152.6, ups=0.12, wpb=1287.7, bsz=80, num_updates=6700, lr=8.65254e-06, gnorm=0.427, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=111801
2022-06-29 10:06:31 - progress_bar.py[line:274] - INFO: epoch 001:   6720 / 7081 loss=-0.001, score=1.295, ntokens=1286.3, nsentences=80, sample_size=1286.3, wps=153, ups=0.12, wpb=1286.3, bsz=80, num_updates=6710, lr=8.6496e-06, gnorm=0.369, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=111885
2022-06-29 10:07:56 - progress_bar.py[line:274] - INFO: epoch 001:   6730 / 7081 loss=-0, score=1.36, ntokens=1283.4, nsentences=80, sample_size=1283.4, wps=151.7, ups=0.12, wpb=1283.4, bsz=80, num_updates=6720, lr=8.64665e-06, gnorm=0.383, clip=10, loss_scale=64, train_wall=85, gb_free=6.7, wall=111969
2022-06-29 10:09:19 - progress_bar.py[line:274] - INFO: epoch 001:   6740 / 7081 loss=-0.002, score=1.227, ntokens=1275, nsentences=80, sample_size=1275, wps=152.3, ups=0.12, wpb=1275, bsz=80, num_updates=6730, lr=8.64371e-06, gnorm=0.323, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=112053
2022-06-29 10:10:43 - progress_bar.py[line:274] - INFO: epoch 001:   6750 / 7081 loss=-0.001, score=1.216, ntokens=1265.9, nsentences=80, sample_size=1265.9, wps=150.8, ups=0.12, wpb=1265.9, bsz=80, num_updates=6740, lr=8.64076e-06, gnorm=0.265, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=112137
2022-06-29 10:12:07 - progress_bar.py[line:274] - INFO: epoch 001:   6760 / 7081 loss=0, score=1.263, ntokens=1256.9, nsentences=80, sample_size=1256.9, wps=149.5, ups=0.12, wpb=1256.9, bsz=80, num_updates=6750, lr=8.63782e-06, gnorm=0.481, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=112221
2022-06-29 10:13:31 - progress_bar.py[line:274] - INFO: epoch 001:   6770 / 7081 loss=0.001, score=1.255, ntokens=1263.2, nsentences=80, sample_size=1263.2, wps=151.2, ups=0.12, wpb=1263.2, bsz=80, num_updates=6760, lr=8.63487e-06, gnorm=0.348, clip=10, loss_scale=64, train_wall=83, gb_free=6.7, wall=112305
2022-06-29 10:14:55 - progress_bar.py[line:274] - INFO: epoch 001:   6780 / 7081 loss=-0.001, score=1.224, ntokens=1258.9, nsentences=80, sample_size=1258.9, wps=149.8, ups=0.12, wpb=1258.9, bsz=80, num_updates=6770, lr=8.63193e-06, gnorm=0.301, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=112389
2022-06-29 10:16:19 - progress_bar.py[line:274] - INFO: epoch 001:   6790 / 7081 loss=0.001, score=1.253, ntokens=1247, nsentences=80, sample_size=1247, wps=149.1, ups=0.12, wpb=1247, bsz=80, num_updates=6780, lr=8.62898e-06, gnorm=0.257, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=112472
2022-06-29 10:17:43 - progress_bar.py[line:274] - INFO: epoch 001:   6800 / 7081 loss=-0.001, score=1.247, ntokens=1284.3, nsentences=80, sample_size=1284.3, wps=152.4, ups=0.12, wpb=1284.3, bsz=80, num_updates=6790, lr=8.62604e-06, gnorm=0.613, clip=20, loss_scale=64, train_wall=84, gb_free=6.7, wall=112556
2022-06-29 10:19:07 - progress_bar.py[line:274] - INFO: epoch 001:   6810 / 7081 loss=-0, score=1.303, ntokens=1273.6, nsentences=80, sample_size=1273.6, wps=151.7, ups=0.12, wpb=1273.6, bsz=80, num_updates=6800, lr=8.62309e-06, gnorm=0.431, clip=20, loss_scale=64, train_wall=84, gb_free=6.7, wall=112640
2022-06-29 10:20:30 - progress_bar.py[line:274] - INFO: epoch 001:   6820 / 7081 loss=0.002, score=1.207, ntokens=1282.3, nsentences=80, sample_size=1282.3, wps=153.4, ups=0.12, wpb=1282.3, bsz=80, num_updates=6810, lr=8.62015e-06, gnorm=0.41, clip=10, loss_scale=64, train_wall=83, gb_free=6.7, wall=112724
2022-06-29 10:21:54 - progress_bar.py[line:274] - INFO: epoch 001:   6830 / 7081 loss=-0, score=1.146, ntokens=1274.7, nsentences=80, sample_size=1274.7, wps=151.8, ups=0.12, wpb=1274.7, bsz=80, num_updates=6820, lr=8.61721e-06, gnorm=0.462, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=112808
2022-06-29 10:23:19 - progress_bar.py[line:274] - INFO: epoch 001:   6840 / 7081 loss=-0.001, score=1.126, ntokens=1289.7, nsentences=80, sample_size=1289.7, wps=153.2, ups=0.12, wpb=1289.7, bsz=80, num_updates=6830, lr=8.61426e-06, gnorm=0.655, clip=20, loss_scale=64, train_wall=84, gb_free=6.7, wall=112892
2022-06-29 10:24:42 - progress_bar.py[line:274] - INFO: epoch 001:   6850 / 7081 loss=-0.001, score=1.339, ntokens=1265.3, nsentences=80, sample_size=1265.3, wps=150.9, ups=0.12, wpb=1265.3, bsz=80, num_updates=6840, lr=8.61132e-06, gnorm=0.39, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=112976
2022-06-29 10:26:06 - progress_bar.py[line:274] - INFO: epoch 001:   6860 / 7081 loss=0, score=1.371, ntokens=1251.4, nsentences=80, sample_size=1251.4, wps=149.8, ups=0.12, wpb=1251.4, bsz=80, num_updates=6850, lr=8.60837e-06, gnorm=0.619, clip=20, loss_scale=64, train_wall=83, gb_free=6.7, wall=113060
2022-06-29 10:27:30 - progress_bar.py[line:274] - INFO: epoch 001:   6870 / 7081 loss=0.002, score=1.28, ntokens=1281.7, nsentences=80, sample_size=1281.7, wps=153, ups=0.12, wpb=1281.7, bsz=80, num_updates=6860, lr=8.60543e-06, gnorm=0.188, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=113143
2022-06-29 10:28:54 - progress_bar.py[line:274] - INFO: epoch 001:   6880 / 7081 loss=0, score=1.239, ntokens=1289.6, nsentences=80, sample_size=1289.6, wps=153.4, ups=0.12, wpb=1289.6, bsz=80, num_updates=6870, lr=8.60248e-06, gnorm=0.368, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=113227
2022-06-29 10:30:18 - progress_bar.py[line:274] - INFO: epoch 001:   6890 / 7081 loss=0, score=1.257, ntokens=1281.8, nsentences=80, sample_size=1281.8, wps=152, ups=0.12, wpb=1281.8, bsz=80, num_updates=6880, lr=8.59954e-06, gnorm=0.258, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=113312
2022-06-29 10:31:42 - progress_bar.py[line:274] - INFO: epoch 001:   6900 / 7081 loss=-0.002, score=1.177, ntokens=1284.1, nsentences=80, sample_size=1284.1, wps=152.8, ups=0.12, wpb=1284.1, bsz=80, num_updates=6890, lr=8.59659e-06, gnorm=0.235, clip=0, loss_scale=64, train_wall=84, gb_free=6.7, wall=113396
2022-06-29 10:33:06 - progress_bar.py[line:274] - INFO: epoch 001:   6910 / 7081 loss=0, score=1.325, ntokens=1287.1, nsentences=80, sample_size=1287.1, wps=153.3, ups=0.12, wpb=1287.1, bsz=80, num_updates=6900, lr=8.59365e-06, gnorm=0.528, clip=10, loss_scale=64, train_wall=84, gb_free=6.7, wall=113480
2022-06-29 10:34:30 - progress_bar.py[line:274] - INFO: epoch 001:   6920 / 7081 loss=0.002, score=1.293, ntokens=1287.2, nsentences=80, sample_size=1287.2, wps=153.8, ups=0.12, wpb=1287.2, bsz=80, num_updates=6910, lr=8.5907e-06, gnorm=0.342, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=113563
2022-06-29 10:35:54 - progress_bar.py[line:274] - INFO: epoch 001:   6930 / 7081 loss=0, score=1.269, ntokens=1282.2, nsentences=80, sample_size=1282.2, wps=153.2, ups=0.12, wpb=1282.2, bsz=80, num_updates=6920, lr=8.58776e-06, gnorm=0.249, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=113647
2022-06-29 10:37:18 - progress_bar.py[line:274] - INFO: epoch 001:   6940 / 7081 loss=0.001, score=1.252, ntokens=1281.6, nsentences=80, sample_size=1281.6, wps=152.6, ups=0.12, wpb=1281.6, bsz=80, num_updates=6930, lr=8.58481e-06, gnorm=0.389, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=113731
2022-06-29 10:38:42 - progress_bar.py[line:274] - INFO: epoch 001:   6950 / 7081 loss=-0.001, score=1.238, ntokens=1289.3, nsentences=80, sample_size=1289.3, wps=153.1, ups=0.12, wpb=1289.3, bsz=80, num_updates=6940, lr=8.58187e-06, gnorm=0.35, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=113815
2022-06-29 10:40:06 - progress_bar.py[line:274] - INFO: epoch 001:   6960 / 7081 loss=-0, score=1.28, ntokens=1282.6, nsentences=80, sample_size=1282.6, wps=152.9, ups=0.12, wpb=1282.6, bsz=80, num_updates=6950, lr=8.57892e-06, gnorm=0.199, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=113899
2022-06-29 10:41:29 - progress_bar.py[line:274] - INFO: epoch 001:   6970 / 7081 loss=-0.001, score=1.366, ntokens=1280, nsentences=80, sample_size=1280, wps=153, ups=0.12, wpb=1280, bsz=80, num_updates=6960, lr=8.57598e-06, gnorm=0.351, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=113983
2022-06-29 10:42:53 - progress_bar.py[line:274] - INFO: epoch 001:   6980 / 7081 loss=0.001, score=1.261, ntokens=1282.2, nsentences=80, sample_size=1282.2, wps=152.7, ups=0.12, wpb=1282.2, bsz=80, num_updates=6970, lr=8.57304e-06, gnorm=0.498, clip=30, loss_scale=128, train_wall=84, gb_free=6.7, wall=114067
2022-06-29 10:44:17 - progress_bar.py[line:274] - INFO: epoch 001:   6990 / 7081 loss=-0.001, score=1.289, ntokens=1282.2, nsentences=80, sample_size=1282.2, wps=152.3, ups=0.12, wpb=1282.2, bsz=80, num_updates=6980, lr=8.57009e-06, gnorm=0.346, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=114151
2022-06-29 10:45:42 - progress_bar.py[line:274] - INFO: epoch 001:   7000 / 7081 loss=0, score=1.316, ntokens=1275.7, nsentences=80, sample_size=1275.7, wps=151.2, ups=0.12, wpb=1275.7, bsz=80, num_updates=6990, lr=8.56715e-06, gnorm=0.478, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=114235
2022-06-29 10:47:06 - progress_bar.py[line:274] - INFO: epoch 001:   7010 / 7081 loss=-0, score=1.212, ntokens=1287.2, nsentences=80, sample_size=1287.2, wps=152.8, ups=0.12, wpb=1287.2, bsz=80, num_updates=7000, lr=8.5642e-06, gnorm=0.242, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=114320
2022-06-29 10:47:06 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-06-29 11:33:53 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0 | score 1.173 | ntokens 161.44 | nsentences 10 | sample_size 161.44 | cider 1.196 | wps 143.8 | wpb 161.4 | bsz 10 | num_updates 7000 | best_cider 1.196
2022-06-29 11:33:53 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 7000 updates
2022-06-29 11:33:53 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_7000.pt
2022-06-29 11:34:03 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_7000.pt
2022-06-29 11:35:36 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_1_7000.pt (epoch 1 @ 7000 updates, score 1.196) (writing took 102.74139237590134 seconds)
2022-06-29 11:37:00 - progress_bar.py[line:274] - INFO: epoch 001:   7020 / 7081 loss=0.001, score=1.323, ntokens=1299.7, nsentences=80, sample_size=1299.7, wps=4.3, ups=0, wpb=1299.7, bsz=80, num_updates=7010, lr=8.56126e-06, gnorm=0.643, clip=30, loss_scale=128, train_wall=84, gb_free=6.7, wall=117314
2022-06-29 11:38:25 - progress_bar.py[line:274] - INFO: epoch 001:   7030 / 7081 loss=-0, score=1.181, ntokens=1300.9, nsentences=80, sample_size=1300.9, wps=154.1, ups=0.12, wpb=1300.9, bsz=80, num_updates=7020, lr=8.55831e-06, gnorm=0.487, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=117398
2022-06-29 11:39:49 - progress_bar.py[line:274] - INFO: epoch 001:   7040 / 7081 loss=0, score=1.28, ntokens=1298, nsentences=80, sample_size=1298, wps=154.3, ups=0.12, wpb=1298, bsz=80, num_updates=7030, lr=8.55537e-06, gnorm=0.304, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=117483
2022-06-29 11:41:13 - progress_bar.py[line:274] - INFO: epoch 001:   7050 / 7081 loss=-0, score=1.277, ntokens=1310.2, nsentences=80, sample_size=1310.2, wps=156.4, ups=0.12, wpb=1310.2, bsz=80, num_updates=7040, lr=8.55242e-06, gnorm=0.364, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=117566
2022-06-29 11:42:37 - progress_bar.py[line:274] - INFO: epoch 001:   7060 / 7081 loss=-0, score=1.208, ntokens=1310.6, nsentences=80, sample_size=1310.6, wps=155.1, ups=0.12, wpb=1310.6, bsz=80, num_updates=7050, lr=8.54948e-06, gnorm=0.296, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=117651
2022-06-29 11:44:02 - progress_bar.py[line:274] - INFO: epoch 001:   7070 / 7081 loss=0.002, score=1.287, ntokens=1306.1, nsentences=80, sample_size=1306.1, wps=154.5, ups=0.12, wpb=1306.1, bsz=80, num_updates=7060, lr=8.54653e-06, gnorm=0.736, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=117735
2022-06-29 11:45:26 - progress_bar.py[line:274] - INFO: epoch 001:   7080 / 7081 loss=-0.001, score=1.336, ntokens=1322.6, nsentences=80, sample_size=1322.6, wps=156.4, ups=0.12, wpb=1322.6, bsz=80, num_updates=7070, lr=8.54359e-06, gnorm=0.498, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=117820
slice_id 1 seek offset 2500
2022-06-29 11:45:31 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 0 seek offset 0
2022-06-29 12:32:23 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss -0 | score 1.17 | ntokens 164.584 | nsentences 10 | sample_size 164.584 | cider 1.195 | wps 146.3 | wpb 164.6 | bsz 10 | num_updates 7071 | best_cider 1.196
2022-06-29 12:32:23 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 7071 updates
local datafile ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2022-06-29 12:32:23 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_last.pt
local datafile ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 1 row count 56643 total row count 113287
slice_id 1 seek offset 56644
2022-06-29 12:33:01 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_last.pt
2022-06-29 12:33:02 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_last.pt (epoch 1 @ 7071 updates, score 1.195) (writing took 38.95250012911856 seconds)
2022-06-29 12:33:02 - train.py[line:323] - INFO: end of epoch 1 (average epoch stats below)
2022-06-29 12:33:02 - progress_bar.py[line:282] - INFO: epoch 001 | loss -0 | score 1.226 | ntokens 1304.41 | nsentences 79.994 | sample_size 1304.41 | wps 76.4 | ups 0.06 | wpb 1304.4 | bsz 80 | num_updates 7071 | lr 8.54329e-06 | gnorm 0.407 | clip 7 | loss_scale 128 | train_wall 68310 | gb_free 6.7 | wall 120675
2022-06-29 12:33:02 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage2_train_half.tsv slice_id 0 row count 56644 total row count 113287
slice_id 0 seek offset 0
2022-06-29 12:33:05 - trainer.py[line:703] - INFO: begin training epoch 2
2022-06-29 12:33:05 - train.py[line:296] - INFO: Start iterating over samples
2022-06-29 12:34:21 - progress_bar.py[line:274] - INFO: epoch 002:      9 / 7081 loss=-0, score=1.221, ntokens=1246.6, nsentences=76, sample_size=1246.6, wps=4.2, ups=0, wpb=1246.6, bsz=76, num_updates=7080, lr=8.54064e-06, gnorm=0.279, clip=0, loss_scale=128, train_wall=80, gb_free=6.7, wall=120754
2022-06-29 12:35:45 - progress_bar.py[line:274] - INFO: epoch 002:     19 / 7081 loss=-0, score=1.28, ntokens=1318.6, nsentences=80, sample_size=1318.6, wps=156.6, ups=0.12, wpb=1318.6, bsz=80, num_updates=7090, lr=8.5377e-06, gnorm=0.556, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=120838
2022-06-29 12:37:10 - progress_bar.py[line:274] - INFO: epoch 002:     29 / 7081 loss=0.001, score=1.229, ntokens=1314.3, nsentences=80, sample_size=1314.3, wps=155.3, ups=0.12, wpb=1314.3, bsz=80, num_updates=7100, lr=8.53476e-06, gnorm=0.392, clip=0, loss_scale=128, train_wall=85, gb_free=6.7, wall=120923
2022-06-29 12:38:35 - progress_bar.py[line:274] - INFO: epoch 002:     39 / 7081 loss=-0.001, score=1.23, ntokens=1310.4, nsentences=80, sample_size=1310.4, wps=154.2, ups=0.12, wpb=1310.4, bsz=80, num_updates=7110, lr=8.53181e-06, gnorm=0.301, clip=10, loss_scale=128, train_wall=85, gb_free=6.7, wall=121008
2022-06-29 12:39:59 - progress_bar.py[line:274] - INFO: epoch 002:     49 / 7081 loss=-0.001, score=1.193, ntokens=1314.4, nsentences=80, sample_size=1314.4, wps=155, ups=0.12, wpb=1314.4, bsz=80, num_updates=7120, lr=8.52887e-06, gnorm=0.367, clip=0, loss_scale=128, train_wall=85, gb_free=6.7, wall=121093
2022-06-29 12:41:24 - progress_bar.py[line:274] - INFO: epoch 002:     59 / 7081 loss=-0, score=1.261, ntokens=1313.5, nsentences=80, sample_size=1313.5, wps=155.2, ups=0.12, wpb=1313.5, bsz=80, num_updates=7130, lr=8.52592e-06, gnorm=0.492, clip=20, loss_scale=128, train_wall=85, gb_free=6.7, wall=121178
2022-06-29 12:42:49 - progress_bar.py[line:274] - INFO: epoch 002:     69 / 7081 loss=0.001, score=1.12, ntokens=1314.6, nsentences=80, sample_size=1314.6, wps=155.7, ups=0.12, wpb=1314.6, bsz=80, num_updates=7140, lr=8.52298e-06, gnorm=0.434, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=121262
2022-06-29 12:44:13 - progress_bar.py[line:274] - INFO: epoch 002:     79 / 7081 loss=-0.002, score=1.332, ntokens=1326.8, nsentences=80, sample_size=1326.8, wps=157, ups=0.12, wpb=1326.8, bsz=80, num_updates=7150, lr=8.52003e-06, gnorm=0.312, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=121347
2022-06-29 12:45:37 - progress_bar.py[line:274] - INFO: epoch 002:     89 / 7081 loss=-0.001, score=1.231, ntokens=1299.1, nsentences=80, sample_size=1299.1, wps=154.6, ups=0.12, wpb=1299.1, bsz=80, num_updates=7160, lr=8.51709e-06, gnorm=0.63, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=121431
2022-06-29 12:47:01 - progress_bar.py[line:274] - INFO: epoch 002:     99 / 7081 loss=0.001, score=1.187, ntokens=1312.5, nsentences=80, sample_size=1312.5, wps=156.1, ups=0.12, wpb=1312.5, bsz=80, num_updates=7170, lr=8.51414e-06, gnorm=0.263, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=121515
2022-06-29 12:48:25 - progress_bar.py[line:274] - INFO: epoch 002:    109 / 7081 loss=0.003, score=1.324, ntokens=1309, nsentences=80, sample_size=1309, wps=155.8, ups=0.12, wpb=1309, bsz=80, num_updates=7180, lr=8.5112e-06, gnorm=0.722, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=121599
2022-06-29 12:49:49 - progress_bar.py[line:274] - INFO: epoch 002:    119 / 7081 loss=0, score=1.257, ntokens=1314.1, nsentences=80, sample_size=1314.1, wps=155.9, ups=0.12, wpb=1314.1, bsz=80, num_updates=7190, lr=8.50825e-06, gnorm=0.308, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=121683
2022-06-29 12:51:14 - progress_bar.py[line:274] - INFO: epoch 002:    129 / 7081 loss=0, score=1.198, ntokens=1318.8, nsentences=80, sample_size=1318.8, wps=155.6, ups=0.12, wpb=1318.8, bsz=80, num_updates=7200, lr=8.50531e-06, gnorm=0.593, clip=10, loss_scale=128, train_wall=85, gb_free=6.7, wall=121768
2022-06-29 12:52:39 - progress_bar.py[line:274] - INFO: epoch 002:    139 / 7081 loss=-0.001, score=1.171, ntokens=1323.3, nsentences=80, sample_size=1323.3, wps=155.9, ups=0.12, wpb=1323.3, bsz=80, num_updates=7210, lr=8.50236e-06, gnorm=0.285, clip=10, loss_scale=128, train_wall=85, gb_free=6.7, wall=121853
2022-06-29 12:54:03 - progress_bar.py[line:274] - INFO: epoch 002:    149 / 7081 loss=0, score=1.307, ntokens=1327.5, nsentences=80, sample_size=1327.5, wps=158, ups=0.12, wpb=1327.5, bsz=80, num_updates=7220, lr=8.49942e-06, gnorm=0.32, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=121937
2022-06-29 12:55:27 - progress_bar.py[line:274] - INFO: epoch 002:    159 / 7081 loss=0.001, score=1.194, ntokens=1320.1, nsentences=80, sample_size=1320.1, wps=157.3, ups=0.12, wpb=1320.1, bsz=80, num_updates=7230, lr=8.49648e-06, gnorm=0.339, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=122021
2022-06-29 12:56:51 - progress_bar.py[line:274] - INFO: epoch 002:    169 / 7081 loss=-0, score=1.333, ntokens=1331.2, nsentences=80, sample_size=1331.2, wps=158.2, ups=0.12, wpb=1331.2, bsz=80, num_updates=7240, lr=8.49353e-06, gnorm=0.38, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=122105
2022-06-29 12:58:16 - progress_bar.py[line:274] - INFO: epoch 002:    179 / 7081 loss=-0.002, score=1.293, ntokens=1331.1, nsentences=80, sample_size=1331.1, wps=157.6, ups=0.12, wpb=1331.1, bsz=80, num_updates=7250, lr=8.49059e-06, gnorm=0.262, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=122189
2022-06-29 12:59:40 - progress_bar.py[line:274] - INFO: epoch 002:    189 / 7081 loss=0.001, score=1.22, ntokens=1318, nsentences=80, sample_size=1318, wps=155.6, ups=0.12, wpb=1318, bsz=80, num_updates=7260, lr=8.48764e-06, gnorm=0.37, clip=0, loss_scale=128, train_wall=85, gb_free=6.7, wall=122274
2022-06-29 13:01:05 - progress_bar.py[line:274] - INFO: epoch 002:    199 / 7081 loss=0.002, score=1.251, ntokens=1316.6, nsentences=80, sample_size=1316.6, wps=155.7, ups=0.12, wpb=1316.6, bsz=80, num_updates=7270, lr=8.4847e-06, gnorm=0.435, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=122358
2022-06-29 13:02:29 - progress_bar.py[line:274] - INFO: epoch 002:    209 / 7081 loss=0, score=1.221, ntokens=1325.7, nsentences=80, sample_size=1325.7, wps=157.3, ups=0.12, wpb=1325.7, bsz=80, num_updates=7280, lr=8.48175e-06, gnorm=0.214, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=122443
2022-06-29 13:03:54 - progress_bar.py[line:274] - INFO: epoch 002:    219 / 7081 loss=-0.001, score=1.283, ntokens=1321.3, nsentences=80, sample_size=1321.3, wps=156.4, ups=0.12, wpb=1321.3, bsz=80, num_updates=7290, lr=8.47881e-06, gnorm=0.277, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=122527
2022-06-29 13:05:18 - progress_bar.py[line:274] - INFO: epoch 002:    229 / 7081 loss=-0.002, score=1.188, ntokens=1306.2, nsentences=80, sample_size=1306.2, wps=155.2, ups=0.12, wpb=1306.2, bsz=80, num_updates=7300, lr=8.47586e-06, gnorm=0.386, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=122611
2022-06-29 13:06:42 - progress_bar.py[line:274] - INFO: epoch 002:    239 / 7081 loss=-0, score=1.242, ntokens=1307.7, nsentences=80, sample_size=1307.7, wps=154.6, ups=0.12, wpb=1307.7, bsz=80, num_updates=7310, lr=8.47292e-06, gnorm=0.384, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=122696
2022-06-29 13:08:07 - progress_bar.py[line:274] - INFO: epoch 002:    249 / 7081 loss=-0, score=1.254, ntokens=1301.9, nsentences=80, sample_size=1301.9, wps=154.5, ups=0.12, wpb=1301.9, bsz=80, num_updates=7320, lr=8.46997e-06, gnorm=0.374, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=122780
2022-06-29 13:09:31 - progress_bar.py[line:274] - INFO: epoch 002:    259 / 7081 loss=0.001, score=1.178, ntokens=1290.6, nsentences=80, sample_size=1290.6, wps=152.8, ups=0.12, wpb=1290.6, bsz=80, num_updates=7330, lr=8.46703e-06, gnorm=0.373, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=122865
2022-06-29 13:10:55 - progress_bar.py[line:274] - INFO: epoch 002:    269 / 7081 loss=-0.001, score=1.284, ntokens=1301.8, nsentences=80, sample_size=1301.8, wps=154.9, ups=0.12, wpb=1301.8, bsz=80, num_updates=7340, lr=8.46408e-06, gnorm=0.351, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=122949
2022-06-29 13:12:20 - progress_bar.py[line:274] - INFO: epoch 002:    279 / 7081 loss=-0.001, score=1.177, ntokens=1298.5, nsentences=80, sample_size=1298.5, wps=153.5, ups=0.12, wpb=1298.5, bsz=80, num_updates=7350, lr=8.46114e-06, gnorm=0.476, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=123033
2022-06-29 13:13:44 - progress_bar.py[line:274] - INFO: epoch 002:    289 / 7081 loss=0, score=1.244, ntokens=1289.1, nsentences=80, sample_size=1289.1, wps=153, ups=0.12, wpb=1289.1, bsz=80, num_updates=7360, lr=8.4582e-06, gnorm=0.281, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=123118
2022-06-29 13:15:09 - progress_bar.py[line:274] - INFO: epoch 002:    299 / 7081 loss=-0.002, score=1.107, ntokens=1297.4, nsentences=80, sample_size=1297.4, wps=153.5, ups=0.12, wpb=1297.4, bsz=80, num_updates=7370, lr=8.45525e-06, gnorm=0.453, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=123202
2022-06-29 13:16:33 - progress_bar.py[line:274] - INFO: epoch 002:    309 / 7081 loss=-0.001, score=1.36, ntokens=1292.2, nsentences=80, sample_size=1292.2, wps=152.8, ups=0.12, wpb=1292.2, bsz=80, num_updates=7380, lr=8.45231e-06, gnorm=0.431, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=123287
2022-06-29 13:17:58 - progress_bar.py[line:274] - INFO: epoch 002:    319 / 7081 loss=-0.003, score=1.278, ntokens=1275.9, nsentences=80, sample_size=1275.9, wps=151.1, ups=0.12, wpb=1275.9, bsz=80, num_updates=7390, lr=8.44936e-06, gnorm=0.651, clip=30, loss_scale=128, train_wall=84, gb_free=6.7, wall=123371
2022-06-29 13:19:22 - progress_bar.py[line:274] - INFO: epoch 002:    329 / 7081 loss=0.001, score=1.27, ntokens=1244.9, nsentences=80, sample_size=1244.9, wps=147.1, ups=0.12, wpb=1244.9, bsz=80, num_updates=7400, lr=8.44642e-06, gnorm=0.991, clip=20, loss_scale=128, train_wall=85, gb_free=6.7, wall=123456
2022-06-29 13:20:47 - progress_bar.py[line:274] - INFO: epoch 002:    339 / 7081 loss=0.001, score=1.175, ntokens=1255, nsentences=80, sample_size=1255, wps=148.5, ups=0.12, wpb=1255, bsz=80, num_updates=7410, lr=8.44347e-06, gnorm=0.684, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=123540
2022-06-29 13:22:11 - progress_bar.py[line:274] - INFO: epoch 002:    349 / 7081 loss=0.004, score=1.258, ntokens=1252.7, nsentences=80, sample_size=1252.7, wps=148.8, ups=0.12, wpb=1252.7, bsz=80, num_updates=7420, lr=8.44053e-06, gnorm=0.42, clip=0, loss_scale=256, train_wall=84, gb_free=6.7, wall=123624
2022-06-29 13:23:36 - progress_bar.py[line:274] - INFO: epoch 002:    359 / 7081 loss=0.002, score=1.29, ntokens=1273, nsentences=80, sample_size=1273, wps=150.4, ups=0.12, wpb=1273, bsz=80, num_updates=7430, lr=8.43758e-06, gnorm=0.5, clip=20, loss_scale=256, train_wall=85, gb_free=6.7, wall=123709
2022-06-29 13:25:00 - progress_bar.py[line:274] - INFO: epoch 002:    369 / 7081 loss=-0.002, score=1.271, ntokens=1291.6, nsentences=80, sample_size=1291.6, wps=152.4, ups=0.12, wpb=1291.6, bsz=80, num_updates=7440, lr=8.43464e-06, gnorm=0.461, clip=0, loss_scale=256, train_wall=85, gb_free=6.7, wall=123794
2022-06-29 13:26:25 - progress_bar.py[line:274] - INFO: epoch 002:    379 / 7081 loss=-0, score=1.359, ntokens=1279.3, nsentences=80, sample_size=1279.3, wps=150.7, ups=0.12, wpb=1279.3, bsz=80, num_updates=7450, lr=8.43169e-06, gnorm=0.375, clip=0, loss_scale=256, train_wall=85, gb_free=6.7, wall=123879
2022-06-29 13:27:50 - progress_bar.py[line:274] - INFO: epoch 002:    389 / 7081 loss=0.004, score=1.298, ntokens=1291.6, nsentences=80, sample_size=1291.6, wps=152.5, ups=0.12, wpb=1291.6, bsz=80, num_updates=7460, lr=8.42875e-06, gnorm=0.545, clip=10, loss_scale=256, train_wall=85, gb_free=6.7, wall=123963
2022-06-29 13:29:14 - progress_bar.py[line:274] - INFO: epoch 002:    399 / 7081 loss=-0, score=1.342, ntokens=1304, nsentences=80, sample_size=1304, wps=154.4, ups=0.12, wpb=1304, bsz=80, num_updates=7470, lr=8.4258e-06, gnorm=0.658, clip=20, loss_scale=256, train_wall=84, gb_free=6.7, wall=124048
2022-06-29 13:30:39 - progress_bar.py[line:274] - INFO: epoch 002:    409 / 7081 loss=-0.003, score=1.232, ntokens=1309.4, nsentences=80, sample_size=1309.4, wps=155.2, ups=0.12, wpb=1309.4, bsz=80, num_updates=7480, lr=8.42286e-06, gnorm=0.363, clip=0, loss_scale=256, train_wall=84, gb_free=6.7, wall=124132
2022-06-29 13:32:03 - progress_bar.py[line:274] - INFO: epoch 002:    419 / 7081 loss=-0.003, score=1.189, ntokens=1304.3, nsentences=80, sample_size=1304.3, wps=155.2, ups=0.12, wpb=1304.3, bsz=80, num_updates=7490, lr=8.41992e-06, gnorm=0.317, clip=0, loss_scale=256, train_wall=84, gb_free=6.7, wall=124216
2022-06-29 13:33:27 - progress_bar.py[line:274] - INFO: epoch 002:    429 / 7081 loss=-0, score=1.303, ntokens=1283.4, nsentences=80, sample_size=1283.4, wps=152.9, ups=0.12, wpb=1283.4, bsz=80, num_updates=7500, lr=8.41697e-06, gnorm=0.426, clip=0, loss_scale=256, train_wall=84, gb_free=6.7, wall=124300
2022-06-29 13:33:27 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 1 seek offset 2500
2022-06-29 14:20:20 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss -0 | score 1.173 | ntokens 158.512 | nsentences 10 | sample_size 158.512 | cider 1.203 | wps 140.9 | wpb 158.5 | bsz 10 | num_updates 7500 | best_cider 1.203
2022-06-29 14:20:20 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 7500 updates
2022-06-29 14:20:20 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_7500.pt
2022-06-29 14:20:29 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_7500.pt
2022-06-29 14:22:05 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_7500.pt (epoch 2 @ 7500 updates, score 1.203) (writing took 104.75468513648957 seconds)
2022-06-29 14:23:29 - progress_bar.py[line:274] - INFO: epoch 002:    439 / 7081 loss=-0.001, score=1.178, ntokens=1270.7, nsentences=80, sample_size=1270.7, wps=4.2, ups=0, wpb=1270.7, bsz=80, num_updates=7510, lr=8.41403e-06, gnorm=0.394, clip=10, loss_scale=256, train_wall=84, gb_free=6.7, wall=127302
2022-06-29 14:24:53 - progress_bar.py[line:274] - INFO: epoch 002:    449 / 7081 loss=0, score=1.341, ntokens=1270.7, nsentences=80, sample_size=1270.7, wps=151.2, ups=0.12, wpb=1270.7, bsz=80, num_updates=7520, lr=8.41108e-06, gnorm=0.53, clip=20, loss_scale=256, train_wall=84, gb_free=6.7, wall=127387
2022-06-29 14:26:17 - progress_bar.py[line:274] - INFO: epoch 002:    459 / 7081 loss=-0.001, score=1.241, ntokens=1263.2, nsentences=80, sample_size=1263.2, wps=150.7, ups=0.12, wpb=1263.2, bsz=80, num_updates=7530, lr=8.40814e-06, gnorm=0.494, clip=10, loss_scale=256, train_wall=84, gb_free=6.7, wall=127470
2022-06-29 14:27:41 - progress_bar.py[line:274] - INFO: epoch 002:    469 / 7081 loss=-0, score=1.14, ntokens=1233.2, nsentences=80, sample_size=1233.2, wps=146.9, ups=0.12, wpb=1233.2, bsz=80, num_updates=7540, lr=8.40519e-06, gnorm=0.618, clip=10, loss_scale=256, train_wall=84, gb_free=6.7, wall=127554
2022-06-29 14:29:05 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-06-29 14:29:13 - progress_bar.py[line:274] - INFO: epoch 002:    480 / 7081 loss=0, score=1.26, ntokens=1239, nsentences=80, sample_size=1239, wps=133.6, ups=0.11, wpb=1239, bsz=80, num_updates=7550, lr=8.40225e-06, gnorm=0.777, clip=40, loss_scale=128, train_wall=93, gb_free=6.7, wall=127647
2022-06-29 14:30:38 - progress_bar.py[line:274] - INFO: epoch 002:    490 / 7081 loss=0.002, score=1.304, ntokens=1242.5, nsentences=80, sample_size=1242.5, wps=147.1, ups=0.12, wpb=1242.5, bsz=80, num_updates=7560, lr=8.3993e-06, gnorm=0.436, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=127731
2022-06-29 14:32:03 - progress_bar.py[line:274] - INFO: epoch 002:    500 / 7081 loss=-0.001, score=1.234, ntokens=1231.6, nsentences=80, sample_size=1231.6, wps=145.6, ups=0.12, wpb=1231.6, bsz=80, num_updates=7570, lr=8.39636e-06, gnorm=0.715, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=127816
2022-06-29 14:33:27 - progress_bar.py[line:274] - INFO: epoch 002:    510 / 7081 loss=0.001, score=1.152, ntokens=1231.2, nsentences=80, sample_size=1231.2, wps=146.4, ups=0.12, wpb=1231.2, bsz=80, num_updates=7580, lr=8.39341e-06, gnorm=0.719, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=127900
2022-06-29 14:34:51 - progress_bar.py[line:274] - INFO: epoch 002:    520 / 7081 loss=-0, score=1.164, ntokens=1245.5, nsentences=80, sample_size=1245.5, wps=147.8, ups=0.12, wpb=1245.5, bsz=80, num_updates=7590, lr=8.39047e-06, gnorm=0.314, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=127984
2022-06-29 14:36:15 - progress_bar.py[line:274] - INFO: epoch 002:    530 / 7081 loss=0.001, score=1.172, ntokens=1244.1, nsentences=80, sample_size=1244.1, wps=147.8, ups=0.12, wpb=1244.1, bsz=80, num_updates=7600, lr=8.38752e-06, gnorm=0.366, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=128069
2022-06-29 14:37:39 - progress_bar.py[line:274] - INFO: epoch 002:    540 / 7081 loss=-0, score=1.127, ntokens=1218.8, nsentences=80, sample_size=1218.8, wps=145.9, ups=0.12, wpb=1218.8, bsz=80, num_updates=7610, lr=8.38458e-06, gnorm=0.402, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=128152
2022-06-29 14:39:03 - progress_bar.py[line:274] - INFO: epoch 002:    550 / 7081 loss=0, score=1.178, ntokens=1232.7, nsentences=80, sample_size=1232.7, wps=146.7, ups=0.12, wpb=1232.7, bsz=80, num_updates=7620, lr=8.38164e-06, gnorm=0.713, clip=40, loss_scale=128, train_wall=84, gb_free=6.7, wall=128236
2022-06-29 14:40:27 - progress_bar.py[line:274] - INFO: epoch 002:    560 / 7081 loss=-0.001, score=1.207, ntokens=1229.8, nsentences=80, sample_size=1229.8, wps=145.5, ups=0.12, wpb=1229.8, bsz=80, num_updates=7630, lr=8.37869e-06, gnorm=0.335, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=128321
2022-06-29 14:41:51 - progress_bar.py[line:274] - INFO: epoch 002:    570 / 7081 loss=-0, score=1.236, ntokens=1219.4, nsentences=80, sample_size=1219.4, wps=145.5, ups=0.12, wpb=1219.4, bsz=80, num_updates=7640, lr=8.37575e-06, gnorm=0.947, clip=30, loss_scale=128, train_wall=84, gb_free=6.7, wall=128404
2022-06-29 14:43:15 - progress_bar.py[line:274] - INFO: epoch 002:    580 / 7081 loss=-0, score=1.259, ntokens=1236.1, nsentences=80, sample_size=1236.1, wps=146.9, ups=0.12, wpb=1236.1, bsz=80, num_updates=7650, lr=8.3728e-06, gnorm=0.387, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=128489
2022-06-29 14:44:38 - progress_bar.py[line:274] - INFO: epoch 002:    590 / 7081 loss=-0.001, score=1.326, ntokens=1216.1, nsentences=80, sample_size=1216.1, wps=145.8, ups=0.12, wpb=1216.1, bsz=80, num_updates=7660, lr=8.36986e-06, gnorm=0.402, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=128572
2022-06-29 14:46:03 - progress_bar.py[line:274] - INFO: epoch 002:    600 / 7081 loss=-0.001, score=1.285, ntokens=1239, nsentences=80, sample_size=1239, wps=147.4, ups=0.12, wpb=1239, bsz=80, num_updates=7670, lr=8.36691e-06, gnorm=0.462, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=128656
2022-06-29 14:47:27 - progress_bar.py[line:274] - INFO: epoch 002:    610 / 7081 loss=0.002, score=1.236, ntokens=1267.7, nsentences=80, sample_size=1267.7, wps=149.9, ups=0.12, wpb=1267.7, bsz=80, num_updates=7680, lr=8.36397e-06, gnorm=0.455, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=128741
2022-06-29 14:48:51 - progress_bar.py[line:274] - INFO: epoch 002:    620 / 7081 loss=-0, score=1.327, ntokens=1271.2, nsentences=80, sample_size=1271.2, wps=151.1, ups=0.12, wpb=1271.2, bsz=80, num_updates=7690, lr=8.36102e-06, gnorm=0.463, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=128825
2022-06-29 14:50:16 - progress_bar.py[line:274] - INFO: epoch 002:    630 / 7081 loss=0, score=1.237, ntokens=1277.8, nsentences=80, sample_size=1277.8, wps=151.4, ups=0.12, wpb=1277.8, bsz=80, num_updates=7700, lr=8.35808e-06, gnorm=0.606, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=128909
2022-06-29 14:51:40 - progress_bar.py[line:274] - INFO: epoch 002:    640 / 7081 loss=0.002, score=1.271, ntokens=1280.1, nsentences=80, sample_size=1280.1, wps=152.1, ups=0.12, wpb=1280.1, bsz=80, num_updates=7710, lr=8.35513e-06, gnorm=0.494, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=128993
2022-06-29 14:53:05 - progress_bar.py[line:274] - INFO: epoch 002:    650 / 7081 loss=-0, score=1.267, ntokens=1280.4, nsentences=80, sample_size=1280.4, wps=151, ups=0.12, wpb=1280.4, bsz=80, num_updates=7720, lr=8.35219e-06, gnorm=0.391, clip=10, loss_scale=128, train_wall=85, gb_free=6.7, wall=129078
2022-06-29 14:54:29 - progress_bar.py[line:274] - INFO: epoch 002:    660 / 7081 loss=-0, score=1.188, ntokens=1273.4, nsentences=80, sample_size=1273.4, wps=150.8, ups=0.12, wpb=1273.4, bsz=80, num_updates=7730, lr=8.34924e-06, gnorm=0.51, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=129163
2022-06-29 14:55:59 - progress_bar.py[line:274] - INFO: epoch 002:    670 / 7081 loss=-0.001, score=1.163, ntokens=1244.5, nsentences=80, sample_size=1244.5, wps=138.5, ups=0.11, wpb=1244.5, bsz=80, num_updates=7740, lr=8.3463e-06, gnorm=0.466, clip=10, loss_scale=128, train_wall=90, gb_free=6.7, wall=129252
2022-06-29 14:57:22 - progress_bar.py[line:274] - INFO: epoch 002:    680 / 7081 loss=0.001, score=1.309, ntokens=1224.2, nsentences=80, sample_size=1224.2, wps=147, ups=0.12, wpb=1224.2, bsz=80, num_updates=7750, lr=8.34336e-06, gnorm=0.662, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=129336
2022-06-29 14:58:45 - progress_bar.py[line:274] - INFO: epoch 002:    690 / 7081 loss=0.001, score=1.217, ntokens=1192, nsentences=80, sample_size=1192, wps=143.5, ups=0.12, wpb=1192, bsz=80, num_updates=7760, lr=8.34041e-06, gnorm=0.288, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=129419
2022-06-29 15:00:07 - progress_bar.py[line:274] - INFO: epoch 002:    700 / 7081 loss=0.001, score=1.329, ntokens=1181.4, nsentences=80, sample_size=1181.4, wps=143.7, ups=0.12, wpb=1181.4, bsz=80, num_updates=7770, lr=8.33747e-06, gnorm=0.404, clip=10, loss_scale=128, train_wall=82, gb_free=6.7, wall=129501
2022-06-29 15:01:29 - progress_bar.py[line:274] - INFO: epoch 002:    710 / 7081 loss=-0.002, score=1.196, ntokens=1171.2, nsentences=80, sample_size=1171.2, wps=142.8, ups=0.12, wpb=1171.2, bsz=80, num_updates=7780, lr=8.33452e-06, gnorm=0.643, clip=30, loss_scale=128, train_wall=82, gb_free=6.7, wall=129583
2022-06-29 15:02:53 - progress_bar.py[line:274] - INFO: epoch 002:    720 / 7081 loss=-0, score=1.223, ntokens=1206.9, nsentences=80, sample_size=1206.9, wps=145, ups=0.12, wpb=1206.9, bsz=80, num_updates=7790, lr=8.33158e-06, gnorm=0.84, clip=20, loss_scale=128, train_wall=83, gb_free=6.7, wall=129666
2022-06-29 15:04:16 - progress_bar.py[line:274] - INFO: epoch 002:    730 / 7081 loss=0.001, score=1.227, ntokens=1215.8, nsentences=80, sample_size=1215.8, wps=145.9, ups=0.12, wpb=1215.8, bsz=80, num_updates=7800, lr=8.32863e-06, gnorm=0.458, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=129750
2022-06-29 15:05:40 - progress_bar.py[line:274] - INFO: epoch 002:    740 / 7081 loss=0, score=1.259, ntokens=1214.4, nsentences=80, sample_size=1214.4, wps=145.1, ups=0.12, wpb=1214.4, bsz=80, num_updates=7810, lr=8.32569e-06, gnorm=0.432, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=129833
2022-06-29 15:07:03 - progress_bar.py[line:274] - INFO: epoch 002:    750 / 7081 loss=-0.002, score=1.221, ntokens=1231.2, nsentences=80, sample_size=1231.2, wps=147.5, ups=0.12, wpb=1231.2, bsz=80, num_updates=7820, lr=8.32274e-06, gnorm=0.457, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=129917
2022-06-29 15:08:27 - progress_bar.py[line:274] - INFO: epoch 002:    760 / 7081 loss=-0.001, score=1.216, ntokens=1234, nsentences=80, sample_size=1234, wps=146.8, ups=0.12, wpb=1234, bsz=80, num_updates=7830, lr=8.3198e-06, gnorm=0.558, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=130001
2022-06-29 15:09:52 - progress_bar.py[line:274] - INFO: epoch 002:    770 / 7081 loss=-0, score=1.191, ntokens=1225.2, nsentences=80, sample_size=1225.2, wps=145.2, ups=0.12, wpb=1225.2, bsz=80, num_updates=7840, lr=8.31685e-06, gnorm=0.369, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=130085
2022-06-29 15:11:16 - progress_bar.py[line:274] - INFO: epoch 002:    780 / 7081 loss=-0, score=1.272, ntokens=1222.2, nsentences=80, sample_size=1222.2, wps=145, ups=0.12, wpb=1222.2, bsz=80, num_updates=7850, lr=8.31391e-06, gnorm=0.809, clip=30, loss_scale=128, train_wall=84, gb_free=6.7, wall=130169
2022-06-29 15:12:40 - progress_bar.py[line:274] - INFO: epoch 002:    790 / 7081 loss=0.001, score=1.276, ntokens=1208.1, nsentences=80, sample_size=1208.1, wps=143.9, ups=0.12, wpb=1208.1, bsz=80, num_updates=7860, lr=8.31096e-06, gnorm=0.264, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=130253
2022-06-29 15:14:04 - progress_bar.py[line:274] - INFO: epoch 002:    800 / 7081 loss=0.001, score=1.244, ntokens=1210.4, nsentences=80, sample_size=1210.4, wps=144.5, ups=0.12, wpb=1210.4, bsz=80, num_updates=7870, lr=8.30802e-06, gnorm=0.432, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=130337
2022-06-29 15:15:28 - progress_bar.py[line:274] - INFO: epoch 002:    810 / 7081 loss=-0, score=1.23, ntokens=1226.5, nsentences=80, sample_size=1226.5, wps=145.8, ups=0.12, wpb=1226.5, bsz=80, num_updates=7880, lr=8.30507e-06, gnorm=0.766, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=130421
2022-06-29 15:16:56 - progress_bar.py[line:274] - INFO: epoch 002:    820 / 7081 loss=-0, score=1.243, ntokens=1249.5, nsentences=80, sample_size=1249.5, wps=141.7, ups=0.11, wpb=1249.5, bsz=80, num_updates=7890, lr=8.30213e-06, gnorm=0.514, clip=10, loss_scale=128, train_wall=88, gb_free=6.7, wall=130509
2022-06-29 15:18:19 - progress_bar.py[line:274] - INFO: epoch 002:    830 / 7081 loss=-0.001, score=1.301, ntokens=1232.4, nsentences=80, sample_size=1232.4, wps=147.7, ups=0.12, wpb=1232.4, bsz=80, num_updates=7900, lr=8.29919e-06, gnorm=0.415, clip=20, loss_scale=128, train_wall=83, gb_free=6.7, wall=130593
2022-06-29 15:19:43 - progress_bar.py[line:274] - INFO: epoch 002:    840 / 7081 loss=-0.001, score=1.284, ntokens=1244.7, nsentences=80, sample_size=1244.7, wps=149.1, ups=0.12, wpb=1244.7, bsz=80, num_updates=7910, lr=8.29624e-06, gnorm=0.387, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=130676
2022-06-29 15:21:06 - progress_bar.py[line:274] - INFO: epoch 002:    850 / 7081 loss=0.001, score=1.251, ntokens=1260.4, nsentences=80, sample_size=1260.4, wps=151.1, ups=0.12, wpb=1260.4, bsz=80, num_updates=7920, lr=8.2933e-06, gnorm=0.461, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=130760
2022-06-29 15:22:30 - progress_bar.py[line:274] - INFO: epoch 002:    860 / 7081 loss=0, score=1.209, ntokens=1236.5, nsentences=80, sample_size=1236.5, wps=148.4, ups=0.12, wpb=1236.5, bsz=80, num_updates=7930, lr=8.29035e-06, gnorm=0.444, clip=20, loss_scale=128, train_wall=83, gb_free=6.7, wall=130843
2022-06-29 15:23:53 - progress_bar.py[line:274] - INFO: epoch 002:    870 / 7081 loss=-0, score=1.19, ntokens=1245.9, nsentences=80, sample_size=1245.9, wps=149.4, ups=0.12, wpb=1245.9, bsz=80, num_updates=7940, lr=8.28741e-06, gnorm=0.353, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=130927
2022-06-29 15:25:17 - progress_bar.py[line:274] - INFO: epoch 002:    880 / 7081 loss=-0.001, score=1.253, ntokens=1238.3, nsentences=80, sample_size=1238.3, wps=148, ups=0.12, wpb=1238.3, bsz=80, num_updates=7950, lr=8.28446e-06, gnorm=0.511, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=131010
2022-06-29 15:26:41 - progress_bar.py[line:274] - INFO: epoch 002:    890 / 7081 loss=-0, score=1.205, ntokens=1262.1, nsentences=80, sample_size=1262.1, wps=149.8, ups=0.12, wpb=1262.1, bsz=80, num_updates=7960, lr=8.28152e-06, gnorm=0.677, clip=30, loss_scale=128, train_wall=84, gb_free=6.7, wall=131095
2022-06-29 15:28:05 - progress_bar.py[line:274] - INFO: epoch 002:    900 / 7081 loss=-0.001, score=1.285, ntokens=1263.6, nsentences=80, sample_size=1263.6, wps=149.8, ups=0.12, wpb=1263.6, bsz=80, num_updates=7970, lr=8.27857e-06, gnorm=0.528, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=131179
2022-06-29 15:29:30 - progress_bar.py[line:274] - INFO: epoch 002:    910 / 7081 loss=0.001, score=1.296, ntokens=1259, nsentences=80, sample_size=1259, wps=149.2, ups=0.12, wpb=1259, bsz=80, num_updates=7980, lr=8.27563e-06, gnorm=0.645, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=131263
2022-06-29 15:30:54 - progress_bar.py[line:274] - INFO: epoch 002:    920 / 7081 loss=0.002, score=1.229, ntokens=1284.9, nsentences=80, sample_size=1284.9, wps=152.6, ups=0.12, wpb=1284.9, bsz=80, num_updates=7990, lr=8.27268e-06, gnorm=0.484, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=131347
2022-06-29 15:32:18 - progress_bar.py[line:274] - INFO: epoch 002:    930 / 7081 loss=-0, score=1.275, ntokens=1281.1, nsentences=80, sample_size=1281.1, wps=151.6, ups=0.12, wpb=1281.1, bsz=80, num_updates=8000, lr=8.26974e-06, gnorm=0.6, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=131432
2022-06-29 15:32:18 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-29 16:19:10 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss -0 | score 1.178 | ntokens 162.088 | nsentences 10 | sample_size 162.088 | cider 1.202 | wps 144.1 | wpb 162.1 | bsz 10 | num_updates 8000 | best_cider 1.203
2022-06-29 16:19:10 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 8000 updates
2022-06-29 16:19:10 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_8000.pt
2022-06-29 16:19:19 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_8000.pt
2022-06-29 16:20:12 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_8000.pt (epoch 2 @ 8000 updates, score 1.202) (writing took 62.47113212849945 seconds)
2022-06-29 16:21:37 - progress_bar.py[line:274] - INFO: epoch 002:    940 / 7081 loss=0.001, score=1.151, ntokens=1302.3, nsentences=80, sample_size=1302.3, wps=4.4, ups=0, wpb=1302.3, bsz=80, num_updates=8010, lr=8.26679e-06, gnorm=0.397, clip=10, loss_scale=128, train_wall=84, gb_free=6.7, wall=134390
2022-06-29 16:23:01 - progress_bar.py[line:274] - INFO: epoch 002:    950 / 7081 loss=-0, score=1.204, ntokens=1310.3, nsentences=80, sample_size=1310.3, wps=155.4, ups=0.12, wpb=1310.3, bsz=80, num_updates=8020, lr=8.26385e-06, gnorm=0.232, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=134475
2022-06-29 16:24:26 - progress_bar.py[line:274] - INFO: epoch 002:    960 / 7081 loss=0, score=1.284, ntokens=1313, nsentences=80, sample_size=1313, wps=155.3, ups=0.12, wpb=1313, bsz=80, num_updates=8030, lr=8.26091e-06, gnorm=0.217, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=134559
2022-06-29 16:25:50 - progress_bar.py[line:274] - INFO: epoch 002:    970 / 7081 loss=0, score=1.249, ntokens=1320.5, nsentences=80, sample_size=1320.5, wps=156.3, ups=0.12, wpb=1320.5, bsz=80, num_updates=8040, lr=8.25796e-06, gnorm=0.19, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=134644
2022-06-29 16:27:15 - progress_bar.py[line:274] - INFO: epoch 002:    980 / 7081 loss=-0.001, score=1.284, ntokens=1313.6, nsentences=80, sample_size=1313.6, wps=155.5, ups=0.12, wpb=1313.6, bsz=80, num_updates=8050, lr=8.25502e-06, gnorm=0.68, clip=20, loss_scale=128, train_wall=84, gb_free=6.7, wall=134728
2022-06-29 16:28:39 - progress_bar.py[line:274] - INFO: epoch 002:    990 / 7081 loss=0, score=1.32, ntokens=1305.4, nsentences=80, sample_size=1305.4, wps=154.5, ups=0.12, wpb=1305.4, bsz=80, num_updates=8060, lr=8.25207e-06, gnorm=0.277, clip=0, loss_scale=128, train_wall=84, gb_free=6.7, wall=134813
2022-06-29 16:30:04 - progress_bar.py[line:274] - INFO: epoch 002:   1000 / 7081 loss=-0.001, score=1.318, ntokens=1320, nsentences=80, sample_size=1320, wps=156.1, ups=0.12, wpb=1320, bsz=80, num_updates=8070, lr=8.24913e-06, gnorm=0.514, clip=10, loss_scale=256, train_wall=84, gb_free=6.7, wall=134897
2022-06-29 16:31:29 - progress_bar.py[line:274] - INFO: epoch 002:   1010 / 7081 loss=0.001, score=1.37, ntokens=1312.9, nsentences=80, sample_size=1312.9, wps=154.1, ups=0.12, wpb=1312.9, bsz=80, num_updates=8080, lr=8.24618e-06, gnorm=0.29, clip=0, loss_scale=256, train_wall=85, gb_free=6.7, wall=134982
2022-06-29 16:32:55 - progress_bar.py[line:274] - INFO: epoch 002:   1020 / 7081 loss=-0.001, score=1.218, ntokens=1312.3, nsentences=80, sample_size=1312.3, wps=152.6, ups=0.12, wpb=1312.3, bsz=80, num_updates=8090, lr=8.24324e-06, gnorm=0.19, clip=0, loss_scale=256, train_wall=86, gb_free=6.7, wall=135068
2022-06-29 16:34:19 - progress_bar.py[line:274] - INFO: epoch 002:   1030 / 7081 loss=0.001, score=1.216, ntokens=1319.7, nsentences=80, sample_size=1319.7, wps=156.1, ups=0.12, wpb=1319.7, bsz=80, num_updates=8100, lr=8.24029e-06, gnorm=0.42, clip=20, loss_scale=256, train_wall=84, gb_free=6.7, wall=135153
2022-06-29 16:35:43 - progress_bar.py[line:274] - INFO: epoch 002:   1040 / 7081 loss=-0.002, score=1.314, ntokens=1319.9, nsentences=80, sample_size=1319.9, wps=157.4, ups=0.12, wpb=1319.9, bsz=80, num_updates=8110, lr=8.23735e-06, gnorm=0.397, clip=10, loss_scale=256, train_wall=84, gb_free=6.7, wall=135237
2022-06-29 16:37:06 - progress_bar.py[line:274] - INFO: epoch 002:   1050 / 7081 loss=-0.001, score=1.249, ntokens=1308.6, nsentences=80, sample_size=1308.6, wps=157.2, ups=0.12, wpb=1308.6, bsz=80, num_updates=8120, lr=8.2344e-06, gnorm=0.295, clip=10, loss_scale=256, train_wall=83, gb_free=6.7, wall=135320
2022-06-29 16:38:30 - progress_bar.py[line:274] - INFO: epoch 002:   1060 / 7081 loss=-0, score=1.33, ntokens=1312.3, nsentences=80, sample_size=1312.3, wps=157.9, ups=0.12, wpb=1312.3, bsz=80, num_updates=8130, lr=8.23146e-06, gnorm=0.413, clip=10, loss_scale=256, train_wall=83, gb_free=6.7, wall=135403
2022-06-29 16:39:53 - progress_bar.py[line:274] - INFO: epoch 002:   1070 / 7081 loss=0.001, score=1.227, ntokens=1296.3, nsentences=80, sample_size=1296.3, wps=155.8, ups=0.12, wpb=1296.3, bsz=80, num_updates=8140, lr=8.22851e-06, gnorm=0.324, clip=10, loss_scale=256, train_wall=83, gb_free=6.7, wall=135486
2022-06-29 16:41:16 - progress_bar.py[line:274] - INFO: epoch 002:   1080 / 7081 loss=-0.001, score=1.229, ntokens=1301.1, nsentences=80, sample_size=1301.1, wps=156.7, ups=0.12, wpb=1301.1, bsz=80, num_updates=8150, lr=8.22557e-06, gnorm=0.651, clip=30, loss_scale=256, train_wall=83, gb_free=6.7, wall=135569
2022-06-29 16:42:39 - progress_bar.py[line:274] - INFO: epoch 002:   1090 / 7081 loss=0.001, score=1.241, ntokens=1283.6, nsentences=80, sample_size=1283.6, wps=154.6, ups=0.12, wpb=1283.6, bsz=80, num_updates=8160, lr=8.22263e-06, gnorm=0.29, clip=0, loss_scale=256, train_wall=83, gb_free=6.7, wall=135652
2022-06-29 16:43:54 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-06-29 16:44:10 - progress_bar.py[line:274] - INFO: epoch 002:   1101 / 7081 loss=-0.001, score=1.369, ntokens=1274.3, nsentences=80, sample_size=1274.3, wps=139.8, ups=0.11, wpb=1274.3, bsz=80, num_updates=8170, lr=8.21968e-06, gnorm=0.324, clip=0, loss_scale=128, train_wall=91, gb_free=6.7, wall=135744
2022-06-29 16:45:33 - progress_bar.py[line:274] - INFO: epoch 002:   1111 / 7081 loss=-0, score=1.314, ntokens=1281.5, nsentences=80, sample_size=1281.5, wps=154.6, ups=0.12, wpb=1281.5, bsz=80, num_updates=8180, lr=8.21674e-06, gnorm=0.327, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=135826
2022-06-29 16:46:56 - progress_bar.py[line:274] - INFO: epoch 002:   1121 / 7081 loss=-0.001, score=1.223, ntokens=1287.8, nsentences=80, sample_size=1287.8, wps=155.6, ups=0.12, wpb=1287.8, bsz=80, num_updates=8190, lr=8.21379e-06, gnorm=0.234, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=135909
2022-06-29 16:48:18 - progress_bar.py[line:274] - INFO: epoch 002:   1131 / 7081 loss=-0.002, score=1.335, ntokens=1266.7, nsentences=80, sample_size=1266.7, wps=153.5, ups=0.12, wpb=1266.7, bsz=80, num_updates=8200, lr=8.21085e-06, gnorm=0.287, clip=0, loss_scale=128, train_wall=82, gb_free=6.7, wall=135992
2022-06-29 16:49:41 - progress_bar.py[line:274] - INFO: epoch 002:   1141 / 7081 loss=-0, score=1.245, ntokens=1259.3, nsentences=80, sample_size=1259.3, wps=151.9, ups=0.12, wpb=1259.3, bsz=80, num_updates=8210, lr=8.2079e-06, gnorm=0.337, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=136075
2022-06-29 16:51:04 - progress_bar.py[line:274] - INFO: epoch 002:   1151 / 7081 loss=0.002, score=1.306, ntokens=1270.4, nsentences=80, sample_size=1270.4, wps=153.4, ups=0.12, wpb=1270.4, bsz=80, num_updates=8220, lr=8.20496e-06, gnorm=0.403, clip=10, loss_scale=128, train_wall=83, gb_free=6.7, wall=136157
2022-06-29 16:52:26 - progress_bar.py[line:274] - INFO: epoch 002:   1161 / 7081 loss=-0, score=1.368, ntokens=1272.2, nsentences=80, sample_size=1272.2, wps=154.2, ups=0.12, wpb=1272.2, bsz=80, num_updates=8230, lr=8.20201e-06, gnorm=0.237, clip=0, loss_scale=128, train_wall=82, gb_free=6.7, wall=136240
2022-06-29 16:53:49 - progress_bar.py[line:274] - INFO: epoch 002:   1171 / 7081 loss=-0, score=1.37, ntokens=1262.5, nsentences=80, sample_size=1262.5, wps=152.2, ups=0.12, wpb=1262.5, bsz=80, num_updates=8240, lr=8.19907e-06, gnorm=0.304, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=136323
2022-06-29 16:55:12 - progress_bar.py[line:274] - INFO: epoch 002:   1181 / 7081 loss=-0.001, score=1.314, ntokens=1255.7, nsentences=80, sample_size=1255.7, wps=151.8, ups=0.12, wpb=1255.7, bsz=80, num_updates=8250, lr=8.19612e-06, gnorm=0.295, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=136406
2022-06-29 16:56:34 - progress_bar.py[line:274] - INFO: epoch 002:   1191 / 7081 loss=-0.001, score=1.208, ntokens=1269.8, nsentences=80, sample_size=1269.8, wps=154.1, ups=0.12, wpb=1269.8, bsz=80, num_updates=8260, lr=8.19318e-06, gnorm=0.336, clip=0, loss_scale=128, train_wall=82, gb_free=6.7, wall=136488
2022-06-29 16:57:57 - progress_bar.py[line:274] - INFO: epoch 002:   1201 / 7081 loss=-0.002, score=1.211, ntokens=1262.5, nsentences=80, sample_size=1262.5, wps=152.4, ups=0.12, wpb=1262.5, bsz=80, num_updates=8270, lr=8.19023e-06, gnorm=0.317, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=136571
2022-06-29 16:59:20 - progress_bar.py[line:274] - INFO: epoch 002:   1211 / 7081 loss=0, score=1.197, ntokens=1237.8, nsentences=80, sample_size=1237.8, wps=149.9, ups=0.12, wpb=1237.8, bsz=80, num_updates=8280, lr=8.18729e-06, gnorm=0.281, clip=0, loss_scale=128, train_wall=82, gb_free=6.7, wall=136653
2022-06-29 17:00:42 - progress_bar.py[line:274] - INFO: epoch 002:   1221 / 7081 loss=-0, score=1.402, ntokens=1240.7, nsentences=80, sample_size=1240.7, wps=151, ups=0.12, wpb=1240.7, bsz=80, num_updates=8290, lr=8.18435e-06, gnorm=0.323, clip=10, loss_scale=128, train_wall=82, gb_free=6.7, wall=136736
2022-06-29 17:02:04 - progress_bar.py[line:274] - INFO: epoch 002:   1231 / 7081 loss=-0, score=1.232, ntokens=1224.5, nsentences=80, sample_size=1224.5, wps=149, ups=0.12, wpb=1224.5, bsz=80, num_updates=8300, lr=8.1814e-06, gnorm=0.333, clip=0, loss_scale=128, train_wall=82, gb_free=6.7, wall=136818
2022-06-29 17:03:27 - progress_bar.py[line:274] - INFO: epoch 002:   1241 / 7081 loss=-0.003, score=1.212, ntokens=1225.9, nsentences=80, sample_size=1225.9, wps=148.5, ups=0.12, wpb=1225.9, bsz=80, num_updates=8310, lr=8.17846e-06, gnorm=0.388, clip=10, loss_scale=128, train_wall=82, gb_free=6.7, wall=136900
2022-06-29 17:04:49 - progress_bar.py[line:274] - INFO: epoch 002:   1251 / 7081 loss=0, score=1.279, ntokens=1218.9, nsentences=80, sample_size=1218.9, wps=148.3, ups=0.12, wpb=1218.9, bsz=80, num_updates=8320, lr=8.17551e-06, gnorm=0.23, clip=0, loss_scale=128, train_wall=82, gb_free=6.7, wall=136983
2022-06-29 17:06:12 - progress_bar.py[line:274] - INFO: epoch 002:   1261 / 7081 loss=-0.001, score=1.338, ntokens=1235, nsentences=80, sample_size=1235, wps=149.6, ups=0.12, wpb=1235, bsz=80, num_updates=8330, lr=8.17257e-06, gnorm=0.881, clip=40, loss_scale=128, train_wall=82, gb_free=6.7, wall=137065
2022-06-29 17:07:34 - progress_bar.py[line:274] - INFO: epoch 002:   1271 / 7081 loss=0.001, score=1.208, ntokens=1228.5, nsentences=80, sample_size=1228.5, wps=149.4, ups=0.12, wpb=1228.5, bsz=80, num_updates=8340, lr=8.16962e-06, gnorm=0.623, clip=20, loss_scale=128, train_wall=82, gb_free=6.7, wall=137147
2022-06-29 17:08:56 - progress_bar.py[line:274] - INFO: epoch 002:   1281 / 7081 loss=-0.001, score=1.262, ntokens=1221.3, nsentences=80, sample_size=1221.3, wps=148.6, ups=0.12, wpb=1221.3, bsz=80, num_updates=8350, lr=8.16668e-06, gnorm=0.322, clip=0, loss_scale=128, train_wall=82, gb_free=6.7, wall=137230
2022-06-29 17:10:19 - progress_bar.py[line:274] - INFO: epoch 002:   1291 / 7081 loss=-0, score=1.265, ntokens=1223.2, nsentences=80, sample_size=1223.2, wps=147.9, ups=0.12, wpb=1223.2, bsz=80, num_updates=8360, lr=8.16373e-06, gnorm=0.309, clip=0, loss_scale=128, train_wall=83, gb_free=6.7, wall=137312
2022-06-29 17:11:41 - progress_bar.py[line:274] - INFO: epoch 002:   1301 / 7081 loss=0, score=1.333, ntokens=1200.6, nsentences=80, sample_size=1200.6, wps=146.1, ups=0.12, wpb=1200.6, bsz=80, num_updates=8370, lr=8.16079e-06, gnorm=0.285, clip=0, loss_scale=128, train_wall=82, gb_free=6.7, wall=137394
2022-06-29 17:13:03 - progress_bar.py[line:274] - INFO: epoch 002:   1311 / 7081 loss=-0.001, score=1.283, ntokens=1211.8, nsentences=80, sample_size=1211.8, wps=147.5, ups=0.12, wpb=1211.8, bsz=80, num_updates=8380, lr=8.15784e-06, gnorm=0.478, clip=10, loss_scale=128, train_wall=82, gb_free=6.7, wall=137477
2022-06-29 17:14:25 - progress_bar.py[line:274] - INFO: epoch 002:   1321 / 7081 loss=-0, score=1.124, ntokens=1217.8, nsentences=80, sample_size=1217.8, wps=148.4, ups=0.12, wpb=1217.8, bsz=80, num_updates=8390, lr=8.1549e-06, gnorm=0.238, clip=0, loss_scale=128, train_wall=82, gb_free=6.7, wall=137559
2022-06-29 17:15:47 - progress_bar.py[line:274] - INFO: epoch 002:   1331 / 7081 loss=0.002, score=1.264, ntokens=1223.2, nsentences=80, sample_size=1223.2, wps=148.7, ups=0.12, wpb=1223.2, bsz=80, num_updates=8400, lr=8.15195e-06, gnorm=0.533, clip=20, loss_scale=128, train_wall=82, gb_free=6.7, wall=137641
2022-06-29 17:17:09 - progress_bar.py[line:274] - INFO: epoch 002:   1341 / 7081 loss=-0, score=1.273, ntokens=1206, nsentences=80, sample_size=1206, wps=147, ups=0.12, wpb=1206, bsz=80, num_updates=8410, lr=8.14901e-06, gnorm=0.428, clip=10, loss_scale=128, train_wall=82, gb_free=6.7, wall=137723
2022-06-29 17:18:15 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-29 17:18:40 - progress_bar.py[line:274] - INFO: epoch 002:   1352 / 7081 loss=-0.001, score=1.271, ntokens=1197.3, nsentences=80, sample_size=1197.3, wps=132.3, ups=0.11, wpb=1197.3, bsz=80, num_updates=8420, lr=8.14607e-06, gnorm=0.405, clip=10, loss_scale=64, train_wall=90, gb_free=6.7, wall=137813
2022-06-29 17:20:03 - progress_bar.py[line:274] - INFO: epoch 002:   1362 / 7081 loss=0, score=1.133, ntokens=1171.6, nsentences=80, sample_size=1171.6, wps=141.7, ups=0.12, wpb=1171.6, bsz=80, num_updates=8430, lr=8.14312e-06, gnorm=0.61, clip=20, loss_scale=64, train_wall=83, gb_free=6.7, wall=137896
2022-06-29 17:21:24 - progress_bar.py[line:274] - INFO: epoch 002:   1372 / 7081 loss=0.001, score=1.277, ntokens=1173.8, nsentences=80, sample_size=1173.8, wps=144.1, ups=0.12, wpb=1173.8, bsz=80, num_updates=8440, lr=8.14018e-06, gnorm=0.462, clip=10, loss_scale=64, train_wall=81, gb_free=6.7, wall=137978
2022-06-29 17:22:46 - progress_bar.py[line:274] - INFO: epoch 002:   1382 / 7081 loss=-0.001, score=1.341, ntokens=1174, nsentences=80, sample_size=1174, wps=143.7, ups=0.12, wpb=1174, bsz=80, num_updates=8450, lr=8.13723e-06, gnorm=0.562, clip=10, loss_scale=64, train_wall=82, gb_free=6.7, wall=138059
2022-06-29 17:24:07 - progress_bar.py[line:274] - INFO: epoch 002:   1392 / 7081 loss=0.001, score=1.208, ntokens=1158.5, nsentences=80, sample_size=1158.5, wps=142.6, ups=0.12, wpb=1158.5, bsz=80, num_updates=8460, lr=8.13429e-06, gnorm=0.591, clip=0, loss_scale=64, train_wall=81, gb_free=6.7, wall=138140
2022-06-29 17:25:28 - progress_bar.py[line:274] - INFO: epoch 002:   1402 / 7081 loss=-0.001, score=1.189, ntokens=1181.4, nsentences=80, sample_size=1181.4, wps=145.2, ups=0.12, wpb=1181.4, bsz=80, num_updates=8470, lr=8.13134e-06, gnorm=0.493, clip=0, loss_scale=64, train_wall=81, gb_free=6.7, wall=138222
2022-06-29 17:26:50 - progress_bar.py[line:274] - INFO: epoch 002:   1412 / 7081 loss=-0, score=1.185, ntokens=1175.8, nsentences=80, sample_size=1175.8, wps=144.6, ups=0.12, wpb=1175.8, bsz=80, num_updates=8480, lr=8.1284e-06, gnorm=0.402, clip=0, loss_scale=64, train_wall=81, gb_free=6.7, wall=138303
2022-06-29 17:28:11 - progress_bar.py[line:274] - INFO: epoch 002:   1422 / 7081 loss=0.002, score=1.341, ntokens=1171.6, nsentences=80, sample_size=1171.6, wps=143.1, ups=0.12, wpb=1171.6, bsz=80, num_updates=8490, lr=8.12545e-06, gnorm=0.488, clip=10, loss_scale=64, train_wall=82, gb_free=6.7, wall=138385
2022-06-29 17:29:33 - progress_bar.py[line:274] - INFO: epoch 002:   1432 / 7081 loss=-0.002, score=1.327, ntokens=1159.4, nsentences=80, sample_size=1159.4, wps=142.5, ups=0.12, wpb=1159.4, bsz=80, num_updates=8500, lr=8.12251e-06, gnorm=0.435, clip=0, loss_scale=64, train_wall=81, gb_free=6.7, wall=138466
slice_id 1 seek offset 2500
2022-06-29 17:29:33 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-29 18:21:30 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss -0 | score 1.168 | ntokens 144.752 | nsentences 10 | sample_size 144.752 | cider 1.193 | wps 116.1 | wpb 144.8 | bsz 10 | num_updates 8500 | best_cider 1.203
2022-06-29 18:21:30 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 8500 updates
2022-06-29 18:21:30 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_8500.pt
2022-06-29 18:21:40 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_8500.pt
2022-06-29 18:22:37 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_8500.pt (epoch 2 @ 8500 updates, score 1.193) (writing took 66.90335639100522 seconds)
2022-06-29 18:24:19 - progress_bar.py[line:274] - INFO: epoch 002:   1442 / 7081 loss=-0, score=1.25, ntokens=1167.8, nsentences=80, sample_size=1167.8, wps=3.6, ups=0, wpb=1167.8, bsz=80, num_updates=8510, lr=8.11956e-06, gnorm=0.557, clip=20, loss_scale=64, train_wall=102, gb_free=6.7, wall=141753
2022-06-29 18:26:02 - progress_bar.py[line:274] - INFO: epoch 002:   1452 / 7081 loss=-0, score=1.217, ntokens=1158.8, nsentences=80, sample_size=1158.8, wps=112.6, ups=0.1, wpb=1158.8, bsz=80, num_updates=8520, lr=8.11662e-06, gnorm=1.191, clip=40, loss_scale=64, train_wall=103, gb_free=6.7, wall=141856
2022-06-29 18:27:45 - progress_bar.py[line:274] - INFO: epoch 002:   1462 / 7081 loss=-0.001, score=1.296, ntokens=1143.2, nsentences=80, sample_size=1143.2, wps=111.4, ups=0.1, wpb=1143.2, bsz=80, num_updates=8530, lr=8.11367e-06, gnorm=0.514, clip=0, loss_scale=64, train_wall=102, gb_free=6.7, wall=141958
2022-06-29 18:29:27 - progress_bar.py[line:274] - INFO: epoch 002:   1472 / 7081 loss=-0, score=1.228, ntokens=1137.4, nsentences=80, sample_size=1137.4, wps=110.8, ups=0.1, wpb=1137.4, bsz=80, num_updates=8540, lr=8.11073e-06, gnorm=0.796, clip=40, loss_scale=64, train_wall=102, gb_free=6.7, wall=142061
2022-06-29 18:31:07 - progress_bar.py[line:274] - INFO: epoch 002:   1482 / 7081 loss=0.001, score=1.342, ntokens=1135.3, nsentences=80, sample_size=1135.3, wps=113.6, ups=0.1, wpb=1135.3, bsz=80, num_updates=8550, lr=8.10779e-06, gnorm=0.457, clip=20, loss_scale=64, train_wall=100, gb_free=6.7, wall=142161
2022-06-29 18:32:49 - progress_bar.py[line:274] - INFO: epoch 002:   1492 / 7081 loss=-0.001, score=1.213, ntokens=1129.4, nsentences=80, sample_size=1129.4, wps=110.8, ups=0.1, wpb=1129.4, bsz=80, num_updates=8560, lr=8.10484e-06, gnorm=0.714, clip=20, loss_scale=64, train_wall=102, gb_free=6.7, wall=142263
2022-06-29 18:34:33 - progress_bar.py[line:274] - INFO: epoch 002:   1502 / 7081 loss=0.001, score=1.44, ntokens=1124.7, nsentences=80, sample_size=1124.7, wps=108.8, ups=0.1, wpb=1124.7, bsz=80, num_updates=8570, lr=8.1019e-06, gnorm=1.278, clip=50, loss_scale=64, train_wall=103, gb_free=6.8, wall=142366
2022-06-29 18:36:12 - progress_bar.py[line:274] - INFO: epoch 002:   1512 / 7081 loss=-0, score=1.299, ntokens=1118.5, nsentences=80, sample_size=1118.5, wps=112.3, ups=0.1, wpb=1118.5, bsz=80, num_updates=8580, lr=8.09895e-06, gnorm=0.557, clip=10, loss_scale=64, train_wall=99, gb_free=6.7, wall=142466
2022-06-29 18:37:52 - progress_bar.py[line:274] - INFO: epoch 002:   1522 / 7081 loss=0, score=1.191, ntokens=1115.3, nsentences=80, sample_size=1115.3, wps=111.3, ups=0.1, wpb=1115.3, bsz=80, num_updates=8590, lr=8.09601e-06, gnorm=0.374, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=142566
2022-06-29 18:39:34 - progress_bar.py[line:274] - INFO: epoch 002:   1532 / 7081 loss=-0, score=1.281, ntokens=1122.1, nsentences=80, sample_size=1122.1, wps=110, ups=0.1, wpb=1122.1, bsz=80, num_updates=8600, lr=8.09306e-06, gnorm=0.709, clip=30, loss_scale=64, train_wall=102, gb_free=6.7, wall=142668
2022-06-29 18:41:16 - progress_bar.py[line:274] - INFO: epoch 002:   1542 / 7081 loss=0.001, score=1.384, ntokens=1102.7, nsentences=80, sample_size=1102.7, wps=108.1, ups=0.1, wpb=1102.7, bsz=80, num_updates=8610, lr=8.09012e-06, gnorm=0.335, clip=0, loss_scale=64, train_wall=102, gb_free=6.7, wall=142770
2022-06-29 18:42:57 - progress_bar.py[line:274] - INFO: epoch 002:   1552 / 7081 loss=0.001, score=1.19, ntokens=1103.9, nsentences=80, sample_size=1103.9, wps=109.3, ups=0.1, wpb=1103.9, bsz=80, num_updates=8620, lr=8.08717e-06, gnorm=0.694, clip=20, loss_scale=64, train_wall=101, gb_free=6.8, wall=142871
2022-06-29 18:44:37 - progress_bar.py[line:274] - INFO: epoch 002:   1562 / 7081 loss=0.001, score=1.242, ntokens=1115.5, nsentences=80, sample_size=1115.5, wps=112, ups=0.1, wpb=1115.5, bsz=80, num_updates=8630, lr=8.08423e-06, gnorm=0.579, clip=10, loss_scale=64, train_wall=99, gb_free=6.7, wall=142970
2022-06-29 18:46:19 - progress_bar.py[line:274] - INFO: epoch 002:   1572 / 7081 loss=0, score=1.206, ntokens=1109.9, nsentences=80, sample_size=1109.9, wps=108.9, ups=0.1, wpb=1109.9, bsz=80, num_updates=8640, lr=8.08128e-06, gnorm=0.387, clip=10, loss_scale=64, train_wall=102, gb_free=6.7, wall=143072
2022-06-29 18:47:59 - progress_bar.py[line:274] - INFO: epoch 002:   1582 / 7081 loss=0.001, score=1.372, ntokens=1103.3, nsentences=80, sample_size=1103.3, wps=109.8, ups=0.1, wpb=1103.3, bsz=80, num_updates=8650, lr=8.07834e-06, gnorm=0.412, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=143173
2022-06-29 18:49:38 - progress_bar.py[line:274] - INFO: epoch 002:   1592 / 7081 loss=-0.001, score=1.31, ntokens=1100.5, nsentences=80, sample_size=1100.5, wps=111.1, ups=0.1, wpb=1100.5, bsz=80, num_updates=8660, lr=8.07539e-06, gnorm=0.547, clip=10, loss_scale=64, train_wall=99, gb_free=6.7, wall=143272
2022-06-29 18:51:20 - progress_bar.py[line:274] - INFO: epoch 002:   1602 / 7081 loss=-0, score=1.272, ntokens=1089.2, nsentences=80, sample_size=1089.2, wps=107.1, ups=0.1, wpb=1089.2, bsz=80, num_updates=8670, lr=8.07245e-06, gnorm=0.62, clip=10, loss_scale=64, train_wall=101, gb_free=6.8, wall=143374
2022-06-29 18:53:01 - progress_bar.py[line:274] - INFO: epoch 002:   1612 / 7081 loss=-0, score=1.308, ntokens=1072.4, nsentences=80, sample_size=1072.4, wps=106.8, ups=0.1, wpb=1072.4, bsz=80, num_updates=8680, lr=8.06951e-06, gnorm=0.841, clip=40, loss_scale=64, train_wall=100, gb_free=6.8, wall=143474
2022-06-29 18:54:38 - progress_bar.py[line:274] - INFO: epoch 002:   1622 / 7081 loss=-0.002, score=1.297, ntokens=1074.8, nsentences=80, sample_size=1074.8, wps=110.1, ups=0.1, wpb=1074.8, bsz=80, num_updates=8690, lr=8.06656e-06, gnorm=0.547, clip=10, loss_scale=64, train_wall=98, gb_free=6.7, wall=143572
2022-06-29 18:56:18 - progress_bar.py[line:274] - INFO: epoch 002:   1632 / 7081 loss=-0, score=1.239, ntokens=1074.8, nsentences=80, sample_size=1074.8, wps=108.2, ups=0.1, wpb=1074.8, bsz=80, num_updates=8700, lr=8.06362e-06, gnorm=0.629, clip=20, loss_scale=64, train_wall=99, gb_free=6.8, wall=143671
2022-06-29 18:57:57 - progress_bar.py[line:274] - INFO: epoch 002:   1642 / 7081 loss=-0.001, score=1.21, ntokens=1066.7, nsentences=80, sample_size=1066.7, wps=106.7, ups=0.1, wpb=1066.7, bsz=80, num_updates=8710, lr=8.06067e-06, gnorm=0.538, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=143771
2022-06-29 18:59:33 - progress_bar.py[line:274] - INFO: epoch 002:   1652 / 7081 loss=-0.001, score=1.309, ntokens=1063.3, nsentences=80, sample_size=1063.3, wps=110.8, ups=0.1, wpb=1063.3, bsz=80, num_updates=8720, lr=8.05773e-06, gnorm=0.755, clip=30, loss_scale=64, train_wall=96, gb_free=6.8, wall=143867
2022-06-29 19:01:13 - progress_bar.py[line:274] - INFO: epoch 002:   1662 / 7081 loss=0.001, score=1.237, ntokens=1061.7, nsentences=80, sample_size=1061.7, wps=106.4, ups=0.1, wpb=1061.7, bsz=80, num_updates=8730, lr=8.05478e-06, gnorm=1.111, clip=40, loss_scale=64, train_wall=100, gb_free=6.8, wall=143967
2022-06-29 19:02:53 - progress_bar.py[line:274] - INFO: epoch 002:   1672 / 7081 loss=-0, score=1.173, ntokens=1080.2, nsentences=80, sample_size=1080.2, wps=107.8, ups=0.1, wpb=1080.2, bsz=80, num_updates=8740, lr=8.05184e-06, gnorm=0.507, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=144067
2022-06-29 19:04:30 - progress_bar.py[line:274] - INFO: epoch 002:   1682 / 7081 loss=-0, score=1.257, ntokens=1075.5, nsentences=80, sample_size=1075.5, wps=110.8, ups=0.1, wpb=1075.5, bsz=80, num_updates=8750, lr=8.04889e-06, gnorm=0.618, clip=10, loss_scale=64, train_wall=97, gb_free=6.8, wall=144164
2022-06-29 19:06:11 - progress_bar.py[line:274] - INFO: epoch 002:   1692 / 7081 loss=-0, score=1.237, ntokens=1091.2, nsentences=80, sample_size=1091.2, wps=108.3, ups=0.1, wpb=1091.2, bsz=80, num_updates=8760, lr=8.04595e-06, gnorm=0.55, clip=10, loss_scale=64, train_wall=101, gb_free=6.7, wall=144265
2022-06-29 19:07:53 - progress_bar.py[line:274] - INFO: epoch 002:   1702 / 7081 loss=0.001, score=1.213, ntokens=1093.6, nsentences=80, sample_size=1093.6, wps=107.5, ups=0.1, wpb=1093.6, bsz=80, num_updates=8770, lr=8.043e-06, gnorm=0.551, clip=10, loss_scale=64, train_wall=102, gb_free=6.7, wall=144366
2022-06-29 19:09:30 - progress_bar.py[line:274] - INFO: epoch 002:   1712 / 7081 loss=-0, score=1.272, ntokens=1080.6, nsentences=80, sample_size=1080.6, wps=111, ups=0.1, wpb=1080.6, bsz=80, num_updates=8780, lr=8.04006e-06, gnorm=0.599, clip=20, loss_scale=64, train_wall=97, gb_free=6.7, wall=144464
2022-06-29 19:11:11 - progress_bar.py[line:274] - INFO: epoch 002:   1722 / 7081 loss=-0, score=1.222, ntokens=1102.2, nsentences=80, sample_size=1102.2, wps=109.8, ups=0.1, wpb=1102.2, bsz=80, num_updates=8790, lr=8.03711e-06, gnorm=0.733, clip=30, loss_scale=64, train_wall=100, gb_free=6.8, wall=144564
2022-06-29 19:12:53 - progress_bar.py[line:274] - INFO: epoch 002:   1732 / 7081 loss=0, score=1.236, ntokens=1114.8, nsentences=80, sample_size=1114.8, wps=109.3, ups=0.1, wpb=1114.8, bsz=80, num_updates=8800, lr=8.03417e-06, gnorm=0.596, clip=10, loss_scale=64, train_wall=102, gb_free=6.7, wall=144666
2022-06-29 19:14:33 - progress_bar.py[line:274] - INFO: epoch 002:   1742 / 7081 loss=-0.001, score=1.285, ntokens=1108, nsentences=80, sample_size=1108, wps=110.8, ups=0.1, wpb=1108, bsz=80, num_updates=8810, lr=8.03123e-06, gnorm=0.621, clip=30, loss_scale=64, train_wall=100, gb_free=6.7, wall=144766
2022-06-29 19:16:12 - progress_bar.py[line:274] - INFO: epoch 002:   1752 / 7081 loss=-0.001, score=1.35, ntokens=1115, nsentences=80, sample_size=1115, wps=112.8, ups=0.1, wpb=1115, bsz=80, num_updates=8820, lr=8.02828e-06, gnorm=0.667, clip=20, loss_scale=64, train_wall=99, gb_free=6.7, wall=144865
2022-06-29 19:17:54 - progress_bar.py[line:274] - INFO: epoch 002:   1762 / 7081 loss=0, score=1.277, ntokens=1137.6, nsentences=80, sample_size=1137.6, wps=111.2, ups=0.1, wpb=1137.6, bsz=80, num_updates=8830, lr=8.02534e-06, gnorm=0.972, clip=50, loss_scale=64, train_wall=102, gb_free=6.7, wall=144967
2022-06-29 19:19:37 - progress_bar.py[line:274] - INFO: epoch 002:   1772 / 7081 loss=-0.001, score=1.24, ntokens=1155.4, nsentences=80, sample_size=1155.4, wps=111.9, ups=0.1, wpb=1155.4, bsz=80, num_updates=8840, lr=8.02239e-06, gnorm=0.576, clip=20, loss_scale=64, train_wall=103, gb_free=6.7, wall=145071
2022-06-29 19:21:17 - progress_bar.py[line:274] - INFO: epoch 002:   1782 / 7081 loss=0.001, score=1.245, ntokens=1148.3, nsentences=80, sample_size=1148.3, wps=114.5, ups=0.1, wpb=1148.3, bsz=80, num_updates=8850, lr=8.01945e-06, gnorm=0.703, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=145171
2022-06-29 19:22:59 - progress_bar.py[line:274] - INFO: epoch 002:   1792 / 7081 loss=-0.001, score=1.276, ntokens=1173, nsentences=80, sample_size=1173, wps=116, ups=0.1, wpb=1173, bsz=80, num_updates=8860, lr=8.0165e-06, gnorm=0.643, clip=20, loss_scale=64, train_wall=101, gb_free=6.7, wall=145272
2022-06-29 19:24:44 - progress_bar.py[line:274] - INFO: epoch 002:   1802 / 7081 loss=-0.002, score=1.272, ntokens=1170.7, nsentences=80, sample_size=1170.7, wps=111.5, ups=0.1, wpb=1170.7, bsz=80, num_updates=8870, lr=8.01356e-06, gnorm=0.912, clip=40, loss_scale=64, train_wall=105, gb_free=6.7, wall=145377
2022-06-29 19:26:27 - progress_bar.py[line:274] - INFO: epoch 002:   1812 / 7081 loss=-0, score=1.226, ntokens=1151.2, nsentences=80, sample_size=1151.2, wps=111.5, ups=0.1, wpb=1151.2, bsz=80, num_updates=8880, lr=8.01061e-06, gnorm=0.51, clip=20, loss_scale=64, train_wall=103, gb_free=6.7, wall=145480
2022-06-29 19:28:09 - progress_bar.py[line:274] - INFO: epoch 002:   1822 / 7081 loss=-0.001, score=1.203, ntokens=1128.3, nsentences=80, sample_size=1128.3, wps=110.2, ups=0.1, wpb=1128.3, bsz=80, num_updates=8890, lr=8.00767e-06, gnorm=0.433, clip=0, loss_scale=64, train_wall=102, gb_free=6.7, wall=145583
2022-06-29 19:29:49 - progress_bar.py[line:274] - INFO: epoch 002:   1832 / 7081 loss=-0.001, score=1.237, ntokens=1134.3, nsentences=80, sample_size=1134.3, wps=113.8, ups=0.1, wpb=1134.3, bsz=80, num_updates=8900, lr=8.00472e-06, gnorm=0.308, clip=0, loss_scale=64, train_wall=100, gb_free=6.7, wall=145682
2022-06-29 19:31:29 - progress_bar.py[line:274] - INFO: epoch 002:   1842 / 7081 loss=0, score=1.388, ntokens=1145.2, nsentences=80, sample_size=1145.2, wps=113.8, ups=0.1, wpb=1145.2, bsz=80, num_updates=8910, lr=8.00178e-06, gnorm=0.512, clip=10, loss_scale=64, train_wall=100, gb_free=6.7, wall=145783
2022-06-29 19:33:13 - progress_bar.py[line:274] - INFO: epoch 002:   1852 / 7081 loss=-0.001, score=1.29, ntokens=1174.1, nsentences=80, sample_size=1174.1, wps=113.5, ups=0.1, wpb=1174.1, bsz=80, num_updates=8920, lr=7.99883e-06, gnorm=0.897, clip=20, loss_scale=64, train_wall=103, gb_free=6.7, wall=145886
2022-06-29 19:34:57 - progress_bar.py[line:274] - INFO: epoch 002:   1862 / 7081 loss=-0, score=1.383, ntokens=1170.9, nsentences=80, sample_size=1170.9, wps=112.2, ups=0.1, wpb=1170.9, bsz=80, num_updates=8930, lr=7.99589e-06, gnorm=0.413, clip=0, loss_scale=128, train_wall=104, gb_free=6.7, wall=145991
2022-06-29 19:36:39 - progress_bar.py[line:274] - INFO: epoch 002:   1872 / 7081 loss=-0.001, score=1.316, ntokens=1171.9, nsentences=80, sample_size=1171.9, wps=115.5, ups=0.1, wpb=1171.9, bsz=80, num_updates=8940, lr=7.99294e-06, gnorm=0.391, clip=0, loss_scale=128, train_wall=101, gb_free=6.7, wall=146092
2022-06-29 19:38:11 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-29 19:38:33 - progress_bar.py[line:274] - INFO: epoch 002:   1883 / 7081 loss=0.001, score=1.333, ntokens=1183.1, nsentences=80, sample_size=1183.1, wps=103.9, ups=0.09, wpb=1183.1, bsz=80, num_updates=8950, lr=7.99e-06, gnorm=0.809, clip=20, loss_scale=64, train_wall=114, gb_free=6.7, wall=146206
2022-06-29 19:40:16 - progress_bar.py[line:274] - INFO: epoch 002:   1893 / 7081 loss=-0.001, score=1.214, ntokens=1188, nsentences=80, sample_size=1188, wps=114.6, ups=0.1, wpb=1188, bsz=80, num_updates=8960, lr=7.98706e-06, gnorm=0.236, clip=0, loss_scale=64, train_wall=103, gb_free=6.7, wall=146310
2022-06-29 19:41:59 - progress_bar.py[line:274] - INFO: epoch 002:   1903 / 7081 loss=-0.001, score=1.261, ntokens=1197.8, nsentences=80, sample_size=1197.8, wps=116.2, ups=0.1, wpb=1197.8, bsz=80, num_updates=8970, lr=7.98411e-06, gnorm=0.653, clip=10, loss_scale=64, train_wall=103, gb_free=6.7, wall=146413
2022-06-29 19:43:42 - progress_bar.py[line:274] - INFO: epoch 002:   1913 / 7081 loss=0, score=1.234, ntokens=1190.3, nsentences=80, sample_size=1190.3, wps=116.4, ups=0.1, wpb=1190.3, bsz=80, num_updates=8980, lr=7.98117e-06, gnorm=0.272, clip=0, loss_scale=64, train_wall=102, gb_free=6.7, wall=146515
2022-06-29 19:45:27 - progress_bar.py[line:274] - INFO: epoch 002:   1923 / 7081 loss=0, score=1.199, ntokens=1212.6, nsentences=80, sample_size=1212.6, wps=115.6, ups=0.1, wpb=1212.6, bsz=80, num_updates=8990, lr=7.97822e-06, gnorm=0.574, clip=20, loss_scale=64, train_wall=105, gb_free=6.7, wall=146620
2022-06-29 19:47:12 - progress_bar.py[line:274] - INFO: epoch 002:   1933 / 7081 loss=-0.002, score=1.323, ntokens=1224.4, nsentences=80, sample_size=1224.4, wps=115.9, ups=0.09, wpb=1224.4, bsz=80, num_updates=9000, lr=7.97528e-06, gnorm=0.518, clip=10, loss_scale=64, train_wall=105, gb_free=6.7, wall=146726
slice_id 1 seek offset 2500
2022-06-29 19:47:12 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 2500
2022-06-29 20:51:40 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss -0 | score 1.171 | ntokens 153.885 | nsentences 10 | sample_size 153.885 | cider 1.194 | wps 99.4 | wpb 153.9 | bsz 10 | num_updates 9000 | best_cider 1.203
2022-06-29 20:51:40 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 9000 updates
2022-06-29 20:51:40 - trainer.py[line:431] - INFO: Saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_9000.pt
2022-06-29 20:51:57 - trainer.py[line:441] - INFO: Finished saving checkpoint to /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_9000.pt
2022-06-29 20:52:50 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint /database/jhkim/stage2_checkpoints_sibal//1e-5_5/checkpoint_2_9000.pt (epoch 2 @ 9000 updates, score 1.194) (writing took 69.67199111264199 seconds)
2022-06-29 20:54:34 - progress_bar.py[line:274] - INFO: epoch 002:   1943 / 7081 loss=0.001, score=1.218, ntokens=1248.6, nsentences=80, sample_size=1248.6, wps=3.1, ups=0, wpb=1248.6, bsz=80, num_updates=9010, lr=7.97233e-06, gnorm=0.74, clip=30, loss_scale=64, train_wall=104, gb_free=6.7, wall=150768
2022-06-29 20:56:18 - progress_bar.py[line:274] - INFO: epoch 002:   1953 / 7081 loss=0.001, score=1.195, ntokens=1264.6, nsentences=80, sample_size=1264.6, wps=121.6, ups=0.1, wpb=1264.6, bsz=80, num_updates=9020, lr=7.96939e-06, gnorm=0.641, clip=30, loss_scale=64, train_wall=104, gb_free=6.7, wall=150872
2022-06-29 20:58:03 - progress_bar.py[line:274] - INFO: epoch 002:   1963 / 7081 loss=-0.001, score=1.16, ntokens=1264, nsentences=80, sample_size=1264, wps=120.4, ups=0.1, wpb=1264, bsz=80, num_updates=9030, lr=7.96644e-06, gnorm=0.317, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=150977
2022-06-29 20:59:48 - progress_bar.py[line:274] - INFO: epoch 002:   1973 / 7081 loss=0, score=1.26, ntokens=1268.1, nsentences=80, sample_size=1268.1, wps=121.3, ups=0.1, wpb=1268.1, bsz=80, num_updates=9040, lr=7.9635e-06, gnorm=0.276, clip=0, loss_scale=64, train_wall=104, gb_free=6.7, wall=151081
2022-06-29 21:01:33 - progress_bar.py[line:274] - INFO: epoch 002:   1983 / 7081 loss=0, score=1.336, ntokens=1273.5, nsentences=80, sample_size=1273.5, wps=121, ups=0.1, wpb=1273.5, bsz=80, num_updates=9050, lr=7.96055e-06, gnorm=0.444, clip=0, loss_scale=64, train_wall=105, gb_free=6.7, wall=151187
Traceback (most recent call last):
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/wgus5950/miniconda3/envs/ofa2/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/wgus5950/miniconda3/envs/ofa2/bin/python3', '-u', '../../train.py', '--local_rank=1', '../../dataset/caption_data/caption_stage2_train_half.tsv,../../dataset/caption_data/caption_val_half.tsv', '--selected-cols=1,4,2', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/database/jhkim/caption_large_best_clean.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=/database/jhkim/stage2_checkpoints_sibal//1e-5_5', '--task=caption', '--arch=ofa_large', '--criterion=scst_reward_criterion', '--batch-size=1', '--update-freq=8', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.0', '--decoder-drop-path-rate=0.0', '--dropout=0.0', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=1e-5', '--end-learning-rate=2e-7', '--max-epoch=5', '--warmup-ratio=0.06', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--no-epoch-checkpoints', '--keep-best-checkpoints=1', '--save-interval=1', '--validate-interval=1', '--save-interval-updates=500', '--validate-interval-updates=500', '--eval-cider', '--eval-cider-cached-tokens=../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', '--eval-args={"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', '--best-checkpoint-metric=cider', '--maximize-best-checkpoint-metric', '--max-src-length=80', '--max-tgt-length=20', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--freeze-resnet', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--scst', '--scst-cider-cached-tokens=../../dataset/caption_data/cider_cached_tokens/coco-train-words.p', '--scst-args={"beam":5,"max_len_b":16,"no_repeat_ngram_size":3}', '--memory-efficient-fp16', '--fp16-scale-window=512', '--num-workers=0']' died with <Signals.SIGTERM: 15>.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1525512
Killing subprocess 1525513
